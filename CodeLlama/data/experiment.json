{
    "class": "class Remove_100(BaseEstimator, TransformerMixin):\n\n    def __init__(self, target):\n        self.target = target\n        self.columns_to_drop = []\n\n    def fit(self, data, y=None):\n        self.fit_transform(data, y=y)\n        return self\n\n    def transform(self, dataset, y=None):\n        return dataset.drop(self.columns_to_drop, axis=1)\n\n    def fit_transform(self, dataset, y=None):\n        data = dataset\n\n        targetless_data = data.drop(self.target, axis=1)\n\n        if len(targetless_data.columns) <= 1:\n            return data\n\n        corr = pd.DataFrame(np.corrcoef(targetless_data.T))\n        corr.columns = targetless_data.columns\n        corr.index = targetless_data.columns\n        corr_matrix = abs(corr)\n\n        corr_matrix[\"column\"] = corr_matrix.index\n        corr_matrix.reset_index(drop=True, inplace=True)\n\n        cols = corr_matrix.column\n        melt = corr_matrix.melt(id_vars=[\"column\"], value_vars=cols).sort_values(\n            by=\"value\", ascending=False\n        )\n        melt[\"value\"] = round(melt[\"value\"], 2)\n\n        c1 = melt[\"value\"] == 1.00\n        c2 = melt[\"column\"] != melt[\"variable\"]\n        melt = melt[((c1 == True) & (c2 == True))]\n\n        melt[\"all_columns\"] = melt[\"column\"] + melt[\"variable\"]\n\n        melt[\"all_columns\"] = [sorted(i) for i in melt[\"all_columns\"]]\n\n        melt = melt.sort_values(by=\"all_columns\")\n\n        melt = melt.iloc[::2, :]\n\n        self.columns_to_drop = melt[\"variable\"]\n\n        return data.drop(self.columns_to_drop, axis=1)",
    "docstring": "- Takes DF, return data frame while removing features that are perfectly correlated (droping one)"
}
{
    "class": "class LinearBoxHelper(BoxHelper):\n    orientation_map = {\n        'horizontal': ('left', 'right'),\n        'vertical': ('top', 'bottom'),\n    }\n\n    ortho_map = {\n        'horizontal': 'vertical',\n        'vertical': 'horizontal',\n    }\n\n    def __init__(self, orientation, *items, **config):\n        super(LinearBoxHelper, self).__init__(orientation[0] + 'box')\n        self.items = items\n        self.orientation = orientation\n        self.ortho_orientation = self.ortho_map[orientation]\n        self.spacing = config.get('spacing', DefaultSpacing.ABUTMENT)\n        self.margins = Box(config.get('margins', DefaultSpacing.BOX_MARGINS))\n\n    def __repr__(self):\n        items = ', '.join(map(repr, self.items))\n        return '{0}box({1})'.format(self.orientation[0], items)\n\n    def _get_constraints(self, component):\n        items = [item for item in self.items if item is not None]\n        if len(items) == 0:\n            return items\n\n        first, last = self.orientation_map[self.orientation]\n        first_boundary = getattr(self, first)\n        last_boundary = getattr(self, last)\n        first_ortho, last_ortho = self.orientation_map[self.ortho_orientation]\n        first_ortho_boundary = getattr(self, first_ortho)\n        last_ortho_boundary = getattr(self, last_ortho)\n\n        if component is not None:\n            attrs = ['top', 'bottom', 'left', 'right']\n            if hasattr(component, 'contents_top'):\n                other_attrs = ['contents_' + attr for attr in attrs]\n            else:\n                other_attrs = attrs[:]\n            constraints = [\n                getattr(self, attr) == getattr(component, other)\n                for (attr, other) in zip(attrs, other_attrs)\n            ]\n        else:\n            constraints = []\n\n        margins = self.margins\n        if self.orientation == 'vertical':\n            first_spacer = EqSpacer(margins.top)\n            last_spacer = EqSpacer(margins.bottom)\n            first_ortho_spacer = FlexSpacer(margins.left)\n            last_ortho_spacer = FlexSpacer(margins.right)\n        else:\n            first_spacer = EqSpacer(margins.left)\n            last_spacer = EqSpacer(margins.right)\n            first_ortho_spacer = FlexSpacer(margins.top)\n            last_ortho_spacer = FlexSpacer(margins.bottom)\n\n        if not is_spacer(items[0]):\n            pre_along_args = [first_boundary, first_spacer]\n        else:\n            pre_along_args = [first_boundary]\n        if not is_spacer(items[-1]):\n            post_along_args = [last_spacer, last_boundary]\n        else:\n            post_along_args = [last_boundary]\n\n        along_args = pre_along_args + items + post_along_args\n        kwds = dict(spacing=self.spacing)\n        helpers = [AbutmentHelper(self.orientation, *along_args, **kwds)]\n        ortho = self.ortho_orientation\n        for item in items:\n            if isinstance(item, ABConstrainable):\n                abutment_items = (\n                    first_ortho_boundary, first_ortho_spacer,\n                    item, last_ortho_spacer, last_ortho_boundary,\n                )\n                helpers.append(AbutmentHelper(ortho, *abutment_items, **kwds))\n            if isinstance(item, DeferredConstraints):\n                helpers.append(item)\n\n        for helper in helpers:\n            constraints.extend(helper.get_constraints(None))\n\n        return constraints",
    "docstring": "A layout helper which arranges items in a linear box."
}
{
    "class": "class Tag(models.Model):\n    name = models.CharField(max_length=255)\n    user = models.ForeignKey(\n        settings.AUTH_USER_MODEL,\n        on_delete=models.CASCADE\n    )\n\n    def __str__(self) -> str:\n        return self.name",
    "docstring": "Tag to be used for a recipe"
}
{
    "class": "class NaimaSpectralModel(SpectralModel):\n    r\"\"\"A wrapper for Naima models.\n\n    For more information see :ref:`naima-spectral-model`.\n\n    Parameters\n    ----------\n    radiative_model : `~naima.models.BaseRadiative`\n        An instance of a radiative model defined in `~naima.models`\n    distance : `~astropy.units.Quantity`, optional\n        Distance to the source. If set to 0, the intrinsic differential\n        luminosity will be returned. Default is 1 kpc\n    seed : str or list of str, optional\n        Seed photon field(s) to be considered for the `radiative_model` flux computation,\n        in case of a `~naima.models.InverseCompton` model. It can be a subset of the\n        `seed_photon_fields` list defining the `radiative_model`. Default is the whole list\n        of photon fields\n    nested_models : dict\n        Additional parameters for nested models not supplied by the radiative model,\n        for now this is used  only for synchrotron self-compton model\n        Compute photon density spectrum from synchrotron emission for synchrotron self-compton model,\n        assuming uniform synchrotron emissivity inside a sphere of radius R\n        (see Section 4.1 of Atoyan & Aharonian 1996)\n\n        based on :\n        \"https://naima.readthedocs.io/en/latest/examples.html\n\n        import naima\n\n        for name, value in kwargs.items():\n            setattr(self._particle_distribution, name, value)\n\n        if \"B\" in self.radiative_model.param_names:\n            self.radiative_model.B = self.B.quantity\n\n        if (\n            isinstance(self.radiative_model, naima.models.InverseCompton)\n            and \"SSC\" in self.nested_models\n        ):\n            dnde = self._evaluate_ssc(energy.flatten())\n        elif self.seed is not None:\n            dnde = self.radiative_model.flux(\n                energy.flatten(), seed=self.seed, distance=self.distance\n            )\n        else:\n            dnde = self.radiative_model.flux(energy.flatten(), distance=self.distance)\n\n        dnde = dnde.reshape(energy.shape)\n        unit = 1 / (energy.unit * u.cm ** 2 * u.s)\n        return dnde.to(unit)\n\n    def to_dict(self, full_output=True):\n        return super().to_dict(full_output=True)\n\n    @classmethod\n    def from_dict(cls, data):\n        raise NotImplementedError(\n            \"Currently the NaimaSpectralModel cannot be read from YAML\"\n        )\n\n    @classmethod\n    def from_parameters(cls, parameters, **kwargs):\n        raise NotImplementedError(\n            \"Currently the NaimaSpectralModel cannot be built from a list of parameters.\"\n        )",
    "docstring": "r\"\"\"A wrapper for Naima models.\n\n    For more information see :ref:`naima-spectral-model`.\n\n    Parameters\n    ----------\n    radiative_model : `~naima.models.BaseRadiative`\n        An instance of a radiative model defined in `~naima.models`\n    distance : `~astropy.units.Quantity`, optional\n        Distance to the source. If set to 0, the intrinsic differential\n        luminosity will be returned. Default is 1 kpc\n    seed : str or list of str, optional\n        Seed photon field(s) to be considered for the `radiative_model` flux computation,\n        in case of a `~naima.models.InverseCompton` model. It can be a subset of the\n        `seed_photon_fields` list defining the `radiative_model`. Default is the whole list\n        of photon fields\n    nested_models : dict\n        Additional parameters for nested models not supplied by the radiative model,\n        for now this is used  only for synchrotron self-compton model"
}
{
    "class": "class _SearchState(object):\n    def __init__(self,\n                 app,\n                 entry_point_group_mappings=None,\n                 entry_point_group_templates=None,\n                 **kwargs):\n        self.app = app\n        self.aliases = {}\n        self.number_of_indexes = 0\n        self._client = kwargs.get('client')\n        self.entry_point_group_templates = entry_point_group_templates\n\n        if entry_point_group_mappings:\n            self.load_entry_point_group_mappings(entry_point_group_mappings)\n\n        with app.app_context():\n            get_mappings = app.config.get('SEARCH_GET_MAPPINGS_IMP')\n            if get_mappings:\n                self.get_mappings = import_string(get_mappings)\n\n    def __getattr__(self, name):\n        if name == 'mappings':\n            return self.get_mappings()\n        else:\n            raise AttributeError\n\n    def get_mappings(self):\n        return {}\n\n    @cached_property\n    def templates(self):\n        result = None\n        if self.entry_point_group_templates:\n            result = self.load_entry_point_group_templates(\n                self.entry_point_group_templates)\n        return {k: v for d in result for k, v in d.items()} \\\n            if result is not None else {}\n\n    def register_mappings(self, alias, package_name):\n        if ES_VERSION[0] == 2:\n            try:\n                resource_listdir(package_name, 'v2')\n                package_name += '.v2'\n            except (OSError, IOError) as ex:\n                if getattr(ex, 'errno', 0) != errno.ENOENT:\n                    raise\n                warnings.warn(\n                    \"Having mappings in a path which doesn't specify the \"\n                    \"Elasticsearch version is deprecated. Please move your \"\n                    \"mappings to a subfolder named according to the \"\n                    \"Elasticsearch version which your mappings are intended \"\n                    \"for. (e.g. '{}/v2/{}')\".format(package_name, alias),\n                    PendingDeprecationWarning)\n        else:\n            package_name = '{}.v{}'.format(package_name, ES_VERSION[0])\n\n        def _walk_dir(aliases, *parts):\n            root_name = build_index_name(*parts)\n            resource_name = os.path.join(*parts)\n\n            if root_name not in aliases:\n                self.number_of_indexes += 1\n\n            data = aliases.get(root_name, {})\n\n            for filename in resource_listdir(package_name, resource_name):\n                index_name = build_index_name(*(parts + (filename, )))\n                file_path = os.path.join(resource_name, filename)\n\n                if resource_isdir(package_name, file_path):\n                    _walk_dir(data, *(parts + (filename, )))\n                    continue\n\n                ext = os.path.splitext(filename)[1]\n                if ext not in {\n                        '.json',\n                }:\n                    continue\n\n                assert index_name not in data, 'Duplicate index'\n                data[index_name] = self.mappings[index_name] = \\\n                    resource_filename(\n                        package_name, os.path.join(resource_name, filename))\n                self.number_of_indexes += 1\n\n            aliases[root_name] = data\n\n        _walk_dir(self.aliases, alias)\n\n    def register_templates(self, directory):\n        try:\n            resource_listdir(directory, 'v{}'.format(ES_VERSION[0]))\n            directory = '{}/v{}'.format(directory, ES_VERSION[0])\n        except (OSError, IOError) as ex:\n            if getattr(ex, 'errno', 0) == errno.ENOENT:\n                raise OSError(\n                    \"Please move your templates to a subfolder named \"\n                    \"according to the Elasticsearch version \"\n                    \"which your templates are intended \"\n                    \"for. (e.g. '{}.v{}')\".format(directory, ES_VERSION[0]))\n        result = {}\n        module_name, parts = directory.split('.')[0], directory.split('.')[1:]\n        parts = tuple(parts)\n\n        def _walk_dir(parts):\n            resource_name = os.path.join(*parts)\n\n            for filename in resource_listdir(module_name, resource_name):\n                template_name = build_index_name(*(parts[1:] + (filename, )))\n                file_path = os.path.join(resource_name, filename)\n\n                if resource_isdir(module_name, file_path):\n                    _walk_dir((parts + (filename, )))\n                    continue\n\n                ext = os.path.splitext(filename)[1]\n                if ext not in {\n                        '.json',\n                }:\n                    continue\n\n                result[template_name] = resource_filename(\n                    module_name, os.path.join(resource_name, filename))\n\n        _walk_dir(parts)\n        return result\n\n    def load_entry_point_group_mappings(self, entry_point_group_mappings):\n        for ep in iter_entry_points(group=entry_point_group_mappings):\n            self.register_mappings(ep.name, ep.module_name)\n\n    def load_entry_point_group_templates(self, entry_point_group_templates):\n        result = []\n        for ep in iter_entry_points(group=entry_point_group_templates):\n            with self.app.app_context():\n                for template_dir in ep.load()():\n                    result.append(self.register_templates(template_dir))\n        return result\n\n    def _client_builder(self):\n        from elasticsearch import Elasticsearch\n        from elasticsearch.connection import RequestsHttpConnection\n\n        return Elasticsearch(\n            hosts=self.app.config.get('SEARCH_ELASTIC_HOSTS'),\n            connection_class=RequestsHttpConnection,\n        )\n\n    @property\n    def client(self):\n        if self._client is None:\n            self._client = self._client_builder()\n        return self._client\n\n    def flush_and_refresh(self, index):\n        self.client.indices.flush(wait_if_ongoing=True, index=index)\n        self.client.indices.refresh(index=index)\n        self.client.cluster.health(wait_for_status='yellow',\n                                   request_timeout=30)\n        return True\n\n    @property\n    def cluster_version(self):\n        versionstr = self.client.info()['version']['number']\n        return [int(x) for x in versionstr.split('.')]\n\n    @property\n    def active_aliases(self):\n        whitelisted_aliases = self.app.config.get('SEARCH_MAPPINGS')\n        if whitelisted_aliases is None:\n            return self.aliases\n        else:\n            return {\n                k: v\n                for k, v in self.aliases.items() if k in whitelisted_aliases\n            }\n\n    def create(self, ignore=None):\n        ignore = ignore or []\n\n        def _create(tree_or_filename, alias=None):\n            for name, value in tree_or_filename.items():\n                if isinstance(value, dict):\n                    for result in _create(value, alias=name):\n                        yield result\n                else:\n                    with open(value, 'r') as body:\n                        yield name, self.client.indices.create(\n                            index=name,\n                            body=json.load(body),\n                            ignore=ignore,\n                        )\n\n            if alias:\n                yield alias, self.client.indices.put_alias(\n                    index=list(_get_indices(tree_or_filename)),\n                    name=alias,\n                    ignore=ignore,\n                )\n\n        for result in _create(self.active_aliases):\n            yield result\n\n    def put_templates(self, ignore=None):\n        ignore = ignore or []\n\n        def _put_template(template):\n            with open(self.templates[template], 'r') as body:\n                return self.templates[template],\\\n                    current_search_client.indices.put_template(\n                        name=template,\n                        body=json.load(body),\n                        ignore=ignore,\n                )\n\n        for template in self.templates:\n            yield _put_template(template)\n\n    def delete(self, ignore=None):\n        ignore = ignore or []\n\n        def _delete(tree_or_filename, alias=None):\n            if alias:\n                yield alias, self.client.indices.delete_alias(\n                    index=list(_get_indices(tree_or_filename)),\n                    name=alias,\n                    ignore=ignore,\n                )\n\n            for name, value in tree_or_filename.items():\n                if isinstance(value, dict):\n                    for result in _delete(value, alias=name):\n                        yield result\n                else:\n                    yield name, self.client.indices.delete(\n                        index=name,\n                        ignore=ignore,\n                    )\n\n        for result in _delete(self.active_aliases):\n            yield result",
    "docstring": "Store connection to elastic client and registered indexes."
}
{
    "class": "class Judge(metaclass=ABCMeta):\n\n    def __init__(self, * , judge_exe, compile_code_exe, problem_id,\n                 work_dir, compiler_name, source_code, sample_num,\n                 mem_limit, time_limit):\n        self.judge_exe = judge_exe\n        self.compile_code_exe = compile_code_exe\n        self.problem_id = str(problem_id)\n        self.work_dir = work_dir\n        self.compiler_name = compiler_name\n        self.source_code = source_code\n        self.sample_num = sample_num\n        self.mem_limit = str(mem_limit)\n        self.time_limit = str(time_limit)\n\n    def compile(self):\n        self.exe_file = os.path.join(self.work_dir, \"a.out\")\n        self.err_log = os.path.join(self.work_dir, \"compile_err_msg\")\n\n        compiler = COMPILERS.get(self.compiler_name, -1)\n\n        proc = subprocess.Popen([self.compile_code_exe, self.source_code,\n                                 self.exe_file, str(compiler), self.err_log],\n                                stdout=subprocess.PIPE)\n        try:\n            compile_result = proc.wait(timeout=10)\n        except subprocess.TimeoutExpired:\n            os.killpg(proc.pid, signal.SIGKILL)\n            proc.wait()\n            compile_result = COMPILE_ERROR\n        except:\n            os.killpg(proc.pid, signal.SIGKILL)\n            proc.wait()\n            compile_result = COMPILE_ERROR\n        return compile_result\n\n    def run(self):\n        if self.compile() == COMPILE_OK:\n            results = (COMPILE_OK, self.run_samples())\n        else:\n            results = (COMPILE_ERROR, self.get_compile_err_msg())\n        return results\n\n    def try_decode(self, data):\n        try:\n            s = data.decode(\"utf-8\").replace('\\x00', '')\n        except:\n            s= \"\"\n        return s\n\n    def get_compile_err_msg(self):\n        if os.path.exists(self.err_log):\n            with open(self.err_log, \"rb\") as f:\n                err_msg =  self.try_decode(f.read(MAX_ERROR_MSG_LENGTH))\n        else:\n            err_msg = \"\"\n        return err_msg\n\n    def run_samples(self):\n        results = [self.run_one_sample(i) for i in range(self.sample_num)]\n        return results\n\n    @abstractmethod\n    def get_test_input_by_id(self, id):\n        return\n\n    def get_sample_output_by_id(self, id):\n        return os.path.join(self.work_dir, \"out%s\" % id)\n\n    @abstractmethod\n    def get_std_answer_by_id(self, id):\n        return\n\n    @abstractmethod\n    def check_answer(self, std_answer, submit_output):\n        return\n\n    def run_one_sample(self, id):\n        test_input = self.get_test_input_by_id(id)\n        sample_output = self.get_sample_output_by_id(id)\n\n        proc = subprocess.Popen([self.judge_exe, self.exe_file,\n                                 test_input, sample_output,\n                                 self.time_limit, self.mem_limit],\n                                stdout=subprocess.PIPE)\n\n        try:\n            result = proc.communicate(timeout=3*int(self.time_limit))[0].decode(\"utf8\")\n            final_result, time_used, mem_used = [int(s) for s in result.split(',')]\n        except subprocess.TimeoutExpired:\n            os.killpg(proc.pid, signal.SIGKILL)\n            proc.wait()\n            raise NeedRejudgeError(\"Timeout for communicating\")\n        except:\n            os.killpg(proc.pid, signal.SIGKILL)\n            proc.wait()\n            raise NeedRejudgeError(\"Other Reasons\")\n\n        if final_result == TASK_ALL_NORMAL:\n            try:\n                std_answer = self.get_std_answer_by_id(id)\n                if self.check_answer(std_answer, sample_output):\n                    return (ACCEPTED, time_used, mem_used)\n                else:\n                    return (WRONG_ANSWER, 0, 0)\n            except:\n                return (WRONG_ANSWER, 0, 0)\n        else:\n            status = TASK_STATUS.get(final_result, -1)\n            return (status, 0, 0)",
    "docstring": "This class implements the Python API for the C lib lambdaOJ2.\n    And this class has 5 abstractmethod, to use it you have to\n    inherit Judge, and implement these abstractmethod.\n\n    * check_answer: (str * str) -> bool\n    * get_test_input_by_id: int -> str\n    * get_std_answer_by_id: int -> str\n\n    The method run is to check submitted code, the return data of\n    run is alawys a 2-element tuple:\n    * (COMPILE_OK : integer, [(status : integer,\n                               time_used : integer, in ms,\n                               mem_used : integer, in KBytes)])\n    * (COMPILE_ERROR: integer, error_message : string)\n\n    The object can only by initialized by key-word parameters:\n      judge_exe: string(full path of judge executable file),\n      compile_code_exe: string(full path of compile_code executable file),\n      problem_id: string(can be used to determine the input and std_answer path),\n      work_dir: string(full path of the temp dir for judging),\n      compiler_name: string, can only be one of keys of .consts.COMPILERS,\n      source_code: string, (full path of the code submitted),\n      sample_num: integer(the number of total test sample),\n      mem_limit: integer(the memory limit, in KBytes),\n      time_limit: integer(the time limit, in Seconds)"
}
{
    "class": "class DropOutLayer():\n\t'This is a class for do drop out operation to reduce overfitting in training'\n\tdef __init__(self):\n\t\tself.matrix = None\n\t\tself.dropout_value = None\n\t\tself.diamention = None\n\n\tdef dropout(self, matrix, dropout_value=0.6):\n\t\tself.matrix = matrix\n\t\tself.dropout_value = dropout_value\n\n\t\tself.diamention = self.matrix.shape\n\n\t\tmask = (np.random.random(self.diamention) < self.dropout_value)\n\n\t\toutput_matrix = self.matrix * mask\n\t\treturn output_matrix",
    "docstring": "This is a class for do drop out operation to reduce overfitting in training"
}
{
    "class": "class EnergyEntry(BASE):\n\n    __tablename__ = \"energy\"\n\n    id = Column(Integer, primary_key=True)\n    n1d = Column(Integer, nullable=False)\n    epsilon = Column(Float, nullable=False)\n    nstep = Column(Integer, nullable=True)\n    mass = Column(Float, nullable=False)\n    E = Column(Float, nullable=False)\n    nlevel = Column(Integer, nullable=False)\n    date = Column(DateTime, default=datetime.utcnow)\n    comment = Column(String, nullable=True)\n    discriminator = Column(\"type\", String(50))\n\n    __mapper_args__ = {\"polymorphic_on\": discriminator}\n    _keys: Set[str] = {\"n1d\", \"epsilon\", \"nstep\", \"mass\", \"E\", \"nlevel\"}\n\n    @classmethod\n    def keys(cls) -> Set[str]:\n        return cls._keys\n\n    @property\n    def L(self) -> float:\n        return self.n1d * self.epsilon\n\n    def mu(self) -> float:\n        return self.mass / 2\n\n    @property\n    def x(self) -> float:\n        r\"\"\"Returns $2 mu E L^2 / 4 \\pi^2$\n\n        Arguments:\n            session:\n                The database session for querying and adding entries.\n\n            kwargs:\n                EnergyEntry creagion arguments.\n        return str(self)\n\n    def __str__(self):\n        keys = \", \".join([f\"{key}={getattr(self, key)}\" for key in self.keys()])\n        return f\"{self.__class__.__name__}({keys})\"",
    "docstring": "Base implementation of energy table entries"
}
{
    "class": "class StudentsT(BaseSingleModel):\n\n    def __init__(self, mean=None, scale=None, dof=None):\n        super().__init__(loc=mean, scale=scale)\n        self._dof = dof\n\n    @property\n    def mean(self):\n        return self.location\n\n    @mean.setter\n    def mean(self, val):\n        self.location = val\n\n    @property\n    def degrees_of_freedom(self):\n        return self._dof\n\n    @degrees_of_freedom.setter\n    def degrees_of_freedom(self, value):\n        self._dof = value\n\n    @property\n    def covariance(self):\n        if self._dof <= 2:\n            msg = 'Degrees of freedom is {} and must be > 2'\n            raise RuntimeError(msg.format(self._dof))\n        return self._dof / (self._dof - 2) * self.scale\n\n    @covariance.setter\n    def covariance(self, val):\n        warn('Covariance is read only.')\n\n    def pdf(self, x):\n        rv = stats.multivariate_t\n        return rv.pdf(x.flatten(), loc=self.location.flatten(), shape=self.scale,\n                      df=self.degrees_of_freedom)\n\n    def sample(self, rng=None):\n        if rng is None:\n            rng = rnd.default_rng()\n\n        rv = stats.multivariate_t\n        rv.random_state = rng\n        x = rv.rvs(loc=self.location.flatten(),\n                   shape=self.scale, df=self.degrees_of_freedom)\n\n        return x.reshape((x.size, 1))",
    "docstring": "Represents a Student's t-distribution."
}
{
    "class": "class ModalView(AnchorLayout):\n\n    auto_dismiss = BooleanProperty(True)\n\n    attach_to = ObjectProperty(None, deprecated=True)\n\n    background_color = ColorProperty([1, 1, 1, 1])\n\n    background = StringProperty(\n        'atlas://data/images/defaulttheme/modalview-background')\n\n    border = ListProperty([16, 16, 16, 16])\n\n    overlay_color = ColorProperty([0, 0, 0, .7])\n\n\n    _anim_alpha = NumericProperty(0)\n\n    _anim_duration = NumericProperty(.1)\n\n    _window = ObjectProperty(allownone=True, rebind=True)\n\n    _is_open = BooleanProperty(False)\n\n    _touch_started_inside = None\n\n    __events__ = ('on_pre_open', 'on_open', 'on_pre_dismiss', 'on_dismiss')\n\n    def __init__(self, **kwargs):\n        self._parent = None\n        super(ModalView, self).__init__(**kwargs)\n\n    def open(self, *_args, **kwargs):\n        from kivy.core.window import Window\n        if self._is_open:\n            return\n        self._window = Window\n        self._is_open = True\n        self.dispatch('on_pre_open')\n        Window.add_widget(self)\n        Window.bind(\n            on_resize=self._align_center,\n            on_keyboard=self._handle_keyboard)\n        self.center = Window.center\n        self.fbind('center', self._align_center)\n        self.fbind('size', self._align_center)\n        if kwargs.get('animation', True):\n            ani = Animation(_anim_alpha=1., d=self._anim_duration)\n            ani.bind(on_complete=lambda *_args: self.dispatch('on_open'))\n            ani.start(self)\n        else:\n            self._anim_alpha = 1.\n            self.dispatch('on_open')\n\n    def dismiss(self, *_args, **kwargs):\n        if not self._is_open:\n            return\n        self.dispatch('on_pre_dismiss')\n        if self.dispatch('on_dismiss') is True:\n            if kwargs.get('force', False) is not True:\n                return\n        if kwargs.get('animation', True):\n            Animation(_anim_alpha=0., d=self._anim_duration).start(self)\n        else:\n            self._anim_alpha = 0\n            self._real_remove_widget()\n\n    def _align_center(self, *_args):\n        if self._is_open:\n            self.center = self._window.center\n\n    def on_touch_down(self, touch):\n        self._touch_started_inside = self.collide_point(*touch.pos)\n        if not self.auto_dismiss or self._touch_started_inside:\n            super().on_touch_down(touch)\n        return True\n\n    def on_touch_move(self, touch):\n        if not self.auto_dismiss or self._touch_started_inside:\n            super().on_touch_move(touch)\n        return True\n\n    def on_touch_up(self, touch):\n        if self.auto_dismiss and self._touch_started_inside is False:\n            self.dismiss()\n        else:\n            super().on_touch_up(touch)\n        self._touch_started_inside = None\n        return True\n\n    def on__anim_alpha(self, _instance, value):\n        if value == 0 and self._is_open:\n            self._real_remove_widget()\n\n    def _real_remove_widget(self):\n        if not self._is_open:\n            return\n        self._window.remove_widget(self)\n        self._window.unbind(\n            on_resize=self._align_center,\n            on_keyboard=self._handle_keyboard)\n        self._is_open = False\n        self._window = None\n\n    def on_pre_open(self):\n\n    def on_open(self):\n\n    def on_pre_dismiss(self):\n\n    def on_dismiss(self):\n\n    def _handle_keyboard(self, _window, key, *_args):\n        if key == 27 and self.auto_dismiss:\n            self.dismiss()\n            return True",
    "docstring": "ModalView class. See module documentation for more information.\n\n    :Events:\n        `on_pre_open`:\n            Fired before the ModalView is opened. When this event is fired\n            ModalView is not yet added to window.\n        `on_open`:\n            Fired when the ModalView is opened.\n        `on_pre_dismiss`:\n            Fired before the ModalView is closed.\n        `on_dismiss`:\n            Fired when the ModalView is closed. If the callback returns True,\n            the dismiss will be canceled.\n\n    .. versionchanged:: 1.11.0\n        Added events `on_pre_open` and `on_pre_dismiss`.\n\n    .. versionchanged:: 2.0.0\n        Added property 'overlay_color'.\n\n    .. versionchanged:: 2.1.0\n        Marked `attach_to` property as deprecated."
}
{
    "class": "class RandomForestClassifier(ForestClassifier, DiffprivlibMixin):\n    r\"\"\"Random Forest Classifier with differential privacy.\n\n    This class implements Differentially Private Random Decision Forests using Smooth Sensitivity [1].\n    :math:`\\epsilon`-Differential privacy is achieved by constructing decision trees via random splitting criterion and\n    applying Exponential Mechanism to produce a noisy label.\n\n    Parameters\n    ----------\n    n_estimators: int, default: 10\n        The number of trees in the forest.\n\n    epsilon: float, default: 1.0\n        Privacy parameter :math:`\\epsilon`.\n\n    cat_feature_threshold: int, default: 10\n        Threshold value used to determine categorical features. For example, value of ``10`` means\n        any feature that has less than or equal to 10 unique values will be treated as a categorical feature.\n\n    n_jobs : int, default: 1\n        Number of CPU cores used when parallelising over classes. ``-1`` means\n        using all processors.\n\n    verbose : int, default: 0\n        Set to any positive number for verbosity.\n\n    accountant : BudgetAccountant, optional\n        Accountant to keep track of privacy budget.\n\n    max_depth: int, default: 15\n        The maximum depth of the tree. Final depth of the tree will be calculated based on the number of continuous\n        and categorical features, but it wont be more than this number.\n        Note: The depth translates to an exponential increase in memory usage.\n\n    random_state: float, optional\n        Sets the numpy random seed.\n\n    feature_domains: dict, optional\n        A dictionary of domain values for all features where keys are the feature indexes in the training data and\n        the values are an array of domain values for categorical features and an array of min and max values for\n        continuous features. For example, if the training data is [[2, 'dog'], [5, 'cat'], [7, 'dog']], then\n        the feature_domains would be {'0': [2, 7], '1': ['dog', 'cat']}. If not provided, feature domains will\n        be constructed from the data, but this will result in :class:`.PrivacyLeakWarning`.\n\n    Attributes\n    ----------\n    n_features_in_: int\n        The number of features when fit is performed.\n\n    n_classes_: int\n        The number of classes.\n\n    classes_: array of shape (n_classes, )\n        The classes labels.\n\n    cat_features_: array of categorical feature indexes\n        Categorical feature indexes.\n\n    max_depth_: int\n        Final max depth used for constructing decision trees.\n\n    estimators_: list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    feature_domains_: dictionary of domain values mapped to feature\n        indexes in the training data\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from diffprivlib.models import RandomForestClassifier\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n\n    References\n    ----------\n    [1] Sam Fletcher, Md Zahidul Islam. \"Differentially Private Random Decision Forests using Smooth Sensitivity\"\n    https://arxiv.org/abs/1606.03572\n\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and n_features is the number of features.\n\n        y : array-like, shape (n_samples,)\n            Target vector relative to X.\n\n        sample_weight : ignored\n            Ignored by diffprivlib.  Present for consistency with sklearn API.\n\n        Returns\n        -------\n        self: class\n",
    "docstring": "r\"\"\"Random Forest Classifier with differential privacy.\n\n    This class implements Differentially Private Random Decision Forests using Smooth Sensitivity [1].\n    :math:`\\epsilon`-Differential privacy is achieved by constructing decision trees via random splitting criterion and\n    applying Exponential Mechanism to produce a noisy label.\n\n    Parameters\n    ----------\n    n_estimators: int, default: 10\n        The number of trees in the forest.\n\n    epsilon: float, default: 1.0\n        Privacy parameter :math:`\\epsilon`.\n\n    cat_feature_threshold: int, default: 10\n        Threshold value used to determine categorical features. For example, value of ``10`` means\n        any feature that has less than or equal to 10 unique values will be treated as a categorical feature.\n\n    n_jobs : int, default: 1\n        Number of CPU cores used when parallelising over classes. ``-1`` means\n        using all processors.\n\n    verbose : int, default: 0\n        Set to any positive number for verbosity.\n\n    accountant : BudgetAccountant, optional\n        Accountant to keep track of privacy budget.\n\n    max_depth: int, default: 15\n        The maximum depth of the tree. Final depth of the tree will be calculated based on the number of continuous\n        and categorical features, but it wont be more than this number.\n        Note: The depth translates to an exponential increase in memory usage.\n\n    random_state: float, optional\n        Sets the numpy random seed.\n\n    feature_domains: dict, optional\n        A dictionary of domain values for all features where keys are the feature indexes in the training data and\n        the values are an array of domain values for categorical features and an array of min and max values for\n        continuous features. For example, if the training data is [[2, 'dog'], [5, 'cat'], [7, 'dog']], then\n        the feature_domains would be {'0': [2, 7], '1': ['dog', 'cat']}. If not provided, feature domains will\n        be constructed from the data, but this will result in :class:`.PrivacyLeakWarning`.\n\n    Attributes\n    ----------\n    n_features_in_: int\n        The number of features when fit is performed.\n\n    n_classes_: int\n        The number of classes.\n\n    classes_: array of shape (n_classes, )\n        The classes labels.\n\n    cat_features_: array of categorical feature indexes\n        Categorical feature indexes.\n\n    max_depth_: int\n        Final max depth used for constructing decision trees.\n\n    estimators_: list of DecisionTreeClassifier\n        The collection of fitted sub-estimators.\n\n    feature_domains_: dictionary of domain values mapped to feature\n        indexes in the training data\n\n    Examples\n    --------\n    >>> from sklearn.datasets import make_classification\n    >>> from diffprivlib.models import RandomForestClassifier\n    >>> X, y = make_classification(n_samples=1000, n_features=4,\n    ...                            n_informative=2, n_redundant=0,\n    ...                            random_state=0, shuffle=False)\n    >>> clf = RandomForestClassifier(n_estimators=100, random_state=0)\n    >>> clf.fit(X, y)\n    >>> print(clf.predict([[0, 0, 0, 0]]))\n    [1]\n\n    References\n    ----------\n    [1] Sam Fletcher, Md Zahidul Islam. \"Differentially Private Random Decision Forests using Smooth Sensitivity\"\n    https://arxiv.org/abs/1606.03572"
}
{
    "class": "class MeetupEnrollInviteUsersAdd(APIView):\n\n    authentication_classes = [\n        SessionAuthentication,\n        BasicAuthentication,\n        TokenAuthentication,\n    ]\n    permission_classes = (IsAuthenticated,)\n\n    serializer_class = MeetupEnrollInviteUsersSerializers\n\n    def get(self, request, format=None):\n        user_id = self.request.GET.get('user_id', -1)\n\n        if int(user_id) > -1:\n            meetup = MeetupEnrollInviteUsers.objects.filter(\n                user__id=user_id,\n                user_check_in=False\n            )\n        else:\n            meetup = MeetupEnrollInviteUsers.objects.all()\n\n        serializer = MeetupEnrollInviteUsersSerializers(meetup, many=True)\n        return Response(serializer.data)\n\n    def post(self, request, format=None):\n        serializer = MeetupEnrollInviteUsersSerializers(data=request.data)\n\n        try:\n            if serializer.is_valid():\n                serializer.save()\n                self.__registre_notification(request, serializer.data)\n                return Response(serializer.initial_data, status=status.HTTP_201_CREATED)\n        except Exception as error:\n            errors = error.args[0]\n            return Response({'error': errors}, content_type=\"application/json\", status=status.HTTP_400_BAD_REQUEST)\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n\n    def __registre_notification(self, request, trans):\n\n        if request.user.is_superuser:\n            text = f\"Lo han inviatado a la meetup: {trans['meetup_name']}, el cual es en la fecha {trans['meetup_date']} \"\n            user = User.objects.get(pk=trans['user'])\n            notification = RegisterNotification(user, text)\n            notification.register_notifiaction()\n\n        else:\n            meetup = Meetup.objects.get(pk=trans['meetup'])\n            user = User.objects.get(pk=meetup.user.pk)\n            text = f\"El usuario name: {request.user.name}, email: {request.user.email} se registro en la  meetup {trans['meetup_name']}\"\n\n            notification = RegisterNotification(user, text)\n            notification.register_notifiaction()",
    "docstring": "``GET`` lists all  Enroll or Invite Users in the Meetup\n\n        ``POST`` Generates a request to Enroll or Invite a user to a Meetup\n\n         see :doc:`Flexible Security Framework `.\n\n         **Example request**:\n\n        .. code-block:: http\n\n            GET  api/meetup_enroll_invite_users/list/\n\n        **Example response**:\n\n        .. code-block:: json\n\n             [\n                {\n                    \"id\": 30,\n                    \"user\": 2,\n                    \"user_name\": \"test\",\n                    \"user_email\": \"test@gmail.com\",\n                    \"meetup\": 1,\n                    \"meetup_name\": \"hola\",\n                    \"meetup_date\": \"2020-10-28T06:00:00Z\",\n                    \"user_check_in\": false\n                }\n            ]\n\n        .. code-block:: http\n\n            POST  api/meetup_enroll_invite_users/create/\n\n            **Example response**:\n\n            .. code-block:: json parameter required\n\n                 {\n                    \"user\" : 2,\n                    \"meetup\" : 1\n\n                }"
}
{
    "class": "class SaveState(object):\n    def __init__(self, outliner):\n        self.outliner = outliner\n        editTarget = outliner.GetEditTargetLayer()\n        self.origLayerContents = \\\n            {self.GetId(editTarget): self._GetDiskContents(editTarget)}\n        self._listener = Tf.Notice.Register(Usd.Notice.StageEditTargetChanged,\n                                            self._OnEditTargetChanged,\n                                            outliner.stage)\n\n    def _OnEditTargetChanged(self, notice, stage):\n        layer = stage.GetEditTarget().GetLayer()\n        self.origLayerContents.setdefault(self.GetId(layer),\n                                          layer.ExportToString())\n\n    def GetOriginalContents(self, layer):\n        return self.origLayerContents[self.GetId(layer)]\n\n    def SaveOriginalContents(self, layer, contents=None):\n        if not contents:\n            contents = layer.ExportToString()\n        self.origLayerContents[self.GetId(layer)] = contents\n\n    def _GetDiskContents(self, layer):\n\n        if not layer.realPath:\n            return None\n\n        currentContents = layer.ExportToString()\n        layer.Reload()\n        diskContents = layer.ExportToString()\n        if diskContents != currentContents:\n            layer.ImportFromString(currentContents)\n        return diskContents\n\n    def CheckOriginalContents(self, editLayer):\n        import difflib\n\n        diskContents = self._GetDiskContents(editLayer)\n        originalContents = self.GetOriginalContents(editLayer)\n        if originalContents and originalContents != diskContents:\n            diff = difflib.unified_diff(originalContents.split('\\n'),\n                                        diskContents.split('\\n'),\n                                        fromfile=\"original\",\n                                        tofile=\"on disk\",\n                                        n=10)\n            dlg = QtWidgets.QMessageBox(\n                QtWidgets.QMessageBox.Warning,\n                'Layer Contents Changed!',\n                'Layer contents have changed on disk since you started '\n                'editing.\\n    %s\\n'\n                'Save anyway and risk overwriting changes?' % editLayer.identifier,\n                buttons=QtWidgets.QMessageBox.No | QtWidgets.QMessageBox.Yes,\n                detailedText='\\n'.join(diff))\n            if dlg.exec_() != QtWidgets.QMessageBox.Yes:\n                return False\n        return True\n\n    def GetId(self, layer):\n        return UsdQtHooks.Call('GetId', layer)",
    "docstring": "State tracker for layer contents in an outliner app"
}
{
    "class": "class dataset_kitti2012_test(dataset_kitti):\n    def __init__(self, root):\n        super(dataset_kitti2012_test, self).__init__(root)\n        self.name = 'kitti2012-te'\n\n        self.flag_img_left_right = ['colored_0', 'colored_1']\n\n        flag = 'data_stereo_flow/testing/colored_0/*_10.png'\n        self.str_filter = os.path.join(root, flag)\n        self.get_paths_all(self.str_filter, flag_sort=True)",
    "docstring": "dataset_kitti2012_test"
}
{
    "class": "class BashJob(Job):\n\n    def __init__(self,\n                 batch: 'batch.Batch',\n                 token: str,\n                 *,\n                 name: Optional[str] = None,\n                 attributes: Optional[Dict[str, str]] = None,\n                 shell: Optional[str] = None):\n        super().__init__(batch, token, name=name, attributes=attributes, shell=shell)\n        self._command: List[str] = []\n\n    def _get_resource(self, item: str) -> '_resource.Resource':\n        if item not in self._resources:\n            r = self._batch._new_job_resource_file(self, value=item)\n            self._resources[item] = r\n            self._resources_inverse[r] = item\n\n        return self._resources[item]\n\n    def declare_resource_group(self, **mappings: Dict[str, Any]) -> 'BashJob':\n\n        for name, d in mappings.items():\n            assert name not in self._resources\n            if not isinstance(d, dict):\n                raise BatchException(f\"value for name '{name}' is not a dict. Found '{type(d)}' instead.\")\n            rg = self._batch._new_resource_group(self, d, root=name)\n            self._resources[name] = rg\n            _add_resource_to_set(self._valid, rg)\n        return self\n\n    def image(self, image: str) -> 'BashJob':\n\n        self._image = image\n        return self\n\n    def command(self, command: str) -> 'BashJob':\n\n        command = self._interpolate_command(command)\n        self._command.append(command)\n        return self\n\n    async def _compile(self, local_tmpdir, remote_tmpdir, *, dry_run=False):\n        if len(self._command) == 0:\n            return False\n\n        job_shell = self._shell if self._shell else DEFAULT_SHELL\n\n        job_command = [cmd.strip() for cmd in self._command]\n        job_command = [f'{{\\n{x}\\n}}' for x in job_command]\n        job_command = '\\n'.join(job_command)\n\n        job_command = f'''\n{job_command}\nchmod u+x {code}\nsource {code}",
    "docstring": "Object representing a single bash job to execute.\n\n    Examples\n    --------\n\n    Create a batch object:\n\n    >>> b = Batch()\n\n    Create a new bash job that prints hello to a temporary file `t.ofile`:\n\n    >>> j = b.new_job()\n    >>> j.command(f'echo \"hello\" > {j.ofile}')\n\n    Write the temporary file `t.ofile` to a permanent location\n\n    >>> b.write_output(j.ofile, 'hello.txt')\n\n    Execute the DAG:\n\n    >>> b.run()\n\n    Notes\n    -----\n    This class should never be created directly by the user. Use :meth:`.Batch.new_job`\n    or :meth:`.Batch.new_bash_job` instead."
}
{
    "class": "class LocationMetadata(_messages.Message):\n  r\"\"\"Cloud KMS metadata for the given google.cloud.location.Location.\n\n  Fields:\n    hsmAvailable: Indicates whether CryptoKeys with protection_level HSM can\n      be created in this location.",
    "docstring": "r\"\"\"Cloud KMS metadata for the given google.cloud.location.Location.\n\n  Fields:\n    hsmAvailable: Indicates whether CryptoKeys with protection_level HSM can\n      be created in this location."
}
{
    "class": "class MyClass(object):\n\n\n    def __init__(self):\n\n    def __call__(self):\n        print(\"__call__ called\")\n        pass\n\n    def __getattr__(self, method):\n        print(\"call\", method)\n        return lambda *args, **kwargs: 100",
    "docstring": "Docstring for MyClass."
}
{
    "class": "class DifferentiableRigidBody(torch.nn.Module):\n\n    def __init__(self, rigid_body_params, device=\"cpu\"):\n\n        super().__init__()\n\n        self._device = device\n        self.joint_id = rigid_body_params[\"joint_id\"]\n        self.name = rigid_body_params[\"link_name\"]\n\n        self.joint_damping = rigid_body_params[\"joint_damping\"]\n        self.inertia = DifferentiableSpatialRigidBodyInertia(rigid_body_params)\n\n        self.trans = rigid_body_params[\"trans\"]\n        self.rot_angles = rigid_body_params[\"rot_angles\"]\n        self.joint_limits = rigid_body_params[\"joint_limits\"]\n\n        self.joint_axis = rigid_body_params[\"joint_axis\"]\n\n        self.joint_pose = CoordinateTransform()\n        self.joint_pose.set_translation(torch.reshape(self.trans, (1, 3)))\n\n        self.joint_vel = SpatialMotionVec()\n        self.joint_acc = SpatialMotionVec()\n\n        self.update_joint_state(torch.zeros(1, 1), torch.zeros(1, 1))\n        self.update_joint_acc(torch.zeros(1, 1))\n\n        self.pose = CoordinateTransform()\n\n        self.vel = SpatialMotionVec()\n        self.acc = SpatialMotionVec()\n\n        self.force = SpatialForceVec()\n\n        return\n\n    def update_joint_state(self, q, qd):\n        batch_size = q.shape[0]\n\n        joint_ang_vel = qd @ self.joint_axis\n        self.joint_vel = SpatialMotionVec(torch.zeros_like(joint_ang_vel), joint_ang_vel)\n\n        roll = self.rot_angles[0]\n        pitch = self.rot_angles[1]\n        yaw = self.rot_angles[2]\n\n        fixed_rotation = (z_rot(yaw) @ y_rot(pitch)) @ x_rot(roll)\n\n        self.joint_pose.set_translation(torch.reshape(self.trans, (1, 3)))\n        if self.joint_axis[0, 0] == 1:\n            rot = x_rot(q)\n        elif self.joint_axis[0, 1] == 1:\n            rot = y_rot(q)\n        else:\n            rot = z_rot(q)\n\n        self.joint_pose.set_rotation(fixed_rotation.repeat(batch_size, 1, 1) @ rot)\n        return\n\n    def update_joint_state_fb(self, q, qd):\n        batch_size = q.shape[0]\n\n        joint_ang_vel = qd @ self.joint_axis\n        self.joint_vel = SpatialMotionVec(torch.zeros_like(joint_ang_vel), joint_ang_vel)\n\n        roll = self.rot_angles[0]\n        pitch = self.rot_angles[1]\n        yaw = self.rot_angles[2]\n\n        fixed_rotation = (z_rot(yaw) @ y_rot(pitch)) @ x_rot(roll)\n\n        self.joint_pose.set_translation(torch.reshape(self.trans, (1, 3)))\n\n        for i in range(batch_size):\n            rot[i] = z_rot(q[2]) @ y_rot(q[1]) @ x_rot[q[0]]g\n\n        self.joint_pose.set_rotation(fixed_rotation.repeat(batch_size, 1, 1) @ rot)\n        return\n\n    def update_joint_acc(self, qdd):\n        joint_ang_acc = qdd @ self.joint_axis\n        self.joint_acc = SpatialMotionVec(torch.zeros_like(joint_ang_acc), joint_ang_acc)\n        return\n\n    def get_joint_limits(self):\n        return self.joint_limits\n\n    def get_joint_damping_const(self):\n        return self.joint_damping",
    "docstring": "Differentiable Representation of a link"
}
{
    "class": "class AtomicResultProperties(ProtoModel):\n\n    calcinfo_nbasis: Optional[int] = Field(None, description=\"The number of basis functions for the computation.\")\n    calcinfo_nmo: Optional[int] = Field(None, description=\"The number of molecular orbitals for the computation.\")\n    calcinfo_nalpha: Optional[int] = Field(None, description=\"The number of alpha electrons in the computation.\")\n    calcinfo_nbeta: Optional[int] = Field(None, description=\"The number of beta electrons in the computation.\")\n    calcinfo_natom: Optional[int] = Field(None, description=\"The number of atoms in the computation.\")\n\n    nuclear_repulsion_energy: Optional[float] = Field(None, description=\"The nuclear repulsion energy energy.\")\n    return_energy: Optional[float] = Field(\n        None, description=\"The energy of the requested method, identical to `return_value` for energy computations.\"\n    )\n\n    scf_one_electron_energy: Optional[float] = Field(\n        None, description=\"The one-electron (core Hamiltonian) energy contribution to the total SCF energy.\"\n    )\n    scf_two_electron_energy: Optional[float] = Field(\n        None, description=\"The two-electron energy contribution to the total SCF energy.\"\n    )\n    scf_vv10_energy: Optional[float] = Field(\n        None, description=\"The VV10 functional energy contribution to the total SCF energy.\"\n    )\n    scf_xc_energy: Optional[float] = Field(\n        None, description=\"The functional (XC) energy contribution to the total SCF energy.\"\n    )\n    scf_dispersion_correction_energy: Optional[float] = Field(\n        None,\n        description=\"The dispersion correction appended to an underlying functional when a DFT-D method is requested.\",\n    )\n    scf_dipole_moment: Optional[Array[float]] = Field(None, description=\"The X, Y, and Z dipole components.\")\n    scf_quadrupole_moment: Optional[Array[float]] = Field(\n        None, description=\"The (3, 3) quadrupole components (redundant; 6 unique).\"\n    )\n    scf_total_energy: Optional[float] = Field(\n        None, description=\"The total electronic energy of the SCF stage of the calculation.\"\n    )\n    scf_iterations: Optional[int] = Field(None, description=\"The number of SCF iterations taken before convergence.\")\n\n    mp2_same_spin_correlation_energy: Optional[float] = Field(\n        None, description=\"The portion of MP2 doubles correlation energy from same-spin (i.e. triplet) correlations.\"\n    )\n    mp2_opposite_spin_correlation_energy: Optional[float] = Field(\n        None,\n        description=\"The portion of MP2 doubles correlation energy from opposite-spin (i.e. singlet) correlations.\",\n    )\n    mp2_singles_energy: Optional[float] = Field(\n        None, description=\"The singles portion of the MP2 correlation energy. Zero except in ROHF.\"\n    )\n    mp2_doubles_energy: Optional[float] = Field(\n        None,\n        description=\"The doubles portion of the MP2 correlation energy including same-spin and opposite-spin correlations.\",\n    )\n    mp2_total_correlation_energy: Optional[float] = Field(\n        None, description=\"The MP2 correlation energy.\"\n    )\n    mp2_correlation_energy: Optional[float] = Field(None, description=\"The MP2 correlation energy.\")\n    mp2_total_energy: Optional[float] = Field(\n        None, description=\"The total MP2 energy (MP2 correlation energy + HF energy).\"\n    )\n    mp2_dipole_moment: Optional[Array[float]] = Field(None, description=\"The MP2 X, Y, and Z dipole components.\")\n\n    ccsd_same_spin_correlation_energy: Optional[float] = Field(\n        None, description=\"The portion of CCSD doubles correlation energy from same-spin (i.e. triplet) correlations.\"\n    )\n    ccsd_opposite_spin_correlation_energy: Optional[float] = Field(\n        None,\n        description=\"The portion of CCSD doubles correlation energy from opposite-spin (i.e. singlet) correlations\",\n    )\n    ccsd_singles_energy: Optional[float] = Field(\n        None, description=\"The singles portion of the CCSD correlation energy. Zero except in ROHF.\"\n    )\n    ccsd_doubles_energy: Optional[float] = Field(\n        None,\n        description=\"The doubles portion of the CCSD correlation energy including same-spin and opposite-spin correlations.\",\n    )\n    ccsd_correlation_energy: Optional[float] = Field(None, description=\"The CCSD correlation energy.\")\n    ccsd_total_energy: Optional[float] = Field(\n        None, description=\"The total CCSD energy (CCSD correlation energy + HF energy).\"\n    )\n    ccsd_dipole_moment: Optional[Array[float]] = Field(None, description=\"The CCSD X, Y, and Z dipole components.\")\n    ccsd_iterations: Optional[int] = Field(None, description=\"The number of CCSD iterations taken before convergence.\")\n\n    ccsd_prt_pr_correlation_energy: Optional[float] = Field(None, description=\"The CCSD(T) correlation energy.\")\n    ccsd_prt_pr_total_energy: Optional[float] = Field(\n        None, description=\"The total CCSD(T) energy (CCSD(T) correlation energy + HF energy).\"\n    )\n    ccsd_prt_pr_dipole_moment: Optional[Array[float]] = Field(\n        None, description=\"The CCSD(T) X, Y, and Z dipole components.\"\n    )\n\n    class Config(ProtoModel.Config):\n        force_skip_defaults = True\n\n    def __repr_args__(self) -> \"ReprArgs\":\n        return [(k, v) for k, v in self.dict().items()]\n\n    @validator(\n        \"scf_dipole_moment\",\n        \"mp2_dipole_moment\",\n        \"ccsd_dipole_moment\",\n        \"ccsd_prt_pr_dipole_moment\",\n        \"scf_quadrupole_moment\",\n    )\n    def _validate_poles(cls, v, values, field):\n        if v is None:\n            return v\n\n        if field.name.endswith(\"_dipole_moment\"):\n            order = 1\n        elif field.name.endswith(\"_quadrupole_moment\"):\n            order = 2\n\n        shape = tuple([3] * order)\n        return np.asarray(v).reshape(shape)\n\n    def dict(self, *args, **kwargs):\n        kwargs[\"encoding\"] = \"json\"\n        return super().dict(*args, **kwargs)",
    "docstring": "Named properties of quantum chemistry computations following the MolSSI QCSchema."
}
{
    "class": "class TrainMeter(object):\n\n    def __init__(self, epoch_iters):\n        self.epoch_iters = epoch_iters\n        self.max_iter = cfg.OPTIM.MAX_EPOCH * epoch_iters\n        self.iter_timer = Timer()\n        self.loss = ScalarMeter(cfg.LOG_PERIOD)\n        self.loss_total = 0.0\n        self.lr = None\n        self.mb_top1_err = ScalarMeter(cfg.LOG_PERIOD)\n        self.mb_top5_err = ScalarMeter(cfg.LOG_PERIOD)\n        self.num_top1_mis = 0\n        self.num_top5_mis = 0\n        self.num_samples = 0\n\n    def reset(self, timer=False):\n        if timer:\n            self.iter_timer.reset()\n        self.loss.reset()\n        self.loss_total = 0.0\n        self.lr = None\n        self.mb_top1_err.reset()\n        self.mb_top5_err.reset()\n        self.num_top1_mis = 0\n        self.num_top5_mis = 0\n        self.num_samples = 0\n\n    def iter_tic(self):\n        self.iter_timer.tic()\n\n    def iter_toc(self):\n        self.iter_timer.toc()\n\n    def update_stats(self, loss, lr, mb_size, top1_err=None, top5_err=None):\n        self.mb_top1_err.add_value(top1_err)\n        self.mb_top5_err.add_value(top5_err)\n        self.loss.add_value(loss)\n        self.lr = lr\n        self.num_top1_mis += top1_err * mb_size\n        self.num_top5_mis += top5_err * mb_size\n        self.loss_total += loss * mb_size\n        self.num_samples += mb_size\n\n    def get_iter_stats(self, cur_epoch, cur_iter):\n        eta_sec = self.iter_timer.average_time * (\n            self.max_iter - (cur_epoch * self.epoch_iters + cur_iter + 1)\n        )\n        eta_td = datetime.timedelta(seconds=int(eta_sec))\n        mem_usage = metrics.gpu_mem_usage()\n        stats = {\n            '_type': 'train_iter',\n            'epoch': '{}/{}'.format(cur_epoch + 1, cfg.OPTIM.MAX_EPOCH),\n            'iter': '{}/{}'.format(cur_iter + 1, self.epoch_iters),\n            'eta': eta_str(eta_td),\n            'loss': self.loss.get_win_median(),\n            'top1_err': self.mb_top1_err.get_win_median(),\n            'top5_err': self.mb_top5_err.get_win_median(),\n            'lr': self.lr,\n            'time_avg': self.iter_timer.average_time,\n            'mem': int(np.ceil(mem_usage))\n        }\n        return stats\n\n    def log_iter_stats(self, cur_epoch, cur_iter):\n        if (cur_iter + 1) % cfg.LOG_PERIOD != 0:\n            return\n        stats = self.get_iter_stats(cur_epoch, cur_iter)\n        lu.log_json_stats(stats)\n\n    def get_epoch_stats(self, cur_epoch):\n        eta_sec = self.iter_timer.average_time * (\n            self.max_iter - (cur_epoch + 1) * self.epoch_iters\n        )\n        eta_td = datetime.timedelta(seconds=int(eta_sec))\n        mem_usage = metrics.gpu_mem_usage()\n        top1_err = self.num_top1_mis / self.num_samples\n        top5_err = self.num_top5_mis / self.num_samples\n        avg_loss = self.loss_total / self.num_samples\n        stats = {\n            '_type': 'train_epoch',\n            'epoch': '{}/{}'.format(cur_epoch + 1, cfg.OPTIM.MAX_EPOCH),\n            'eta': eta_str(eta_td),\n            'loss': avg_loss,\n            'top1_err': top1_err,\n            'top5_err': top5_err,\n            'lr': self.lr,\n            'time_avg': self.iter_timer.average_time,\n            'mem': int(np.ceil(mem_usage))\n        }\n        return stats\n\n    def log_epoch_stats(self, cur_epoch):\n        stats = self.get_epoch_stats(cur_epoch)\n        lu.log_json_stats(stats)",
    "docstring": "Measures training stats."
}
{
    "class": "class VieSelectBookmark(sublime_plugin.TextCommand):\n    def run(self, edit=None, character=None, select=True):\n        bookmarker = VieBookmarker(view=self.view)\n\n        bookmarker.go_to_mark(character=character, select=select)",
    "docstring": "Re-Selects a bookmark"
}
{
    "class": "class GridSample(Filter):\n\n    def __init__(self, config: GridSampleConfig, **kwargs):\n        super().__init__(config)\n\n    def filter(self, data_dict: dict):\n        assert isinstance(self.config, GridSampleConfig)\n\n        pc = data_dict[self.config.pointcloud_key]\n        assert_debug(isinstance(pc, np.ndarray), \"Cannot Distort a non numpy frame\")\n        check_sizes(pc, [-1, 3])\n\n        voxel_coords = voxelise(pc, self.config.voxel_size, self.config.voxel_size, self.config.voxel_size)\n        voxel_hashes = np.zeros((pc.shape[0]), dtype=np.int64)\n        voxel_hashing(voxel_coords, voxel_hashes)\n\n        sample, indices = grid_sample(pc, voxel_hashes)\n        data_dict[self.config.output_sample_key] = sample\n        data_dict[self.config.output_indices_key] = indices",
    "docstring": "Distort a frame using the estimated initial motion"
}
{
    "class": "class SecondQuartFeature(feature.Feature):\n\n    def __init__(self):\n        self.__points = None\n\n    def calculate(self) -> float:\n        if self.__points is None:\n            raise RuntimeError(\"Run prepare() before calculate()\")\n\n        count = 0\n        for x in self.__points:\n            if 0.25 < x <= 0.5:\n                count += 1\n        return count\n\n    def prepare(self, bitmap: bitmap_grayscale) -> None:\n        self.__points = super()._map_bitmap_to_single_dimention(bitmap)",
    "docstring": "Klasa oblicza .\n    Cecha 19."
}
{
    "class": "class RSD(object):\n    THIS_DIR = os.path.dirname(__file__) if os.path.dirname(__file__) else '.'\n    RSD_FILES = ['featurize.pl', 'process.pl', 'rules.pl']\n\n    CONSTRUCT = '_construct.pl'\n    SAVE = '_save.pl'\n    SUBGROUPS = '_subgroups.pl'\n\n    SCRIPTS = [CONSTRUCT, SAVE, SUBGROUPS]\n\n    ESSENTIAL_PARAMS = {\n        'clauselength' : 8,\n        'depth' : 4,\n        'negation' : 'none',\n        'min_coverage' : 1,\n        'filtering' : 'true'\n    }\n\n    def __init__(self, verbosity=logging.NOTSET):\n        self.tmpdir = tempfile.mkdtemp()\n        self.settings = dict()\n        logger.setLevel(verbosity)\n\n        for fn in RSD.RSD_FILES:\n            shutil.copy(\"%s/%s\" % (RSD.THIS_DIR, fn), self.tmpdir)\n\n    def set(self, name, value):\n        self.settings[name] = value\n\n    def settingsAsFacts(self, settings):\n        pattern = re.compile('set\\(([a-zA-Z0-9_]+),(\\[a-zA-Z0-9_]+)\\)')\n        pairs = pattern.findall(settings)\n        for name, val in pairs:\n            self.set(name, val)\n\n    def induce(self, b, filestem='default',\n               examples=None,\n               pos=None,\n               neg=None,\n               cn2sd=True,\n               printOutput=False):\n        self.__prepare(filestem, b, examples=examples, pos=pos, neg=neg)\n\n        self.__scripts(filestem)\n\n        dumpFile = None\n        if not printOutput:\n            dumpFile = tempfile.TemporaryFile()\n\n        logger.info(\"Running RSD...\")\n        try:\n            for script in RSD.SCRIPTS:\n                if script == RSD.SUBGROUPS and not cn2sd:\n                    continue\n                p = SafePopen(['yap', '-s50000', '-h200000', '-L', script],\n                              cwd=self.tmpdir,\n                              stdout=dumpFile,\n                              stderr=dumpFile).safe_run()\n                stdout_str, stderr_str = p.communicate()\n                logger.debug(stdout_str)\n                logger.debug(stderr_str)\n            logger.info(\"Done.\")\n\n            features = open('%s/%s' % (self.tmpdir, filestem + '_frs.pl')).read()\n            weka = open('%s/%s' % (self.tmpdir, filestem + '.arff')).read()\n            rules = open('%s/%s' % (self.tmpdir, filestem + '.rules')).read() if cn2sd else ''\n\n            self.__cleanup()\n            return (features, weka, rules)\n        except OSError:\n            raise RuntimeError(\"Yap compiler could not be loaded! (see http://www.dcc.fc.up.pt/~vsc/Yap/).\")\n\n    def __prepare(self, filestem, b, examples=None, pos=None, neg=None):\n        if examples:\n            examplesFile = open('%s/%s.pl' % (self.tmpdir, filestem), 'w')\n            examplesFile.write(examples)\n            examplesFile.close()\n        elif pos and neg:\n            posFile = open('%s/%s.f' % (self.tmpdir, filestem), 'w')\n            negFile = open('%s/%s.n' % (self.tmpdir, filestem), 'w')\n            posFile.write(pos)\n            negFile.write(neg)\n            posFile.close()\n            negFile.close()\n        else:\n            raise Exception('You need to provide either a single file of classified examples or \\\n                two files, positive and negative examples.')\n        bFile = open('%s/%s.b' % (self.tmpdir, filestem), 'w')\n        for setting, val in self.settings.items():\n            bFile.write(':- set(%s,%s).\\n' % (setting, val))\n        bFile.write(b)\n        bFile.close()\n\n    def __cleanup(self):\n        try:\n            shutil.rmtree(self.tmpdir)\n        except:\n            logger.info('Problem removing temporary files. The files are probably in use.')\n\n    def __scripts(self, filestem):\n        script_construct = open('%s/%s' % (self.tmpdir, RSD.CONSTRUCT), 'w')\n        script_save = open('%s/%s' % (self.tmpdir, RSD.SAVE), 'w')\n        script_subgroups = open('%s/%s' % (self.tmpdir, RSD.SUBGROUPS), 'w')\n\n        for fn in RSD.SCRIPTS:\n            os.chmod('%s/%s' % (self.tmpdir, fn), S_IREAD | S_IEXEC)\n\n        new_script = lambda script: lambda x: script.write(x + '\\n')\n\n        w = new_script(script_construct)\n        w(':- initialization(main).')\n        w('main :-')\n        w('[featurize],')\n        w('r(%s),' % filestem)\n        w('w.')\n        script_construct.close()\n\n        w = new_script(script_save)\n        w(':- initialization(main).')\n        w('main :-')\n        w('[process],')\n        w('r(%s),' % filestem)\n        w('w,')\n        w('w(weka, %s),' % filestem)\n        w('w(rsd, %s).' % filestem)\n        script_save.close()\n\n        w = new_script(script_subgroups)\n        w(':- initialization(main).')\n        w('main :-')\n        w('[rules],')\n        w('r(%s),' % filestem)\n        w('i,')\n        w('w.')\n        script_subgroups.close()",
    "docstring": "RSD python wrapper."
}
{
    "class": "class ExtractMedia():\n\n    def __init__(self, filename, image, detected_faces=None):\n        logger.trace(\"Initializing %s: (filename: '%s', image shape: %s, detected_faces: %s)\",\n                     self.__class__.__name__, filename, image.shape, detected_faces)\n        self._filename = filename\n        self._image = image\n        self._detected_faces = detected_faces\n\n    @property\n    def filename(self):\n        return self._filename\n\n    @property\n    def image(self):\n        return self._image\n\n    @property\n    def image_shape(self):\n        return self._image.shape\n\n    @property\n    def image_size(self):\n        return self._image.shape[:2]\n\n    @property\n    def detected_faces(self):\n        return self._detected_faces\n\n    def get_image_copy(self, color_format):\n        logger.trace(\"Requested color format '%s' for frame '%s'\", color_format, self._filename)\n        image = getattr(self, \"_image_as_{}\".format(color_format.lower()))()\n        return image\n\n    def add_detected_faces(self, faces):\n        logger.trace(\"Adding detected faces for filename: '%s'. (faces: %s, lrtb: %s)\",\n                     self._filename, faces,\n                     [(face.left, face.right, face.top, face.bottom) for face in faces])\n        self._detected_faces = faces\n\n    def remove_image(self):\n        logger.trace(\"Removing image for filename: '%s'\", self._filename)\n        del self._image\n        self._image = None\n\n    def set_image(self, image):\n        logger.trace(\"Reapplying image: (filename: `%s`, image shape: %s)\",\n                     self._filename, image.shape)\n        self._image = image\n\n    def _image_as_bgr(self):\n        return self._image[..., :3].copy()\n\n    def _image_as_rgb(self):\n        return self._image[..., 2::-1].copy()\n\n    def _image_as_gray(self):\n        return cv2.cvtColor(self._image.copy(), cv2.COLOR_BGR2GRAY)",
    "docstring": "An object that passes through the :class:`~plugins.extract.pipeline.Extractor` pipeline.\n\n    Parameters\n    ----------\n    filename: str\n        The base name of the original frame's filename\n    image: :class:`numpy.ndarray`\n        The original frame\n    detected_faces: list, optional\n        A list of :class:`~lib.faces_detect.DetectedFace` objects. Detected faces can be added\n        later with :func:`add_detected_faces`. Default: None"
}
{
    "class": "class Decoder(nn.Module):\n\n    def __init__(\n            self, n_tgt_vocab, n_layers=6, n_head=8,\n            d_word_vec=512, d_model=512, d_inner_hid=1024, dim_per_head=None, dropout=0.1,\n            positional_embedding=\"sin\", layer_norm_first=True, padding_idx=PAD, ffn_activation=\"relu\"):\n\n        super(Decoder, self).__init__()\n\n        self.n_head = n_head\n        self.num_layers = n_layers\n        self.d_model = d_model\n        self.layer_norm_first = layer_norm_first\n\n        self.embeddings = Embeddings(n_tgt_vocab, d_word_vec,\n                                     dropout=dropout,\n                                     positional_embedding=positional_embedding,\n                                     padding_idx=padding_idx\n                                     )\n\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(d_model=d_model, d_inner_hid=d_inner_hid, n_head=n_head, dropout=dropout,\n                         dim_per_head=dim_per_head, layer_norm_first=layer_norm_first, ffn_activation=ffn_activation)\n            for _ in range(n_layers)])\n\n        self.layer_norm = nn.LayerNorm(d_model)\n\n        self._dim_per_head = dim_per_head\n\n    @property\n    def dim_per_head(self):\n        if self._dim_per_head is None:\n            return self.d_model // self.n_head\n        else:\n            return self._dim_per_head\n\n    def forward(self, tgt_seq, enc_output, enc_mask, enc_attn_caches=None, self_attn_caches=None, sample_K=0,\n                seed=0):\n\n        batch_size, tgt_len = tgt_seq.size()\n\n        query_len = tgt_len\n        key_len = tgt_len\n\n        src_len = enc_output.size(1)\n\n        emb = self.embeddings(tgt_seq)\n\n        if not self.layer_norm_first:\n            emb = self.layer_norm(emb)\n\n        if self_attn_caches is not None:\n            emb = emb[:, -1:].contiguous()\n            query_len = 1\n\n        dec_slf_attn_pad_mask = tgt_seq.detach().eq(PAD).unsqueeze(1).expand(batch_size, query_len, key_len)\n        dec_slf_attn_sub_mask = get_attn_causal_mask(emb)\n\n        dec_slf_attn_mask = torch.gt(dec_slf_attn_pad_mask + dec_slf_attn_sub_mask, 0)\n        dec_enc_attn_mask = enc_mask.unsqueeze(1).expand(batch_size, query_len, src_len)\n\n        output = emb\n        new_self_attn_caches = []\n        new_enc_attn_caches = []\n        for i in range(self.num_layers):\n            if i == self.num_layers - 1:\n                layer_sample_K = sample_K\n            else:\n                layer_sample_K = 0\n\n            output, attn, self_attn_cache, enc_attn_cache \\\n                = self.layer_stack[i](output,\n                                      enc_output,\n                                      dec_slf_attn_mask,\n                                      dec_enc_attn_mask,\n                                      enc_attn_cache=enc_attn_caches[i] if enc_attn_caches is not None else None,\n                                      self_attn_cache=self_attn_caches[i] if self_attn_caches is not None else None,\n                                      sample_K=layer_sample_K, seed=seed)\n\n            new_self_attn_caches += [self_attn_cache]\n            new_enc_attn_caches += [enc_attn_cache]\n\n        if self.layer_norm_first:\n            output = self.layer_norm(output)\n\n        return output, new_self_attn_caches, new_enc_attn_caches",
    "docstring": "A decoder model with self attention mechanism."
}
{
    "class": "class CaseTesting(NamedTuple):\n    date: datetime.date\n    province: str\n    country: str\n    tested: int\n    under_investigation: int\n\n    def __add__(self, other: 'CaseTesting') -> 'CaseTesting':\n        if not isinstance(other, self.__class__):\n            raise TypeError('Must only add to another CaseTesting object.')\n\n        if self.date != other.date:\n            raise ValueError('Cannot add testing status for different dates.')\n\n        province = self.province if other.province == self.province else 'aggr'\n        country = self.country if other.country == self.country else 'aggr'\n        return CaseTesting(\n            date=self.date,\n            province=province,\n            country=country,\n            tested=self.tested + other.tested,\n            under_investigation=(self.under_investigation +\n                                 other.under_investigation)\n        )",
    "docstring": "Report the current testing level at some date.\n\n    Attributes\n    ----------\n    date : :class:`datetime.date`\n        date when cases where reported\n    province : string\n        the subnational region (e.g., province, state, etc.) that the cases are\n        reported for\n    country : string\n        the national region (e.g., national state, autonomous region, etc.)\n        that the cases are reported for\n    tested : int\n        total number of individuals tested for COVID-19\n    under_investigation : int\n        number of individuals still under investigation for COVID-19 (i.e.,\n        incomplete tests)"
}
{
    "class": "class Alebot(async_chat, IRCCommandsMixin):\n\n\n    Hooks = []\n    Plugins = {}\n    _Paths = None\n    path = None\n    Logger = logging.getLogger('alebot')\n\n    def __init__(self, path=None, disableLog=False):\n        if not disableLog:\n            handler = logging.StreamHandler()\n            handler.setFormatter(logging.Formatter(\n                '%(asctime)s - %(levelname)s - %(message)s'))\n            self.logger.addHandler(handler)\n            self.logger.setLevel(\"INFO\")\n        self.logger.info(\"Initiation the bot..\")\n\n        if path:\n            self.path = path\n        else:\n            self.path = os.getcwd()\n        self.logger.info(\"Using '%s' as bot path.\" % self.path)\n\n        async_chat.__init__(self)\n        self.set_terminator('\\r\\n')\n        self.buffer = ''\n\n        self.paths\n\n        self.config = {\n            'nick': 'alebot',\n            'ident': 'alebot',\n            'realname':\n            'alebot python irc bot. https://github.com/alexex/alebot',\n            'server': 'irc.freenode.net',\n            'port': 6667,\n            'logToStdout': True,\n            'logLevel': 'INFO',\n            'logFormatter': '%(asctime)s - %(levelname)s - %(message)s',\n            'logFile': False\n        }\n\n        self.load_config()\n\n        self.load_plugins()\n\n        self.activate_hooks()\n\n    @property\n    def logger(self):\n        return self.__class__.Logger\n\n    @classmethod\n    def Paths(cls, path=None):\n        if not cls._Paths:\n            paths = []\n\n            if __file__:\n                paths.append(os.path.join(os.path.dirname(__file__),\n                             'plugins'))\n\n            if path:\n                userpath = os.path.join(path, 'plugins')\n                if not os.path.exists(path):\n                    cls.Logger.warn(\"User plugin path '%s' does not exist!\" %\n                                    userpath)\n                elif not os.path.isdir(path):\n                    cls.Logger.warn(\"User plugin path '%s' is not a dir.\" %\n                                    userpath)\n                else:\n                    cls.Logger.info(\"User plugin path '%s' added.\", userpath)\n                    paths.append(userpath)\n            else:\n                cls.Logger.warn(\"No user plugin path given!\")\n            cls._Paths = paths\n        return cls._Paths\n\n    @property\n    def paths(self):\n        return self.__class__.Paths(self.path)\n\n    def configure_logging(self):\n        for handler in self.logger.handlers:\n            self.logger.removeHandler(handler)\n        self.logger.setLevel(self.config.get('logLevel'))\n        formatter = logging.Formatter(self.config.get('logFormatter'))\n        if self.config.get('logToStdout'):\n            handler = logging.StreamHandler()\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n        if self.config.get('logFile'):\n            handler = logging.FileHandler(self.config.get('logFile'))\n            handler.setFormatter(formatter)\n            self.logger.addHandler(handler)\n\n    @classmethod\n    def load_plugins(cls, path=None):\n        cls.Hooks = []\n        cls.Plugins = {}\n\n        for _, name, _ in pkgutil.iter_modules(cls.Paths()):\n            cls.load_plugin(name)\n\n    @classmethod\n    def load_plugin(cls, name, path=[]):\n        if not path:\n            path = cls.Paths()\n        fid, pathname, desc = imp.find_module(name, path)\n        if cls.Plugins.get(name):\n            return\n        try:\n            plugin = imp.load_module(name, fid, pathname, desc)\n            cls.Plugins[name] = plugin\n            cls.Logger.info(\"Loaded plugin '%s' from '%s'\" % (name, pathname))\n        except Exception as e:\n            cls.Logger.warning(\"Could not load plugin '%s': %s\" %\n                               (pathname, e))\n        if fid:\n            fid.close()\n\n    @classmethod\n    def get_plugin(cls, name):\n        if not cls.Plugins.get(name):\n            cls.load_plugin(name)\n        return cls.Plugins[name]\n\n    def load_config(self):\n        error = False\n        path = os.path.join(self.path, 'config.json')\n        try:\n            f = open(path, 'r')\n            config = json.load(f)\n            f.close()\n            self.config = dict(self.config.items() + config.items())\n        except Exception as e:\n            error = e\n            config = False\n\n        self.configure_logging()\n        if config:\n            self.logger.info(\"Configuration loaded.\")\n        else:\n            self.logger.info(\"No configuration loaded: %s\" % error)\n\n    def save_config(self):\n\n        try:\n            f = open('config.json', 'w')\n            json.dump(self.config, f, indent=4)\n            f.close()\n            self.logger.info(\"Configuration saved.\")\n        except Exception as e:\n            self.logger.info(\"Configuration could not be saved: %s\" % e)\n\n    @classmethod\n    def hook(cls, Hook):\n        cls.Hooks.append(Hook)\n        return Hook\n\n    def activate_hooks(self):\n        self.hooks = []\n        for Hook in Alebot.Hooks:\n            self.hooks.append(Hook(self))\n\n    def call_hooks(self, event):\n        for hook in self.hooks:\n            try:\n                if (hook.match(event)):\n                    hook.call(event)\n            except Exception as e:\n                self.logger.error(\"Hook %s failed: %s\" % (hook, e))\n\n    def connect(self):\n        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)\n        async_chat.connect(self, (self.config['server'], self.config['port']))\n        asyncore.loop()\n\n    def handle_connect(self):\n        event = Event('SOCK_CONNECTED')\n        self.call_hooks(event)\n\n    def collect_incoming_data(self, data):\n        self.buffer += data\n\n    def found_terminator(self):\n\n        line = self.buffer\n        self.buffer = ''\n\n        line = line.split(' ', 3)\n\n        event = Event()\n\n        if (line[0][0] == ':'):\n\n            event.user = line[0][1:]\n            event.name = line[1]\n            event.target = line[2]\n\n            if(len(line) >= 4):\n                event.body = line[3][1:]\n\n        elif (line[0] == 'PING'):\n            event.name = line[0]\n            event.body = line[1][1:]\n        elif (line[0] == 'ERROR'):\n            event.name = 'ERROR'\n            event.body = ' '.join(line[1:])[1:]\n        else:\n            event.name = 'UNKNOWN'\n            event.body = ' '.join(line)\n\n        self.call_hooks(event)\n\n    def send_raw(self, data):\n        crlfed = '%s\\r\\n' % data\n        self.push(crlfed.encode('utf-8', 'ignore'))",
    "docstring": "The main bot class, where all the magic happens.\n\n        This class handles the socket and all incoming and outgoing\n        data. This classes methods should be used for sending data.\n\n        It keeps an index of loaded plugins and helps with the\n        management of requirements.\n\n        To interact witht he bot it supplies a hook function that can\n        be used to register callbacks with the bot.\n\n        Please check out :class:`.Hook` and :func:`hook` to find\n        out how hooking your plugins in works.\n\n        Please note that this bot does absolutely nothing by itself.\n\n        It won't even answer pings or identify. But there are some\n        system plugins to do that. Check the plugins folder.\n\n        .. attribute:: config\n\n            Holds the bot configuration.\n\n        .. attribute:: Hooks\n\n            Registered hooks\n\n        .. attribute:: Plugins\n\n            Registered modules"
}
{
    "class": "class foaf___OnlineEcommerceAccount(foaf___OnlineAccount):\n\tdef __init__(self,URI=None):\n\t\tfoaf___OnlineAccount.__init__(self)\n\t\tself._initialised = False\n\t\tself.shortname = \"OnlineEcommerceAccount\"\n\t\tself.URI = URI\n\t\tself._initialised = True\n\tclassURI = \"http://xmlns.com/foaf/0.1/OnlineEcommerceAccount\"\n\n\t__setattr__ = protector\n\t__str__ = objToStr",
    "docstring": "foaf:OnlineEcommerceAccount\n\tAn online e-commerce account."
}
{
    "class": "class PersonSchema(Schema):\n\n    name = fields.String()\n    occupation = fields.String()\n    homepage = fields.Url()",
    "docstring": "Simple data with string keys and values, Can be represented with JSON. No difference\n    from regular Marshmallow\n\n    >>> from binascii import hexlify\n    >>> person = {\n    ...     \"name\": \"Simon\",\n    ...     \"occupation\": \"Skydiving Instructor\",\n    ...     \"homepage\": \"https://example.com/letsgoskydiving\"\n    ... }\n    >>> expected = (b'a3646e616d656553696d6f6e6a6f636375706174696f6e74536b79646976696e672'\n    ... b'0496e7374727563746f7268686f6d6570616765782368747470733a2f2f6578616d706c652e636f'\n    ... b'6d2f6c657473676f736b79646976696e67')\n    >>> schema = PersonSchema()\n    >>> hexlify(schema.dumps(person)) == expected\n    True"
}
{
    "class": "class LinearDecoder(nn.Module):\n    \"To go on top of a RNNCore module and create a Language Model.\"\n\n    initrange=0.1\n\n    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n        super().__init__()\n        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n        self.output_dp = RNNDropout(output_p)\n        if bias: self.decoder.bias.data.zero_()\n        if tie_encoder: self.decoder.weight = tie_encoder.weight\n\n    def forward(self, input):\n        raw_outputs, outputs = input\n        output = self.output_dp(outputs[-1])\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded, raw_outputs, outputs",
    "docstring": "To go on top of a RNNCore module and create a Language Model."
}
{
    "class": "class RidgeCV(Ridge):\n    ALL_SOLVERS = dict(svd=solve_ridge_cv_svd)\n\n    def __init__(self, alphas=[0.1, 1], fit_intercept=False, solver=\"svd\",\n                 solver_params=None, cv=5, Y_in_cpu=False, force_cpu=False):\n        self.alphas = alphas\n        self.fit_intercept = fit_intercept\n        self.solver = solver\n        self.solver_params = solver_params\n        self.cv = cv\n        self.Y_in_cpu = Y_in_cpu\n        self.force_cpu = force_cpu\n\n    @force_cpu_backend\n    def fit(self, X, y=None):\n        X = check_array(X, ndim=2)\n        self.dtype_ = _get_string_dtype(X)\n        device = \"cpu\" if self.Y_in_cpu else None\n        y = check_array(y, dtype=self.dtype_, ndim=[1, 2], device=device)\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"Inconsistent number of samples.\")\n\n        alphas = check_array(self.alphas, dtype=self.dtype_, ndim=1)\n        self.n_features_in_ = X.shape[1]\n\n        ravel = False\n        if y.ndim == 1:\n            y = y[:, None]\n            ravel = True\n\n        cv = check_cv(self.cv)\n\n        tmp = self._call_solver(X=X, Y=y, cv=cv, alphas=alphas,\n                                fit_intercept=self.fit_intercept,\n                                Y_in_cpu=self.Y_in_cpu)\n        if self.fit_intercept:\n            self.best_alphas_, self.coef_, self.cv_scores_ = tmp[:3]\n            self.intercept_, = tmp[3:]\n        else:\n            self.best_alphas_, self.coef_, self.cv_scores_ = tmp\n\n        self.cv_scores_ = self.cv_scores_[0]\n\n        if ravel:\n            self.coef_ = self.coef_[:, 0]\n            if self.fit_intercept:\n                self.intercept_ = self.intercept_[0]\n\n        return self",
    "docstring": "Ridge regression with efficient cross-validation over alpha.\n\n    Solve the ridge regression::\n\n        b* = argmin_b ||X @ b - Y||^2 + alpha ||b||^2,\n\n    with a grid-search over cross-validation to find the best alpha.\n\n    Parameters\n    ----------\n    alphas : array of shape (n_alphas, )\n        List of L2 regularization parameter to try.\n\n    fit_intercept : boolean\n        Whether to fit an intercept.\n        If False, X and Y must be zero-mean over samples.\n\n    solver : str\n        Algorithm used during the fit, \"svd\" only for now.\n\n    solver_params : dict or None\n        Additional parameters for the solver.\n        See more details in the docstring of the function:\n        ``RidgeCV.ALL_SOLVERS[solver]``\n\n    cv : int or scikit-learn splitter\n        Cross-validation splitter. If an int, KFold is used.\n\n    Y_in_cpu : bool\n        If True, keep the target values ``y`` in CPU memory (slower).\n\n    force_cpu : bool\n        If True, computations will be performed on CPU, ignoring the\n        current backend. If False, use the current backend.\n\n    Attributes\n    ----------\n    coef_ : array of shape (n_features) or (n_features, n_targets)\n        Ridge coefficients.\n\n    intercept_ : float or array of shape (n_targets, )\n        Intercept. Only returned when fit_intercept is True.\n\n    best_alphas_ : array of shape (n_targets, )\n        Selected best hyperparameter alphas.\n\n    cv_scores_ : array of shape (n_targets, )\n        Cross-validation scores averaged over splits, for the best alpha.\n\n    n_features_in_ : int\n        Number of features used during the fit.\n\n    Examples\n    --------\n    >>> from himalaya.ridge import RidgeCV\n    >>> import numpy as np\n    >>> n_samples, n_features, n_targets = 10, 5, 3\n    >>> X = np.random.randn(n_samples, n_features)\n    >>> Y = np.random.randn(n_samples, n_targets)\n    >>> clf = RidgeCV()\n    >>> clf.fit(X, Y)\n    RidgeCV()"
}
{
    "class": "class CommPilotExpressSRAvailableInOffice(OCIType):\n\n    _ELEMENTS = (\n        E(\n            \"busy_setting\", \"busySetting\", CommPilotExpressSRRedirection, is_complex=True,\n            is_required=True,\n        ),\n        E(\n            \"no_answer_setting\", \"noAnswerSetting\", CommPilotExpressSRNoAnswer,\n            is_complex=True, is_required=True,\n        ),\n        E(\n            \"incoming_call_notify\", \"incomingCallNotify\", CommPilotExpressSREmailNotify,\n            is_complex=True, is_required=True,\n        ),\n    )\n\n    busy_setting: \"CommPilotExpressSRRedirection\" = Field(\n        type=CommPilotExpressSRRedirection, required=True, mutable=False,\n    )\n    no_answer_setting: \"CommPilotExpressSRNoAnswer\" = Field(\n        type=CommPilotExpressSRNoAnswer, required=True, mutable=False,\n    )\n    incoming_call_notify: \"CommPilotExpressSREmailNotify\" = Field(\n        type=CommPilotExpressSREmailNotify, required=True, mutable=False,\n    )",
    "docstring": "CommPilot Express SR Available In Office Settings used in the context of a get.\n\n\n    Attributes:\n        busy_setting: busySetting\n        no_answer_setting: noAnswerSetting\n        incoming_call_notify: incomingCallNotify"
}
{
    "class": "class ConcentrationTearsheet(BaseTearsheet):\n\n    def __init__(self, concentration_calculator: ConcentrationCalculator):\n        self.concentration_calculator = concentration_calculator\n\n    def create_portfolio_panel(\n        self, weights: PortfolioWeights\n    ) -> pandas.Series:\n        return pandas.Series(\n            self.concentration_calculator.summarise_portfolio(weights)\n        )",
    "docstring": "Class to provide a summary of concentration metrics for portfolios"
}
{
    "class": "class Msg(object):\n\n    @staticmethod\n    def Unjail(request,\n            target,\n            options=(),\n            channel_credentials=None,\n            call_credentials=None,\n            insecure=False,\n            compression=None,\n            wait_for_ready=None,\n            timeout=None,\n            metadata=None):\n        return grpc.experimental.unary_unary(request, target, '/cosmos.slashing.v1beta1.Msg/Unjail',\n            cosmos_dot_slashing_dot_v1beta1_dot_tx__pb2.MsgUnjail.SerializeToString,\n            cosmos_dot_slashing_dot_v1beta1_dot_tx__pb2.MsgUnjailResponse.FromString,\n            options, channel_credentials,\n            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)",
    "docstring": "Msg defines the slashing Msg service."
}
{
    "class": "class VersionMismatchError(BaseRedmineError):\n    def __init__(self, feature):\n        super(VersionMismatchError, self).__init__(\"{0} isn't supported on specified Redmine version\".format(feature))",
    "docstring": "Feature isn't supported on specified Redmine version."
}
{
    "class": "class AutoResponse(NumericalTypeAnswer):\n    value = models.CharField(null=True, max_length=100)\n\n    @classmethod\n    def choice_name(cls):\n        return 'Auto Generated'\n\n    class Meta:\n        app_label = 'survey'\n        abstract = False\n\n    @classmethod\n    def create(cls, interview, question, answer):\n        value = int(answer)\n        prefix = 'flow-test'\n        if interview.interview_channel:\n            prefix = interview.interview_channel.user_identifier\n        text_value = '%s-%s' % (prefix, cls.prep_value(value))\n        return super(\n            AutoResponse,\n            cls).create(\n            interview,\n            question,\n            answer,\n            as_text=value,\n            as_value=text_value)",
    "docstring": "Shall be used to capture responses auto generated"
}
{
    "class": "class Arg(object):\n\n\n    def __init__(self, data=None, map=None, access=None, lgmaps=None, unroll_map=False):\n        self.data = data\n        self._map = map\n        if map is None:\n            self.map_tuple = ()\n        elif isinstance(map, Map):\n            self.map_tuple = (map, )\n        else:\n            self.map_tuple = tuple(map)\n        self._access = access\n        self._in_flight = False\n\n        self.unroll_map = unroll_map\n        self.lgmaps = None\n        if self._is_mat and lgmaps is not None:\n            self.lgmaps = as_tuple(lgmaps)\n        else:\n            if lgmaps is not None:\n                raise ValueError(\"Local to global maps only for matrices\")\n\n        if configuration[\"type_check\"] and not (self._is_global or map is None):\n            for j, m in enumerate(map):\n                if m.iterset.total_size > 0 and len(m.values_with_halo) == 0:\n                    raise MapValueError(\"%s is not initialized.\" % map)\n                if self._is_mat and m.toset != data.sparsity.dsets[j].set:\n                    raise MapValueError(\n                        \"To set of %s doesn't match the set of %s.\" % (map, data))\n            if self._is_dat and map.toset != data.dataset.set:\n                raise MapValueError(\n                    \"To set of %s doesn't match the set of %s.\" % (map, data))\n\n    @cached_property\n    def _kernel_args_(self):\n        return self.data._kernel_args_\n\n    @cached_property\n    def _argtypes_(self):\n        return self.data._argtypes_\n\n    @cached_property\n    def _wrapper_cache_key_(self):\n        if self.map is not None:\n            map_ = tuple(None if m is None else m._wrapper_cache_key_ for m in self.map)\n        else:\n            map_ = self.map\n        return (type(self), self.access, self.data._wrapper_cache_key_, map_, self.unroll_map)\n\n    @property\n    def _key(self):\n        return (self.data, self._map, self._access)\n\n    def __eq__(self, other):\n        r\"\"\":class:`Arg`\\s compare equal of they are defined on the same data,\n        use the same :class:`Map` with the same index and the same access\n        descriptor.\"\"\"\n        return self._key == other._key\n\n    def __ne__(self, other):\n        r\"\"\":class:`Arg`\\s compare equal of they are defined on the same data,\n        use the same :class:`Map` with the same index and the same access\n        descriptor.\"\"\"\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return \"OP2 Arg: dat %s, map %s, access %s\" % \\\n            (self.data, self._map, self._access)\n\n    def __repr__(self):\n        return \"Arg(%r, %r, %r)\" % \\\n            (self.data, self._map, self._access)\n\n    def __iter__(self):\n        for arg in self.split:\n            yield arg\n\n    @cached_property\n    def split(self):\n        if self._is_mixed_dat:\n            return tuple(_make_object('Arg', d, m, self._access)\n                         for d, m in zip(self.data, self._map))\n        elif self._is_mixed_mat:\n            s = self.data.sparsity.shape\n            mr, mc = self.map\n            return tuple(_make_object('Arg', self.data[i, j], (mr.split[i], mc.split[j]),\n                                      self._access)\n                         for j in range(s[1]) for i in range(s[0]))\n        else:\n            return (self,)\n\n    @cached_property\n    def name(self):\n        return \"arg%d\" % self.position\n\n    @cached_property\n    def ctype(self):\n        return self.data.ctype\n\n    @cached_property\n    def dtype(self):\n        return self.data.dtype\n\n    @cached_property\n    def map(self):\n        return self._map\n\n    @cached_property\n    def access(self):\n        return self._access\n\n    @cached_property\n    def _is_dat_view(self):\n        return isinstance(self.data, DatView)\n\n    @cached_property\n    def _is_mat(self):\n        return isinstance(self.data, Mat)\n\n    @cached_property\n    def _is_mixed_mat(self):\n        return self._is_mat and self.data.sparsity.shape > (1, 1)\n\n    @cached_property\n    def _is_global(self):\n        return isinstance(self.data, Global)\n\n    @cached_property\n    def _is_global_reduction(self):\n        return self._is_global and self._access in [INC, MIN, MAX]\n\n    @cached_property\n    def _is_dat(self):\n        return isinstance(self.data, Dat)\n\n    @cached_property\n    def _is_mixed_dat(self):\n        return isinstance(self.data, MixedDat)\n\n    @cached_property\n    def _is_mixed(self):\n        return self._is_mixed_dat or self._is_mixed_mat\n\n    @cached_property\n    def _is_direct(self):\n        return isinstance(self.data, Dat) and self.map is None\n\n    @cached_property\n    def _is_indirect(self):\n        return isinstance(self.data, Dat) and self.map is not None\n\n    @collective\n    def global_to_local_begin(self):\n        assert self._is_dat, \"Doing halo exchanges only makes sense for Dats\"\n        assert not self._in_flight, \\\n            \"Halo exchange already in flight for Arg %s\" % self\n        if self._is_direct:\n            return\n        if self.access in [READ, RW, INC, MIN, MAX]:\n            self._in_flight = True\n            self.data.global_to_local_begin(self.access)\n\n    @collective\n    def global_to_local_end(self):\n        assert self._is_dat, \"Doing halo exchanges only makes sense for Dats\"\n        if self.access in [READ, RW, INC, MIN, MAX] and self._in_flight:\n            self._in_flight = False\n            self.data.global_to_local_end(self.access)\n\n    @collective\n    def local_to_global_begin(self):\n        assert self._is_dat, \"Doing halo exchanges only makes sense for Dats\"\n        assert not self._in_flight, \\\n            \"Halo exchange already in flight for Arg %s\" % self\n        if self._is_direct:\n            return\n        if self.access in [INC, MIN, MAX]:\n            self._in_flight = True\n            self.data.local_to_global_begin(self.access)\n\n    @collective\n    def local_to_global_end(self):\n        assert self._is_dat, \"Doing halo exchanges only makes sense for Dats\"\n        if self.access in [INC, MIN, MAX] and self._in_flight:\n            self._in_flight = False\n            self.data.local_to_global_end(self.access)\n        if self.access is not READ:\n            self.data.halo_valid = False\n\n    @collective\n    def reduction_begin(self, comm):\n        assert self._is_global, \\\n            \"Doing global reduction only makes sense for Globals\"\n        assert not self._in_flight, \\\n            \"Reduction already in flight for Arg %s\" % self\n        if self.access is not READ:\n            self._in_flight = True\n            if self.access is INC:\n                op = MPI.SUM\n            elif self.access is MIN:\n                op = MPI.MIN\n            elif self.access is MAX:\n                op = MPI.MAX\n            comm.Allreduce(self.data._data, self.data._buf, op=op)\n\n    @collective\n    def reduction_end(self, comm):\n        assert self._is_global, \\\n            \"Doing global reduction only makes sense for Globals\"\n        if self.access is not READ and self._in_flight:\n            self._in_flight = False\n            self.data._data[:] = self.data._buf[:]",
    "docstring": "An argument to a :func:`pyop2.op2.par_loop`.\n\n    .. warning ::\n        User code should not directly instantiate :class:`Arg`.\n        Instead, use the call syntax on the :class:`DataCarrier`."
}
{
    "class": "class QuantityTextSlider(QuantityText):\n    \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, description=\"\")\n        \n        self.qslider = QuantitySlider(*args,\n                                      label=False,\n                                     **kwargs)\n        \n        \n        link = ipyw.link(\n            (self, \"value\"), \n            (self.qslider, \"value\")\n        )\n        \n        self.children = [\n            self.qslider,\n            self.text, \n        ]",
    "docstring": "A QuantitySlider where the value is displayed and can be changed using a QuantityText\n    \n    Both widgets are merged in the QuantiyText VBox as children\n    We drop the Quantityslider's label\n    We drop the QuantityText's description"
}
{
    "class": "class VampriorVAE(BetaVAE):\n\n  def __init__(\n      self,\n      n_components: int = 500,\n      pseudoinputs_mean: float = -0.05,\n      pseudoinputs_std: float = 0.01,\n      beta: Union[float, Interpolation] = linear(vmin=1e-6,\n                                                 vmax=1.,\n                                                 steps=2000,\n                                                 delay_in=0),\n      **kwargs):\n    super().__init__(beta=beta, **kwargs)\n    self.n_components = n_components\n    self.pseudoinputs_mean = pseudoinputs_mean\n    self.pseudoinputs_std = pseudoinputs_std\n\n  def build(self, input_shape=None):\n    ret = super().build(input_shape)\n    self.vamprior = Vamprior(input_shape=input_shape[1:],\n                             fn_encoding=self.encode,\n                             n_components=self.n_components,\n                             pseudoinputs_mean=self.pseudoinputs_mean,\n                             pseudoinputs_std=self.pseudoinputs_std,\n                             dtype=self.dtype)\n    self.latents.prior = self.vamprior\n    return ret",
    "docstring": "An example of Vamprior VAE. In practice, vamprior could be integrated into\n  any VAE model by changing the prior of specified latents variable.\n\n  References\n  ----------\n  Tomczak, J.M., Welling, M., 2018. VAE with a VampPrior. arXiv:1705.07120 [cs, stat]."
}
{
    "class": "class RegistrationForm(FlaskForm):\n\n\n    username = StringField(\n        \"Username\",\n        validators=[DataRequired(), Length(min=5, max=40)])\n    email = StringField(\n        \"Email\",\n        validators=[DataRequired(), Email()])\n    first_name = StringField(\n        \"First name\",\n        validators=[DataRequired(), Length(min=3, max=40)])\n    last_name = StringField(\n        \"Last name\",\n        validators=[DataRequired(), Length(min=3, max=40)])\n    password = PasswordField(\n        \"Password\",\n        validators=[DataRequired(), Length(min=8, max=40)])\n    password2 = PasswordField(\n        \"Repeat Password\",\n        validators=[EqualTo(\"password\", \"Field must be equal to Password.\")])\n\n\n    def __init__(self, user_handler: UserHandler) -> None:\n        super(RegistrationForm, self).__init__()\n\n        self._user_handler: UserHandler = user_handler\n\n\n    def validate_username(self, username: StringField) -> None:\n        if self._user_handler.get_user(username.data.lower()) is not None:\n            raise ValidationError(\"Please use a different username.\")\n\n    def validate_email(self, email: StringField) -> None:\n        if self._user_handler.get_user(email.data.lower()) is not None:\n            raise ValidationError(\"Please use a different email address.\")",
    "docstring": "Registration form."
}
{
    "class": "class LinearDiscriminantAnalysis(BaseModel):\r\n\r\n    acronym = \"LDA\"\r\n    fullname = \"Linear Discriminant Analysis\"\r\n    needs_scaling = False\r\n    goal = \"class\"\r\n\r\n    def __init__(self, *args):\r\n        super().__init__(*args)\r\n        self.params = {\"solver\": [\"svd\", 0], \"shrinkage\": [0, 1]}\r\n\r\n    @property\r\n    def est_class(self):\r\n        return LDA\r\n\r\n    def get_params(self, x):\r\n        params = super().get_params(x)\r\n\r\n        if params.get(\"solver\") == \"svd\":\r\n            params.pop(\"shrinkage\")\r\n\r\n        return params\r\n\r\n    def get_estimator(self, params=None):\r\n        params = copy(params or {})\r\n        return self.est_class(**params)\r\n\r\n    def get_dimensions(self):\r\n        dimensions = [\r\n            Categorical([\"svd\", \"lsqr\", \"eigen\"], name=\"solver\"),\r\n            Categorical(np.linspace(0.0, 1.0, 11), name=\"shrinkage\"),\r\n        ]\r\n        return [d for d in dimensions if d.name in self.params]",
    "docstring": "Linear Discriminant Analysis."
}
{
    "class": "class deformableCNv2(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(deformableCNv2, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        if isinstance(kernel_size, (list, tuple)):\n            self.padding = [int(ii / 2) for ii in kernel_size]\n        else:\n            self.padding = int(kernel_size / 2)\n\n        self.layers = nn.Sequential(\n            dcnv2.DeformConv2(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, bias=False),\n            nn.BatchNorm2d(self.out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def __repr__(self):\n        s = '{name} ({in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}, padding={padding})'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x",
    "docstring": "This convenience layer groups a 2D convolution, a batchnorm and a ReLU.\n    They are executed in a sequential manner.\n\n    Args:\n        in_channels (int): Number of input channels\n        out_channels (int): Number of output channels\n        kernel_size (int or tuple): Size of the kernel of the convolution\n        stride (int or tuple): Stride of the convolution\n        padding (int or tuple): padding of the convolution"
}
{
    "class": "class ExtraFunction(Function):\n\n    def __init__(self, cname, prototype, pathfordoc):\n        self.name = cname\n        self.pyname = cname.split('era')[-1].lower()\n        self.filepath, self.filename = os.path.split(pathfordoc)\n\n        self.prototype = prototype.strip()\n        if prototype.endswith('{') or prototype.endswith(';'):\n            self.prototype = prototype[:-1].strip()\n\n        incomment = False\n        lastcomment = None\n        with open(pathfordoc, 'r') as f:\n            for l in f:\n                if incomment:\n                    if l.lstrip().startswith('*/'):\n                        incomment = False\n                        lastcomment = ''.join(lastcomment)\n                    else:\n                        if l.startswith('**'):\n                            l = l[2:]\n                        lastcomment.append(l)\n                else:\n                    if l.lstrip().startswith('/*'):\n                        incomment = True\n                        lastcomment = []\n                    if l.startswith(self.prototype):\n                        self.doc = lastcomment\n                        break\n            else:\n                raise ValueError('Did not find prototype {} in file '\n                                 '{}'.format(self.prototype, pathfordoc))\n\n        self.args = []\n        argset = re.search(fr\"{self.name}\\(([^)]+)?\\)\",\n                           self.prototype).group(1)\n        if argset is not None:\n            for arg in argset.split(', '):\n                self.args.append(Argument(arg, self.doc))\n        self.ret = re.match(f\"^(.*){self.name}\",\n                            self.prototype).group(1).strip()\n        if self.ret != 'void':\n            self.args.append(Return(self.ret, self.doc))\n\n    def __repr__(self):\n        r = super().__repr__()\n        if r.startswith('Function'):\n            r = 'Extra' + r\n        return r",
    "docstring": "An \"extra\" function - e.g. one not following the SOFA/ERFA standard format.\n\n    Parameters\n    ----------\n    cname : str\n        The name of the function in C\n    prototype : str\n        The prototype for the function (usually derived from the header)\n    pathfordoc : str\n        The path to a file that contains the prototype, with the documentation\n        as a multiline string *before* it."
}
{
    "class": "class IndexRefreshOp(Operation):\n\n    def __init__(self, index, **kwargs):\n        self._index = index\n        self._kwargs = kwargs\n\n    def on_post_commit(self, uow):\n        self._index.refresh(**self._kwargs)",
    "docstring": "Search index refresh operation."
}
{
    "class": "class TransportationType(Enum):\n\n    BUS = \"Bus\"\n    TRAM = \"Tram\"\n    TRAIN = \"Train\"\n    FERRY = \"Ferry\"\n    CITY_BUS = \"CityBus\"\n    PLUS_BUS = \"PlusBus\"\n    CLOCK_BUS = \"ClockBus\"\n    CABLEWAY = \"Cableway\"\n    INTERCITY_BUS = \"IntercityBus\"\n    SUBURBAN_RAILWAY = \"SuburbanRailway\"\n    HAILED_SHARED_TAXI = \"HailedSharedTaxi\"\n\n    RAPID_TRANSIT = \"RapidTransit\"\n\n\n    STAY = \"StayForConnection\"\n    FOOTPATH = \"Footpath\"\n    STAIRWAY_UP = \"MobilityStairsUp\"\n    STAIRWAY_DOWN = \"MobilityStairsDown\"\n    RAMP_UP = \"MobilityRampUp\"\n    RAMP_DOWN = \"MobilityRampDown\"",
    "docstring": "Represents means of transportation\n\n    For departures do not use the 'by foot` types like stairway and footpath!"
}
{
    "class": "class TopPeers:\n\n    QUALNAME = \"pyrogram.raw.base.contacts.TopPeers\"\n\n    def __init__(self):\n        raise TypeError(\"Base types can only be used for type checking purposes: \"\n                        \"you tried to use a base type instance as argument, \"\n                        \"but you need to instantiate one of its constructors instead. \"\n                        \"More info: https://docs.pyrogram.org/telegram/base/top-peers\")",
    "docstring": "This base type has 3 constructors available.\n\n    Constructors:\n        .. hlist::\n            :columns: 2\n\n            - :obj:`contacts.TopPeers <pyrogram.raw.types.contacts.TopPeers>`\n            - :obj:`contacts.TopPeersDisabled <pyrogram.raw.types.contacts.TopPeersDisabled>`\n            - :obj:`contacts.TopPeersNotModified <pyrogram.raw.types.contacts.TopPeersNotModified>`\n\n    See Also:\n        This object can be returned by 1 method:\n\n        .. hlist::\n            :columns: 2\n\n            - :obj:`contacts.GetTopPeers <pyrogram.raw.functions.contacts.GetTopPeers>`"
}
{
    "class": "class BatchScanArgs(object):\r\n def ZZZ(self):\r\n  return BatchScanArgs()\r\n instance=ZZZ()\r\n Barcode=property(lambda self: object(),lambda self,v: None,lambda self: None)\r\n\r\n CacheKey=property(lambda self: object(),lambda self,v: None,lambda self: None)\r\n\r\n ItemCodeExpected=property(lambda self: object(),lambda self,v: None,lambda self: None)\r\n\r\n WarehouseCode=property(lambda self: object(),lambda self,v: None,lambda self: None)\r\n\r\n WarehouseLocationCode=property(lambda self: object(),lambda self,v: None,lambda self: None)\r",
    "docstring": "BatchScanArgs()"
}
{
    "class": "class MarkovChainModel(Configurable[MarkovChainModelConfig], nn.Module):\n\n    def __init__(self, config: MarkovChainModelConfig):\n        super().__init__(config)\n\n        self.initial_probs: torch.Tensor\n        self.register_buffer(\"initial_probs\", torch.empty(config.num_states))\n\n        self.transition_probs: torch.Tensor\n        self.register_buffer(\"transition_probs\", torch.empty(config.num_states, config.num_states))\n\n        self.reset_parameters()\n\n    @jit.unused\n    def reset_parameters(self) -> None:\n        nn.init.uniform_(self.initial_probs)\n        self.initial_probs.div_(self.initial_probs.sum())\n\n        nn.init.uniform_(self.transition_probs)\n        self.transition_probs.div_(self.transition_probs.sum(1, keepdim=True))\n\n    @overload\n    @_jit._overload_method\n    def forward(self, sequences: torch.Tensor) -> torch.Tensor:\n        ...\n\n    @overload\n    @_jit._overload_method\n    def forward(self, sequences: PackedSequence) -> torch.Tensor:\n        ...\n\n    def forward(self, sequences) -> torch.Tensor:\n        if isinstance(sequences, torch.Tensor):\n            log_probs = self.initial_probs[sequences[:, 0]].log()\n            sources = sequences[:, :-1]\n            targets = sequences[:, 1:].unsqueeze(-1)\n            transition_probs = self.transition_probs[sources].gather(-1, targets).squeeze(-1)\n            return log_probs + transition_probs.log().sum(-1)\n        if isinstance(sequences, PackedSequence):\n            data = sequences.data\n            batch_sizes = sequences.batch_sizes\n\n            log_probs = self.initial_probs[data[: batch_sizes[0]]].log()\n            offset = 0\n            for prev_size, curr_size in zip(batch_sizes, batch_sizes[1:]):\n                log_probs[:curr_size] += self.transition_probs[\n                    data[offset : offset + curr_size],\n                    data[offset + prev_size : offset + prev_size + curr_size],\n                ].log()\n                offset += prev_size\n\n            if sequences.unsorted_indices is not None:\n                return log_probs[sequences.unsorted_indices]\n            return log_probs\n        raise ValueError(\"unsupported input type\")\n\n    def sample(self, num_sequences: int, sequence_length: int) -> torch.Tensor:\n        samples = torch.empty(\n            num_sequences, sequence_length, device=self.transition_probs.device, dtype=torch.long\n        )\n        samples[:, 0] = self.initial_probs.multinomial(num_sequences, replacement=True)\n        for i in range(1, sequence_length):\n            samples[:, i] = self.transition_probs[samples[:, i - 1]].multinomial(1).squeeze(-1)\n        return samples\n\n    def stationary_distribution(\n        self, tol: float = 1e-7, max_iterations: int = 1000\n    ) -> torch.Tensor:\n        A = self.transition_probs.t()\n        v = torch.rand(A.size(0), device=A.device, dtype=A.dtype)\n\n        for _ in range(max_iterations):\n            v_old = v\n            v = A.mv(v)\n            v = v / v.norm()\n            if (v - v_old).norm() < tol:\n                break\n\n        return v / v.sum()",
    "docstring": "PyTorch module for a Markov chain. The initial state probabilities as well as the transition\n    probabilities are non-trainable parameters."
}
{
    "class": "class OrderedSet(collections.abc.MutableSet):\n    \"A set that maintains insertion order\"\n\n    def __init__(self):\n        self._data = {}\n\n    def add(self, item):\n        if item not in self:\n            self._data[item] = 1\n\n    def discard(self, item):\n        if item in self:\n            del self._data[item]\n\n    def __contains__(self, item):\n        return item in self._data\n\n    def __iter__(self):\n        return self._data.keys().__iter__()\n\n    def __len__(self):\n        return len(self._data)",
    "docstring": "A set that maintains insertion order"
}
{
    "class": "class HttpDaemon(DaemonBase):\n\n    def __init__(\n        self,\n        name: str,\n        address: Host = None,\n        handler_class: Type[BaseHTTPRequestHandler] = None,\n        env: TelemetryEnvironment = None,\n        time_keeper: TimeKeeper = None,\n    ):\n\n        if address is None:\n            address = Host()\n\n        if handler_class is None:\n            handler_class = SimpleHTTPRequestHandler\n\n        super().__init__(name, env, time_keeper)\n        self.server = ThreadingHTTPServer(address, handler_class)\n        self.server.mapper = HttpRequestMapper()\n        host = self.server.server_address\n        LOG.info(\"'%s' daemon bound to %s:%d\", self.name, host[0], host[1])\n        self.ssl_context: Optional[ssl.SSLContext] = None\n        self.closed = False\n        self.uses_tls = False\n\n        def stop_injector() -> None:\n            self.server.shutdown()\n\n        self.function[\"inject_stop\"] = stop_injector\n\n    def use_tls(self, keyfile_path: str, certfile_path: str) -> None:\n\n        with self.lock:\n            assert self.get_state() == DaemonState.IDLE\n            self.ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)\n            self.ssl_context.load_cert_chain(certfile_path, keyfile_path)\n\n    def get_base_url(self) -> str:\n\n        protocol = \"http\" if self.ssl_context is None else \"https\"\n        return (\n            f\"{protocol}://{self.server.server_address[0]}\"\n            f\":{self.server.server_address[1]}/\"\n        )\n\n    def add_handler(\n        self,\n        request_type: str,\n        path: str,\n        handle: RequestHandle,\n        description: str = \"no description\",\n        data: dict = None,\n        response_type: str = \"application/json\",\n    ) -> None:\n\n        mapper: HttpRequestMapper\n        mapper = self.server.mapper\n        return mapper.add_handler(\n            request_type, path, handle, description, data, response_type\n        )\n\n    def close(self) -> bool:\n\n        with self.lock:\n            if self.get_state() != DaemonState.IDLE or self.closed:\n                return False\n            self.closed = True\n\n        self.server.server_close()\n        return True\n\n    def serve(self, *args, main_thread: MainThread = None, **kwargs) -> int:\n\n        result = super().serve(*args, main_thread=main_thread, **kwargs)\n        assert self.close()\n        return result\n\n    def run(self, *_, **kwargs) -> None:\n\n        if not self.closed:\n            if \"first_start\" in kwargs and \"service_registry\" in kwargs:\n                first_start: bool = kwargs[\"first_start\"]\n                service_registry: ServiceRegistry = kwargs[\"service_registry\"]\n                if first_start:\n                    service_registry.add(\n                        self.name, [self.server.server_address]\n                    )\n            if self.ssl_context is not None:\n                with self.ssl_context.wrap_socket(\n                    self.server.socket, server_side=True\n                ) as ssock:\n                    self.server.socket = ssock\n                    self.server.serve_forever(0.1)\n            else:\n                self.server.serve_forever(0.1)",
    "docstring": "Allows daemonic management of an http server."
}
{
    "class": "class Analyzer(object):\r\n    \r\n    def __repr__(self):\r\n        return \"%s()\" % self.__class__.__name__\r\n\r\n    def __eq__(self, other):\r\n        return self.__class__ is other.__class__ and self.__dict__ == other.__dict__\r\n\r\n    def __call__(self, value):\r\n        raise NotImplementedError\r\n    \r\n    def clean(self):\r\n        pass",
    "docstring": "Abstract base class for analyzers. Since the analyzer protocol is just\r\n    __call__, this is pretty simple -- it mostly exists to provide common\r\n    implementations of __repr__ and __eq__."
}
{
    "class": "class CustomerRates(Resource):\n    response = None\n    args = None\n    args_wrap = None\n\n    def __init__(self):\n        request.dataDictWrap = dict((col, request.args.get(col)) for col in request.args)\n        if request.dataDictWrap:\n            _add_argument(reqparse, field_inputs_wrap, location=['dataDictWrap'])\n\n        super(CustomerRates, self).__init__()\n\n    def get(self):\n\n        validSet = set(field_inputs_wrap.keys())\n        requestArgsSet = set(request.dataDictWrap.keys())\n\n        if not ExtraParamsIsValid(requestArgsSet, validSet):\n            return omitError(ErrorMsg=repr(requestArgsSet.difference(validSet))), 400\n\n        try:\n            for k, v in field_inputs.items():\n                field_inputs[k]['required'] = False\n\n            orgArgs, self.args = GetRequestArgs(None, field_inputs_wrap,\n                    dict((col, request.args.get(col)) for col in request.args))\n            self.response = dict(marshal(self.args, resource_fields_wrap).items())\n\n            self.args[field_inputs_wrap_head] = []\n        except Exception as error:\n            logger.debug('traceback.format_exc(%s)', traceback.format_exc())\n\n            return omitError(ErrorMsg=repr(error)), 400\n\n        itemsPerPage = self.response['itemsPerPage']\n        page = self.response['page']\n        orderBy = getattr(obj, self.response['orderBy'])\n        isDesc = getattr(orderBy, 'desc' if self.response['desc'] else 'asc')\n\n        r = obj.query.filter(obj.isdel == False).order_by(isDesc())\n        _r = r.all()\n        self.args['total'] = len(_r)\n\n        if itemsPerPage is not 0:\n            _r = r.offset(itemsPerPage * (page-1))\\\n                    .limit(itemsPerPage)\\\n                    .all()\n\n        r = _r\n        for _r in r:\n            __r = dict((col, getattr(_r, col)) for col in _r.__table__.columns.keys())\n            self.args[field_inputs_wrap_head].append(__r)\n\n\n        _resource_fields = resource_fields.copy()\n        _resource_fields_wrap = resource_fields_wrap.copy()\n        _resource_fields_wrap[field_inputs_wrap_head] = fields.List(fields.Nested(_resource_fields))\n        self.args['type'] = \"business\"\n        self.args['subtype'] = \"rate\"\n        return marshal(self.args, _resource_fields_wrap), 200\n\n    def post(self):\n\n\n        try:\n            orgArgs, self.args = GetTwoLayerRequestArgs(field_inputs_wrap_head, field_inputs_post)\n\n        except Exception as error:\n            logger.debug('traceback.format_exc(%s)', traceback.format_exc())\n\n            return omitError(ErrorMsg=repr(error)), 400\n\n        r = obj.query.filter(obj.user_id == self.args['user_id'],\n                obj.business_id == self.args['business_id'],\n                obj.isdel == False).scalar()\n\n        if r is not None:\n            return omitError('CE_DATA_DUPLICATE',\n                    'user_id {}, business_id {} are duplicate'.format(self.args['user_id'],\n                        self.args['business_id'])), 400\n\n\n        if db.session.query(obj).filter(obj.isdel == False).count() > max:\n            return omitError('CE_EXCEED_LIMIT', 'limit is {}'.format(max)), 400\n\n\n        r = obj()\n        try:\n            r = PrepareObjORM(r, self.args.items())\n\n        except Exception as error:\n            return omitError(ErrorMsg=repr(error)), 400\n\n\n        try:\n            db.session.add(r)\n            db.session.flush()\n            db.session.refresh(r)\n            db.session.commit()\n        except Exception as error:\n            db.session.rollback()\n            logger.warning('session commit error(%s)', error)\n\n            if exc.IntegrityError == type(error):\n                return omitError('CE_NAME_CONFLICT', repr(error)), 400\n\n            return omitError(ErrorMsg=repr(error)), 400\n\n\n        out = SerialObjOutput(r, objname=field_inputs_wrap_head,\n                resource_fields=resource_fields_post), 200\n\n        next(iter(out))[field_inputs_wrap_head].update({'id': r.id})\n        next(iter(out))['type'] = 'business'\n        next(iter(out))['subtype'] = 'rate'\n\n        return out\n\n    def delete(self):\n        try:\n            ids = request.args.get(__heads__).split(',')\n        except Exception as error:\n            return omitError(ErrorMsg='param `{}` not found'.format(__heads__)), 400\n\n        for id in ids:\n            try:\n                id = inputs.natural(id)\n            except Exception as error:\n                return omitError(ErrorMsg='id `{}` not int'.format(id)), 400\n\n            r = obj.query.filter(obj.id == id, obj.isdel == False).scalar()\n            if r is None:\n                return omitError('CE_NOT_EXIST',\n                        'id {} not found'.format(id)), 400\n\n        _r = []\n        for id in ids:\n            id = inputs.natural(id)\n\n            r = obj.query.filter(obj.id == id, obj.isdel == False).scalar()\n            r.isdel = True\n            _r.append(r)\n\n\n        try:\n            for v in _r:\n                db.session.merge(v)\n\n            db.session.flush()\n            db.session.commit()\n        except Exception as error:\n            logger.warning('session commit error(%s)', error)\n            db.session.rollback()\n            return omitError(ErrorMsg=repr(error)), 400\n\n        return '', 204",
    "docstring": "entry point about '/rest/customer/rates'\n        when user do below actions:\n        delete/create/getall"
}
{
    "class": "class roa_prefix_ipv4(roa_prefix):\n\n\n  range_type = resource_range_ipv4",
    "docstring": "IPv4 ROA prefix."
}
{
    "class": "class SwissDeckType(IntEnum):\n\n\tSWISS_DECK_NONE = 0\n\tSWISS_DECK_CONQUEST = 1\n\tSWISS_DECK_LAST_STAND = 2",
    "docstring": "PegasusUtilTournament.SwissDeckType"
}
{
    "class": "class First(Take):\n\n    def __init__(self, **kwargs):\n        super().__init__(num=1, **kwargs)",
    "docstring": "Take the first item"
}
{
    "class": "class ConvUpLayer(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 bias=True,\n                 bias_init_val=0,\n                 activate=True):\n        super(ConvUpLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n\n        if bias and not activate:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter('bias', None)\n\n        if activate:\n            if bias:\n                self.activation = FusedLeakyReLU(out_channels)\n            else:\n                self.activation = ScaledLeakyReLU(0.2)\n        else:\n            self.activation = None\n\n    def forward(self, x):\n        out = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        out = F.conv2d(\n            out,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        if self.activation is not None:\n            out = self.activation(out)\n        return out",
    "docstring": "Convolutional upsampling layer. It uses bilinear upsampler + Conv.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input. Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        activate (bool): Whether use activateion. Default: True."
}
{
    "class": "class FalseKneesTapas(GenericTapasComic):\n\n    name = \"falseknees-tapa\"\n    long_name = \"False Knees (from Tapas.io)\"\n    url = \"https://tapas.io/series/FalseKnees\"\n    _categories = (\"FALSEKNEES\",)",
    "docstring": "Class to retrieve False Knees comics."
}
{
    "class": "class DeleteUserStatItems2(Operation):\n\n\n    _url: str = \"/social/v2/admin/namespaces/{namespace}/users/{userId}/stats/{statCode}/statitems\"\n    _method: str = \"DELETE\"\n    _consumes: List[str] = []\n    _produces: List[str] = [\"application/json\"]\n    _securities: List[List[str]] = [[\"BEARER_AUTH\"], [\"BEARER_AUTH\"]]\n    _location_query: str = None\n\n    namespace: str\n    stat_code: str\n    user_id: str\n    additional_key: str\n\n\n\n    @property\n    def url(self) -> str:\n        return self._url\n\n    @property\n    def method(self) -> str:\n        return self._method\n\n    @property\n    def consumes(self) -> List[str]:\n        return self._consumes\n\n    @property\n    def produces(self) -> List[str]:\n        return self._produces\n\n    @property\n    def securities(self) -> List[List[str]]:\n        return self._securities\n\n    @property\n    def location_query(self) -> str:\n        return self._location_query\n\n\n\n\n\n    def get_all_params(self) -> dict:\n        return {\n            \"path\": self.get_path_params(),\n            \"query\": self.get_query_params(),\n        }\n\n    def get_path_params(self) -> dict:\n        result = {}\n        if hasattr(self, \"namespace\"):\n            result[\"namespace\"] = self.namespace\n        if hasattr(self, \"stat_code\"):\n            result[\"statCode\"] = self.stat_code\n        if hasattr(self, \"user_id\"):\n            result[\"userId\"] = self.user_id\n        return result\n\n    def get_query_params(self) -> dict:\n        result = {}\n        if hasattr(self, \"additional_key\"):\n            result[\"additionalKey\"] = self.additional_key\n        return result\n\n\n\n\n\n    def with_namespace(self, value: str) -> DeleteUserStatItems2:\n        self.namespace = value\n        return self\n\n    def with_stat_code(self, value: str) -> DeleteUserStatItems2:\n        self.stat_code = value\n        return self\n\n    def with_user_id(self, value: str) -> DeleteUserStatItems2:\n        self.user_id = value\n        return self\n\n    def with_additional_key(self, value: str) -> DeleteUserStatItems2:\n        self.additional_key = value\n        return self\n\n\n\n    def to_dict(self, include_empty: bool = False) -> dict:\n        result: dict = {}\n        if hasattr(self, \"namespace\") and self.namespace:\n            result[\"namespace\"] = str(self.namespace)\n        elif include_empty:\n            result[\"namespace\"] = \"\"\n        if hasattr(self, \"stat_code\") and self.stat_code:\n            result[\"statCode\"] = str(self.stat_code)\n        elif include_empty:\n            result[\"statCode\"] = \"\"\n        if hasattr(self, \"user_id\") and self.user_id:\n            result[\"userId\"] = str(self.user_id)\n        elif include_empty:\n            result[\"userId\"] = \"\"\n        if hasattr(self, \"additional_key\") and self.additional_key:\n            result[\"additionalKey\"] = str(self.additional_key)\n        elif include_empty:\n            result[\"additionalKey\"] = \"\"\n        return result\n\n\n\n    def parse_response(self, code: int, content_type: str, content: Any) -> Tuple[None, Union[None, ErrorEntity, HttpResponse]]:\n        pre_processed_response, error = self.pre_process_response(code=code, content_type=content_type, content=content)\n        if error is not None:\n            return None, None if error.is_no_content() else error\n        code, content_type, content = pre_processed_response\n\n        if code == 204:\n            return None, None\n        if code == 401:\n            return None, ErrorEntity.create_from_dict(content)\n        if code == 403:\n            return None, ErrorEntity.create_from_dict(content)\n        if code == 404:\n            return None, ErrorEntity.create_from_dict(content)\n\n        return None, self.handle_undocumented_response(code=code, content_type=content_type, content=content)\n\n\n\n    @classmethod\n    def create(\n        cls,\n        namespace: str,\n        stat_code: str,\n        user_id: str,\n        additional_key: Optional[str] = None,\n    ) -> DeleteUserStatItems2:\n        instance = cls()\n        instance.namespace = namespace\n        instance.stat_code = stat_code\n        instance.user_id = user_id\n        if additional_key is not None:\n            instance.additional_key = additional_key\n        return instance\n\n    @classmethod\n    def create_from_dict(cls, dict_: dict, include_empty: bool = False) -> DeleteUserStatItems2:\n        instance = cls()\n        if \"namespace\" in dict_ and dict_[\"namespace\"] is not None:\n            instance.namespace = str(dict_[\"namespace\"])\n        elif include_empty:\n            instance.namespace = \"\"\n        if \"statCode\" in dict_ and dict_[\"statCode\"] is not None:\n            instance.stat_code = str(dict_[\"statCode\"])\n        elif include_empty:\n            instance.stat_code = \"\"\n        if \"userId\" in dict_ and dict_[\"userId\"] is not None:\n            instance.user_id = str(dict_[\"userId\"])\n        elif include_empty:\n            instance.user_id = \"\"\n        if \"additionalKey\" in dict_ and dict_[\"additionalKey\"] is not None:\n            instance.additional_key = str(dict_[\"additionalKey\"])\n        elif include_empty:\n            instance.additional_key = \"\"\n        return instance\n\n    @staticmethod\n    def get_field_info() -> Dict[str, str]:\n        return {\n            \"namespace\": \"namespace\",\n            \"statCode\": \"stat_code\",\n            \"userId\": \"user_id\",\n            \"additionalKey\": \"additional_key\",\n        }\n\n    @staticmethod\n    def get_required_map() -> Dict[str, bool]:\n        return {\n            \"namespace\": True,\n            \"statCode\": True,\n            \"userId\": True,\n            \"additionalKey\": False,\n        }\n",
    "docstring": "Delete User's statItems (deleteUserStatItems_2)\n\n    Delete user's stat items for given namespace, statCode, and user Id.\n    If query param *additionalKey* is provided, it will delete user stat items of specific key (i.e. characterName).\n    Otherwise, it will delete all stat items related to the user Id.\n\n    Delete user's statItems given stat code.\n    Other detail info:\n\n      *  Required permission : resource=\"ADMIN:NAMESPACE:{namespace}:USER:{userId}:STATITEM\", action=8 (DELETE)\n      *  Returns : no content\n\n    Required Permission(s):\n        - ADMIN:NAMESPACE:{namespace}:USER:{userId}:STATITEM [DELETE]\n\n    Properties:\n        url: /social/v2/admin/namespaces/{namespace}/users/{userId}/stats/{statCode}/statitems\n\n        method: DELETE\n\n        tags: [\"UserStatistic\"]\n\n        consumes: []\n\n        produces: [\"application/json\"]\n\n        securities: [BEARER_AUTH] or [BEARER_AUTH]\n\n        namespace: (namespace) REQUIRED str in path\n\n        stat_code: (statCode) REQUIRED str in path\n\n        user_id: (userId) REQUIRED str in path\n\n        additional_key: (additionalKey) OPTIONAL str in query\n\n    Responses:\n        204: No Content - (delete successfully)\n\n        401: Unauthorized - ErrorEntity (20001: unauthorized access)\n\n        403: Forbidden - ErrorEntity (20013: insufficient permission)\n\n        404: Not Found - ErrorEntity (12242: Stat item of [{statCode}] of user [{profileId}] cannot be found in namespace [{namespace}])"
}
{
    "class": "class PollIOLoop(IOLoop):\n    def initialize(self, impl, time_func=None):\n        super(PollIOLoop, self).initialize()\n        self._impl = impl\n        if hasattr(self._impl, 'fileno'):\n            set_close_exec(self._impl.fileno())\n        self.time_func = time_func or time.time\n        self._handlers = {}\n        self._events = {}\n        self._callbacks = []\n        self._callback_lock = threading.Lock()\n        self._timeouts = []\n        self._cancellations = 0\n        self._running = False\n        self._stopped = False\n        self._closing = False\n        self._thread_ident = None\n        self._blocking_signal_threshold = None\n\n        self._waker = Waker()\n        self.add_handler(self._waker.fileno(),\n                         lambda fd, events: self._waker.consume(),\n                         self.READ)\n\n    def close(self, all_fds=False):\n        with self._callback_lock:\n            self._closing = True\n        self.remove_handler(self._waker.fileno())\n        if all_fds:\n            for fd in self._handlers.keys():\n                try:\n                    close_method = getattr(fd, 'close', None)\n                    if close_method is not None:\n                        close_method()\n                    else:\n                        os.close(fd)\n                except Exception:\n                    gen_log.debug(\"error closing fd %s\", fd, exc_info=True)\n        self._waker.close()\n        self._impl.close()\n\n    def add_handler(self, fd, handler, events):\n        self._handlers[fd] = stack_context.wrap(handler)\n        self._impl.register(fd, events | self.ERROR)\n\n    def update_handler(self, fd, events):\n        self._impl.modify(fd, events | self.ERROR)\n\n    def remove_handler(self, fd):\n        self._handlers.pop(fd, None)\n        self._events.pop(fd, None)\n        try:\n            self._impl.unregister(fd)\n        except Exception:\n            gen_log.debug(\"Error deleting fd from IOLoop\", exc_info=True)\n\n    def set_blocking_signal_threshold(self, seconds, action):\n        if not hasattr(signal, \"setitimer\"):\n            gen_log.error(\"set_blocking_signal_threshold requires a signal module \"\n                          \"with the setitimer method\")\n            return\n        self._blocking_signal_threshold = seconds\n        if seconds is not None:\n            signal.signal(signal.SIGALRM,\n                          action if action is not None else signal.SIG_DFL)\n\n    def start(self):\n        self._setup_logging()\n        if self._stopped:\n            self._stopped = False\n            return\n        old_current = getattr(IOLoop._current, \"instance\", None)\n        IOLoop._current.instance = self\n        self._thread_ident = thread.get_ident()\n        self._running = True\n\n        old_wakeup_fd = None\n        if hasattr(signal, 'set_wakeup_fd') and os.name == 'posix':\n            try:\n                old_wakeup_fd = signal.set_wakeup_fd(self._waker.write_fileno())\n                if old_wakeup_fd != -1:\n                    signal.set_wakeup_fd(old_wakeup_fd)\n                    old_wakeup_fd = None\n            except ValueError:\n                pass\n\n        try:\n            while True:\n                poll_timeout = _POLL_TIMEOUT\n\n                with self._callback_lock:\n                    callbacks = self._callbacks\n                    self._callbacks = []\n                for callback in callbacks:\n                    self._run_callback(callback)\n                callbacks = callback = None\n\n                if self._timeouts:\n                    now = self.time()\n                    while self._timeouts:\n                        if self._timeouts[0].callback is None:\n                            heapq.heappop(self._timeouts)\n                            self._cancellations -= 1\n                        elif self._timeouts[0].deadline <= now:\n                            timeout = heapq.heappop(self._timeouts)\n                            self._run_callback(timeout.callback)\n                            del timeout\n                        else:\n                            seconds = self._timeouts[0].deadline - now\n                            poll_timeout = min(seconds, poll_timeout)\n                            break\n                    if (self._cancellations > 512\n                            and self._cancellations > (len(self._timeouts) >> 1)):\n                        self._cancellations = 0\n                        self._timeouts = [x for x in self._timeouts\n                                          if x.callback is not None]\n                        heapq.heapify(self._timeouts)\n\n                if self._callbacks:\n                    poll_timeout = 0.0\n\n                if not self._running:\n                    break\n\n                if self._blocking_signal_threshold is not None:\n                    signal.setitimer(signal.ITIMER_REAL, 0, 0)\n\n                try:\n                    event_pairs = self._impl.poll(poll_timeout)\n                except Exception as e:\n                    if (getattr(e, 'errno', None) == errno.EINTR or\n                        (isinstance(getattr(e, 'args', None), tuple) and\n                         len(e.args) == 2 and e.args[0] == errno.EINTR)):\n                        continue\n                    else:\n                        raise\n\n                if self._blocking_signal_threshold is not None:\n                    signal.setitimer(signal.ITIMER_REAL,\n                                     self._blocking_signal_threshold, 0)\n\n                self._events.update(event_pairs)\n                while self._events:\n                    fd, events = self._events.popitem()\n                    try:\n                        self._handlers[fd](fd, events)\n                    except (OSError, IOError) as e:\n                        if e.args[0] == errno.EPIPE:\n                            pass\n                        else:\n                            self.handle_callback_exception(self._handlers.get(fd))\n                    except Exception:\n                        self.handle_callback_exception(self._handlers.get(fd))\n\n        finally:\n            self._stopped = False\n            if self._blocking_signal_threshold is not None:\n                signal.setitimer(signal.ITIMER_REAL, 0, 0)\n            IOLoop._current.instance = old_current\n            if old_wakeup_fd is not None:\n                signal.set_wakeup_fd(old_wakeup_fd)\n\n    def stop(self):\n        self._running = False\n        self._stopped = True\n        self._waker.wake()\n\n    def time(self):\n        return self.time_func()\n\n    def add_timeout(self, deadline, callback):\n        timeout = _Timeout(deadline, stack_context.wrap(callback), self)\n        heapq.heappush(self._timeouts, timeout)\n        return timeout\n\n    def remove_timeout(self, timeout):\n        timeout.callback = None\n        self._cancellations += 1\n\n    def add_callback(self, callback, *args, **kwargs):\n        with self._callback_lock:\n            if self._closing:\n                raise RuntimeError(\"IOLoop is closing\")\n            list_empty = not self._callbacks\n            self._callbacks.append(functools.partial(\n                stack_context.wrap(callback), *args, **kwargs))\n            if list_empty and thread.get_ident() != self._thread_ident:\n                self._waker.wake()\n\n    def add_callback_from_signal(self, callback, *args, **kwargs):\n        with stack_context.NullContext():\n            if thread.get_ident() != self._thread_ident:\n                self.add_callback(callback, *args, **kwargs)\n            else:\n                self._callbacks.append(functools.partial(\n                    stack_context.wrap(callback), *args, **kwargs))",
    "docstring": "Base class for IOLoops built around a select-like function.\n\n    For concrete implementations, see `tornado.platform.epoll.EPollIOLoop`\n    (Linux), `tornado.platform.kqueue.KQueueIOLoop` (BSD and Mac), or\n    `tornado.platform.select.SelectIOLoop` (all platforms)."
}
{
    "class": "class Cache(object):\n\n    def __init__(self, size):\n        self.slots = []\n\n        for row in range(size):\n            self.slots.append(CacheRow(slot=row))\n\n    def getAddressValue(self,address):\n        slot = (0b000011110000 & address) >> 4\n        offset = (0b000000001111 & address)\n        return self.slots[slot].data[offset]\n\n    def checkCache(self,address):\n\n        slotMASK = 0b000011110000\n        slot = (slotMASK & address) >> 4\n\n        tagMASK = 0b111100000000\n        tag = (tagMASK & address) >> 8\n\n        offsetMASK = 0b000000001111\n        offset = (offsetMASK & address)\n\n        if self.slots[slot].valid == 1:\n            if self.slots[slot].tag == tag:\n                return True\n        else:\n            return False\n\n    def isDirty(self,address):\n        slotMASK = 0b000011110000\n        slot = (slotMASK & address) >> 4\n        return self.slots[slot].dirty\n\n    def getBlock(self,address):\n        slotMASK = 0b000011110000\n        slot = (slotMASK & address) >> 4\n        return self.slots[slot]\n\n    def insertBlock(self, cacheRowObject):\n        self.slots[cacheRowObject.slot] = cacheRowObject\n\n    def updateData(self, address, value):\n        slotMASK = 0b000011110000\n        slot = (slotMASK & address) >> 4\n\n        offsetMASK = 0b000000001111\n        offset = (offsetMASK & address)\n\n        self.slots[slot].data[offset] = value\n        self.slots[slot].dirty = True\n\n    def __str__(self):\n        header = '\\n{0:>6}'+\\\n                '{1:>6}'+\\\n                '{2:>6}'+\\\n                '{3:>6}'+\\\n                '{4:>10}\\n'\n        toReturn = header.format('Slot','Valid','Tag', 'Dirty', 'Data')\n        toReturn += '---------------'+\\\n                    '----------------'+\\\n                    '----------------'+\\\n                    '--------------------------\\n'\n\n\n        for row in self.slots:\n            toReturn += str(row)+'\\n'\n        return toReturn",
    "docstring": "Cache object simulates the running cache with several methods\n\n        getAddressValue(self, address)\n            returns value of data at given address\n\n        checkCache(self,address)\n            checks to see if given address is in the cache\n\n        insertBlock((self, cacheRowObject)\n            insert given cacheRow into cache at appropiate row\n\n        updateData(self, address, value)\n            updated given address with new data value"
}
{
    "class": "class PubsubConfig(_messages.Message):\n  r\"\"\"Configuration to publish a Cloud Pub/Sub message.\n\n  Enums:\n    MessageFormatValueValuesEnum: The format of the Cloud Pub/Sub messages.\n\n  Fields:\n    messageFormat: The format of the Cloud Pub/Sub messages.\n    serviceAccountEmail: Email address of the service account used for\n      publishing Cloud Pub/Sub messages. This service account needs to be in\n      the same project as the PubsubConfig. When added, the caller needs to\n      have iam.serviceAccounts.actAs permission on this service account. If\n      unspecified, it defaults to the compute engine default service account.\n    topic: A topic of Cloud Pub/Sub. Values are of the form\n      `projects/<project>/topics/<topic>`. The project needs to be the same\n      project as this config is in.\n\n    Values:\n      MESSAGE_FORMAT_UNSPECIFIED: Unspecified.\n      PROTOBUF: The message payload is a serialized protocol buffer of\n        SourceRepoEvent.\n      JSON: The message payload is a JSON string of SourceRepoEvent.",
    "docstring": "r\"\"\"Configuration to publish a Cloud Pub/Sub message.\n\n  Enums:\n    MessageFormatValueValuesEnum: The format of the Cloud Pub/Sub messages.\n\n  Fields:\n    messageFormat: The format of the Cloud Pub/Sub messages.\n    serviceAccountEmail: Email address of the service account used for\n      publishing Cloud Pub/Sub messages. This service account needs to be in\n      the same project as the PubsubConfig. When added, the caller needs to\n      have iam.serviceAccounts.actAs permission on this service account. If\n      unspecified, it defaults to the compute engine default service account.\n    topic: A topic of Cloud Pub/Sub. Values are of the form\n      `projects/<project>/topics/<topic>`. The project needs to be the same\n      project as this config is in."
}
{
    "class": "class RandomFlip(RandomTransform):\n\n    def __init__(\n            self,\n            axes: Union[int, Tuple[int, ...]] = 0,\n            flip_probability: float = 0.5,\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.axes = self.parse_axes(axes)\n        self.flip_probability = self.parse_probability(\n            flip_probability,\n        )\n\n    def apply_transform(self, sample: Subject) -> dict:\n        axes_to_flip_hot = self.get_params(self.axes, self.flip_probability)\n        random_parameters_dict = {'axes': axes_to_flip_hot}\n        items = sample.get_images_dict(intensity_only=False).items()\n        for image_name, image_dict in items:\n            data = image_dict[DATA]\n            is_2d = data.shape[-3] == 1\n            dims = []\n            for dim, flip_this in enumerate(axes_to_flip_hot):\n                if not flip_this:\n                    continue\n                actual_dim = dim + 1\n                if is_2d:\n                    actual_dim += 1\n                if actual_dim > 3:\n                    message = (\n                        f'Image \"{image_name}\" with shape {data.shape} seems to'\n                        ' be 2D, so all axes must be in (0, 1),'\n                        f' but they are {self.axes}'\n                    )\n                    raise RuntimeError(message)\n                dims.append(actual_dim)\n            data = torch.flip(data, dims=dims)\n            image_dict[DATA] = data\n        sample.add_transform(self, random_parameters_dict)\n        return sample\n\n    @staticmethod\n    def get_params(axes: Tuple[int, ...], probability: float) -> List[bool]:\n        axes_hot = [False, False, False]\n        for axis in axes:\n            random_number = torch.rand(1)\n            flip_this = bool(probability > random_number)\n            axes_hot[axis] = flip_this\n        return axes_hot\n\n    @staticmethod\n    def parse_axes(axes: Union[int, Tuple[int, ...]]):\n        axes_tuple = to_tuple(axes)\n        for axis in axes_tuple:\n            is_int = isinstance(axis, int)\n            if not is_int or axis not in (0, 1, 2):\n                raise ValueError('All axes must be 0, 1 or 2')\n        return axes_tuple",
    "docstring": "Reverse the order of elements in an image along the given axes.\n\n    Args:\n        axes: Axis or tuple of axes along which the image will be flipped.\n        flip_probability: Probability that the image will be flipped. This is\n            computed on a per-axis basis.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    .. note:: If the input image is 2D, all axes should be in ``(0, 1)``."
}
{
    "class": "class E_ODM(Enum):\n    CLINICAL_DATA = odm('ClinicalData')\n    SUBJECT_DATA = odm('SubjectData')\n    STUDY_EVENT_DATA = odm('StudyEventData')\n    FORM_DATA = odm('FormData')\n    ITEM_GROUP_DATA = odm('ItemGroupData')\n    ITEM_DATA = odm('ItemData')\n    METADATA_VERSION = odm('MetaDataVersion')\n\n    USER_REF = odm('UserRef')\n    SOURCE_ID = odm('SourceID')\n    DATE_TIME_STAMP_ = odm('DateTimeStamp')\n    REASON_FOR_CHANGE = odm('ReasonForChange')\n    LOCATION_REF = odm('LocationRef')\n    QUERY = mdsol('Query')\n    PROTOCOL_DEVIATION = mdsol('ProtocolDeviation')\n    REVIEW = mdsol('Review')\n    COMMENT = mdsol('Comment')\n    SIGNATURE = odm('Signature')\n    SIGNATURE_REF = odm('SignatureRef')\n    SOURCEID = odm('SourceID')\n\n    ITEM_DEF = odm('ItemDef')\n    RANGE_CHECK = odm('RangeCheck')\n    CODELIST_REF = odm('CodeListRef')\n    CODELIST = odm('CodeList')\n    CODELIST_ITEM = odm('CodeListItem')\n    ENUMERATED_ITEM = odm('EnumeratedItem')",
    "docstring": "Defines ODM Elements"
}
{
    "class": "class PagesRequest(Request, Dynamic):\n    dirname: str = \"pages\"\n    endpoint: str = \"pages\"\n\n    @property\n    def data(self):\n        return Pages(self.request_data)",
    "docstring": "Class for github pages requests.\n       https://docs.github.com/en/free-pro-team@latest/rest/reference/repos#pages"
}
{
    "class": "class Geometry1p1DMicro(Geometry):\n\n    def __init__(self, custom_geometry={}):\n        super().__init__()\n\n        var = pybamm.standard_spatial_vars\n        l_n = pybamm.geometric_parameters.l_n\n        l_s = pybamm.geometric_parameters.l_s\n\n        self[\"negative particle\"] = {\n            \"primary\": {var.r_n: {\"min\": pybamm.Scalar(0), \"max\": pybamm.Scalar(1)}},\n            \"secondary\": {var.x_n: {\"min\": pybamm.Scalar(0), \"max\": l_n}},\n        }\n        self[\"positive particle\"] = {\n            \"primary\": {var.r_p: {\"min\": pybamm.Scalar(0), \"max\": pybamm.Scalar(1)}},\n            \"secondary\": {var.x_p: {\"min\": l_n + l_s, \"max\": pybamm.Scalar(1)}},\n        }\n        self.update(custom_geometry)",
    "docstring": "A geometry class to store the details features of the 1+1D cell geometry.\n    This is the geometry used in the standard DFN or P2D model.\n\n    **Extends**: :class:`Geometry`\n\n    Parameters\n    ----------\n\n    custom_geometry : dict containing any extra user defined geometry"
}
{
    "class": "class RandomCardViewSet(viewsets.ReadOnlyModelViewSet):\n    serializer_class = ResponseSerializer\n    def get_queryset(self):\n        q_id = self.request.query_params.get('q', None)\n        queryset = Response.objects.exclude(a__isnull=True)\n        if q_id is not None:\n            queryset = Response.objects.filter(q=q_id).order_by('?')[:1]\n        else:\n            queryset = Response.objects.all().order_by('?')[:1]\n        return queryset",
    "docstring": "Read-only API endpoint returning a random card. An optional `q` query string parameter will filter possible responses to the question to which they respond."
}
{
    "class": "class Job(Document):\n\n    user = StringField(required=True)\n    authstrat = StringField(\n        required=True, default=\"kbaseworkspace\", validation=valid_authstrat\n    )\n    wsid = IntField(required=False, default=None)\n    status = StringField(required=True, validation=valid_status)\n\n    updated = FloatField(default=time.time)\n\n    queued = FloatField(\n        default=None\n    )\n    estimating = FloatField(\n        default=None\n    )\n    running = FloatField(default=None)\n    finished = FloatField(default=None)\n\n    errormsg = StringField()\n    msg = StringField()\n    error = DynamicField()\n\n    terminated_code = IntField(validation=valid_termination_code)\n    error_code = IntField(validation=valid_errorcode)\n    batch_job = BooleanField(default=False)\n    scheduler_type = StringField()\n    scheduler_id = StringField()\n    scheduler_estimator_id = StringField()\n    job_input = EmbeddedDocumentField(JobInput, required=True)\n    job_output = DynamicField()\n    condor_job_ads = DynamicField()\n    batch_id = StringField()\n    child_jobs = ListField()\n    retry_ids = ListField()\n    retry_parent = StringField()\n    retry_saved_toggle = BooleanField(\n        default=False\n    )\n\n    meta = {\"collection\": \"ee2_jobs\"}\n\n    def save(self, *args, **kwargs):\n        self.updated = time.time()\n        return super(Job, self).save(*args, **kwargs)\n\n    def __repr__(self):\n        return self.to_json()",
    "docstring": "A job is created the execution engine service and it's updated from\n    the job and the portal process for the rest of the time"
}
{
    "class": "class PrimitiveResourceProviderEmbedded(PrimitiveResourceProvider):\n\n    def __init__(self, host):\n        self.host = host\n\n    def programming_interface(self, variant):\n        if variant == ProgInterfaceIcspC8D24:\n            return ProgInterfaceIcspC8D24Embedded(self.host)\n        raise Exception(\"Invalid ICSP variant requested.\")\n\n    def debug_interface(self, family):\n        return DebugInterfaceEmbedded(self.host)\n\n    def board_interface(self):\n        return BoardInterfaceEmbedded(self.host)\n\n    def pin_driver(self):\n        return HardwareInterfaceEmbedded(self.host)",
    "docstring": "Resource provider for Embedded"
}
{
    "class": "class MqttLight(Light):\n\n    def __init__(self, hass, name, topic, templates, qos, retain, payload,\n                 optimistic, brightness_scale):\n        self._hass = hass\n        self._name = name\n        self._topic = topic\n        self._qos = qos\n        self._retain = retain\n        self._payload = payload\n        self._optimistic = optimistic or topic[\"state_topic\"] is None\n        self._optimistic_rgb = optimistic or topic[\"rgb_state_topic\"] is None\n        self._optimistic_brightness = (optimistic or\n                                       topic[\"brightness_state_topic\"] is None)\n        self._brightness_scale = brightness_scale\n        self._state = False\n        self._supported_features = 0\n        self._supported_features |= (\n            topic['rgb_state_topic'] is not None and SUPPORT_RGB_COLOR)\n        self._supported_features |= (\n            topic['brightness_state_topic'] is not None and SUPPORT_BRIGHTNESS)\n\n        templates = {key: ((lambda value: value) if tpl is None else\n                           partial(render_with_possible_json_value, hass, tpl))\n                     for key, tpl in templates.items()}\n\n        def state_received(topic, payload, qos):\n            payload = templates['state'](payload)\n            if payload == self._payload[\"on\"]:\n                self._state = True\n            elif payload == self._payload[\"off\"]:\n                self._state = False\n\n            self.update_ha_state()\n\n        if self._topic[\"state_topic\"] is not None:\n            mqtt.subscribe(self._hass, self._topic[\"state_topic\"],\n                           state_received, self._qos)\n\n        def brightness_received(topic, payload, qos):\n            device_value = float(templates['brightness'](payload))\n            percent_bright = device_value / self._brightness_scale\n            self._brightness = int(percent_bright * 255)\n            self.update_ha_state()\n\n        if self._topic[\"brightness_state_topic\"] is not None:\n            mqtt.subscribe(self._hass, self._topic[\"brightness_state_topic\"],\n                           brightness_received, self._qos)\n            self._brightness = 255\n        elif self._topic[\"brightness_command_topic\"] is not None:\n            self._brightness = 255\n        else:\n            self._brightness = None\n\n        def rgb_received(topic, payload, qos):\n            self._rgb = [int(val) for val in\n                         templates['rgb'](payload).split(',')]\n            self.update_ha_state()\n\n        if self._topic[\"rgb_state_topic\"] is not None:\n            mqtt.subscribe(self._hass, self._topic[\"rgb_state_topic\"],\n                           rgb_received, self._qos)\n            self._rgb = [255, 255, 255]\n        if self._topic[\"rgb_command_topic\"] is not None:\n            self._rgb = [255, 255, 255]\n        else:\n            self._rgb = None\n\n    @property\n    def brightness(self):\n        return self._brightness\n\n    @property\n    def rgb_color(self):\n        return self._rgb\n\n    @property\n    def should_poll(self):\n        return False\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def is_on(self):\n        return self._state\n\n    @property\n    def assumed_state(self):\n        return self._optimistic\n\n    @property\n    def supported_features(self):\n        return self._supported_features\n\n    def turn_on(self, **kwargs):\n        should_update = False\n\n        if ATTR_RGB_COLOR in kwargs and \\\n           self._topic[\"rgb_command_topic\"] is not None:\n\n            mqtt.publish(self._hass, self._topic[\"rgb_command_topic\"],\n                         \"{},{},{}\".format(*kwargs[ATTR_RGB_COLOR]),\n                         self._qos, self._retain)\n\n            if self._optimistic_rgb:\n                self._rgb = kwargs[ATTR_RGB_COLOR]\n                should_update = True\n\n        if ATTR_BRIGHTNESS in kwargs and \\\n           self._topic[\"brightness_command_topic\"] is not None:\n            percent_bright = float(kwargs[ATTR_BRIGHTNESS]) / 255\n            device_brightness = int(percent_bright * self._brightness_scale)\n            mqtt.publish(self._hass, self._topic[\"brightness_command_topic\"],\n                         device_brightness, self._qos, self._retain)\n\n            if self._optimistic_brightness:\n                self._brightness = kwargs[ATTR_BRIGHTNESS]\n                should_update = True\n\n        mqtt.publish(self._hass, self._topic[\"command_topic\"],\n                     self._payload[\"on\"], self._qos, self._retain)\n\n        if self._optimistic:\n            self._state = True\n            should_update = True\n\n        if should_update:\n            self.update_ha_state()\n\n    def turn_off(self, **kwargs):\n        mqtt.publish(self._hass, self._topic[\"command_topic\"],\n                     self._payload[\"off\"], self._qos, self._retain)\n\n        if self._optimistic:\n            self._state = False\n            self.update_ha_state()",
    "docstring": "MQTT light."
}
{
    "class": "class FilterModule(object):\n  def filters(self):\n    return {\n        'dotted_dict': dotted_dict\n    }",
    "docstring": "Converts CloudFormation Template Parameters to Stack Input mappings"
}
{
    "class": "class LineEndStyle(Enum):\n\n    BUTT = _JLineEndStyle.BUTT\n\n    ROUND = _JLineEndStyle.ROUND\n\n    SQUARE = _JLineEndStyle.SQUARE",
    "docstring": "An enum defining styles for shapes drawn at the end of a line."
}
{
    "class": "class AmazonS3Dataset(Dataset):\n\n    _validation = {\n        'linked_service_name': {'required': True},\n        'type': {'required': True},\n        'bucket_name': {'required': True},\n    }\n\n    _attribute_map = {\n        'additional_properties': {'key': '', 'type': '{object}'},\n        'description': {'key': 'description', 'type': 'str'},\n        'structure': {'key': 'structure', 'type': 'object'},\n        'schema': {'key': 'schema', 'type': 'object'},\n        'linked_service_name': {'key': 'linkedServiceName', 'type': 'LinkedServiceReference'},\n        'parameters': {'key': 'parameters', 'type': '{ParameterSpecification}'},\n        'annotations': {'key': 'annotations', 'type': '[object]'},\n        'folder': {'key': 'folder', 'type': 'DatasetFolder'},\n        'type': {'key': 'type', 'type': 'str'},\n        'bucket_name': {'key': 'typeProperties.bucketName', 'type': 'object'},\n        'key': {'key': 'typeProperties.key', 'type': 'object'},\n        'prefix': {'key': 'typeProperties.prefix', 'type': 'object'},\n        'version': {'key': 'typeProperties.version', 'type': 'object'},\n        'format': {'key': 'typeProperties.format', 'type': 'DatasetStorageFormat'},\n        'compression': {'key': 'typeProperties.compression', 'type': 'DatasetCompression'},\n    }\n\n    def __init__(self, **kwargs):\n        super(AmazonS3Dataset, self).__init__(**kwargs)\n        self.bucket_name = kwargs.get('bucket_name', None)\n        self.key = kwargs.get('key', None)\n        self.prefix = kwargs.get('prefix', None)\n        self.version = kwargs.get('version', None)\n        self.format = kwargs.get('format', None)\n        self.compression = kwargs.get('compression', None)\n        self.type = 'AmazonS3Object'",
    "docstring": "A single Amazon Simple Storage Service (S3) object or a set of S3 objects.\n\n    All required parameters must be populated in order to send to Azure.\n\n    :param additional_properties: Unmatched properties from the message are\n     deserialized this collection\n    :type additional_properties: dict[str, object]\n    :param description: Dataset description.\n    :type description: str\n    :param structure: Columns that define the structure of the dataset. Type:\n     array (or Expression with resultType array), itemType: DatasetDataElement.\n    :type structure: object\n    :param schema: Columns that define the physical type schema of the\n     dataset. Type: array (or Expression with resultType array), itemType:\n     DatasetSchemaDataElement.\n    :type schema: object\n    :param linked_service_name: Required. Linked service reference.\n    :type linked_service_name:\n     ~azure.mgmt.datafactory.models.LinkedServiceReference\n    :param parameters: Parameters for dataset.\n    :type parameters: dict[str,\n     ~azure.mgmt.datafactory.models.ParameterSpecification]\n    :param annotations: List of tags that can be used for describing the\n     Dataset.\n    :type annotations: list[object]\n    :param folder: The folder that this Dataset is in. If not specified,\n     Dataset will appear at the root level.\n    :type folder: ~azure.mgmt.datafactory.models.DatasetFolder\n    :param type: Required. Constant filled by server.\n    :type type: str\n    :param bucket_name: Required. The name of the Amazon S3 bucket. Type:\n     string (or Expression with resultType string).\n    :type bucket_name: object\n    :param key: The key of the Amazon S3 object. Type: string (or Expression\n     with resultType string).\n    :type key: object\n    :param prefix: The prefix filter for the S3 object name. Type: string (or\n     Expression with resultType string).\n    :type prefix: object\n    :param version: The version for the S3 object. Type: string (or Expression\n     with resultType string).\n    :type version: object\n    :param format: The format of files.\n    :type format: ~azure.mgmt.datafactory.models.DatasetStorageFormat\n    :param compression: The data compression method used for the Amazon S3\n     object.\n    :type compression: ~azure.mgmt.datafactory.models.DatasetCompression"
}
{
    "class": "class FileCopy(object):\n\n    def __init__(self, argument_spec):\n        self.spec = argument_spec\n        self.module = None\n        self.init_module()\n\n        self.local_file = self.module.params['local_file']\n        self.remote_file = self.module.params['remote_file']\n        self.file_system = self.module.params['file_system']\n\n        self.transfer_result = None\n        self.changed = False\n\n    def init_module(self):\n\n        self.module = AnsibleModule(\n            argument_spec=self.spec, supports_check_mode=True)\n\n    def remote_file_exists(self, dst, file_system='flash:'):\n\n        full_path = file_system + dst\n        file_name = os.path.basename(full_path)\n        file_path = os.path.dirname(full_path)\n        file_path = file_path + '/'\n        xml_str = CE_NC_GET_FILE_INFO % (file_name, file_path)\n        ret_xml = get_nc_config(self.module, xml_str)\n        if \"<data/>\" in ret_xml:\n            return False, 0\n\n        xml_str = ret_xml.replace('\\r', '').replace('\\n', '').\\\n            replace('xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"', \"\").\\\n            replace('xmlns=\"http://www.huawei.com/netconf/vrp\"', \"\")\n\n        root = ElementTree.fromstring(xml_str)\n        topo = root.find(\"data/vfm/dirs/dir\")\n        if topo is None:\n            return False, 0\n\n        for eles in topo:\n            if eles.tag in [\"DirSize\"]:\n                return True, int(eles.text.replace(',', ''))\n\n        return False, 0\n\n    def local_file_exists(self):\n\n        return os.path.isfile(self.local_file)\n\n    def enough_space(self):\n\n        commands = list()\n        cmd = 'dir %s' % self.file_system\n        commands.append(cmd)\n        output = run_commands(self.module, commands)\n        if not output:\n            return True\n\n        match = re.search(r'\\((.*) KB free\\)', output[0])\n        kbytes_free = match.group(1)\n        kbytes_free = kbytes_free.replace(',', '')\n        file_size = os.path.getsize(self.local_file)\n        if int(kbytes_free) * 1024 > file_size:\n            return True\n\n        return False\n\n    def transfer_file(self, dest):\n\n        if not self.local_file_exists():\n            self.module.fail_json(\n                msg='Could not transfer file. Local file doesn\\'t exist.')\n\n        if not self.enough_space():\n            self.module.fail_json(\n                msg='Could not transfer file. Not enough space on device.')\n\n        hostname = self.module.params['provider']['host']\n        username = self.module.params['provider']['username']\n        password = self.module.params['provider']['password']\n        port = self.module.params['provider']['port']\n\n        ssh = paramiko.SSHClient()\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        ssh.connect(hostname=hostname, username=username, password=password, port=port)\n        full_remote_path = '{}{}'.format(self.file_system, dest)\n        scp = SCPClient(ssh.get_transport())\n        try:\n            scp.put(self.local_file, full_remote_path)\n        except:\n            time.sleep(10)\n            file_exists, temp_size = self.remote_file_exists(\n                dest, self.file_system)\n            file_size = os.path.getsize(self.local_file)\n            if file_exists and int(temp_size) == int(file_size):\n                pass\n            else:\n                scp.close()\n                self.module.fail_json(msg='Could not transfer file. There was an error '\n                                      'during transfer. Please make sure the format of '\n                                      'input parameters is right.')\n\n        scp.close()\n        return True\n\n    def get_scp_enable(self):\n\n        xml_str = CE_NC_GET_SCP_ENABLE\n        ret_xml = get_nc_config(self.module, xml_str)\n        if \"<data/>\" in ret_xml:\n            return False\n\n        xml_str = ret_xml.replace('\\r', '').replace('\\n', '').\\\n            replace('xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"', \"\").\\\n            replace('xmlns=\"http://www.huawei.com/netconf/vrp\"', \"\")\n\n        root = ElementTree.fromstring(xml_str)\n        topo = root.find(\"data/sshs/sshServer\")\n        if topo is None:\n            return False\n\n        for eles in topo:\n            if eles.tag in [\"scpEnable\"]:\n                return True, eles.text\n\n        return False\n\n    def work(self):\n\n        if not HAS_SCP:\n            self.module.fail_json(\n                msg=\"'Error: No scp package, please install it.'\")\n\n        if not HAS_PARAMIKO:\n            self.module.fail_json(\n                msg=\"'Error: No paramiko package, please install it.'\")\n\n        if self.local_file and len(self.local_file) > 4096:\n            self.module.fail_json(\n                msg=\"'Error: The maximum length of local_file is 4096.'\")\n\n        if self.remote_file and len(self.remote_file) > 4096:\n            self.module.fail_json(\n                msg=\"'Error: The maximum length of remote_file is 4096.'\")\n\n        retcode, cur_state = self.get_scp_enable()\n        if retcode and cur_state == 'Disable':\n            self.module.fail_json(\n                msg=\"'Error: Please ensure SCP server is enabled.'\")\n\n        if not os.path.isfile(self.local_file):\n            self.module.fail_json(\n                msg=\"Local file {} not found\".format(self.local_file))\n\n        dest = self.remote_file or ('/' + os.path.basename(self.local_file))\n        remote_exists, file_size = self.remote_file_exists(\n            dest, file_system=self.file_system)\n        if remote_exists and (os.path.getsize(self.local_file) != file_size):\n            remote_exists = False\n\n        if not remote_exists:\n            self.changed = True\n            file_exists = False\n        else:\n            file_exists = True\n            self.transfer_result = 'The local file already exists on the device.'\n\n        if not file_exists:\n            self.transfer_file(dest)\n            self.transfer_result = 'The local file has been successfully transferred to the device.'\n\n        if self.remote_file is None:\n            self.remote_file = '/' + os.path.basename(self.local_file)\n\n        self.module.exit_json(\n            changed=self.changed,\n            transfer_result=self.transfer_result,\n            local_file=self.local_file,\n            remote_file=self.remote_file,\n            file_system=self.file_system)",
    "docstring": "File copy function class"
}
{
    "class": "class IndexBoundsHandler(object):\n    def __init__(self, global_shape):\n        self._global_shape = global_shape\n\n    def __getitem__(self, index):\n        return self.out_of_bounds(index)\n\n    def int_out_of_bounds(self, index, axis=0):\n        if index > self._global_shape[axis]:\n            raise IndexError('index is larger than the upper bound')\n\n        if index < 0:\n            index += self._global_shape[axis]\n\n        if index < 0:\n            raise IndexError('index is smaller than the lower bound')\n\n        return index\n\n    def slice_out_of_bounds(self, index, axis=0):\n        start = index.start\n        stop = index.stop\n\n        if start is None:\n            start = 0\n\n        if stop is None:\n            stop = self._global_shape[axis]\n\n\n        index_start = self.int_out_of_bounds(start, axis)\n        index_stop = self.int_out_of_bounds(stop, axis)\n\n        return slice(index_start, index_stop, index.step)\n\n    def out_of_bounds(self, index):\n        if type(index) is int:\n            return self.int_out_of_bounds(index)\n\n        elif type(index) is slice:\n            return self.slice_out_of_bounds(index)\n\n        elif type(index) is tuple:\n\n            local_index = []\n\n            for k, item in enumerate(index):\n\n                if type(item) is slice:\n                    temp_index = self.slice_out_of_bounds(item, k)\n\n                elif type(item) is int:\n                    temp_index = self.int_out_of_bounds(item, k)\n                if temp_index is None:\n                    return temp_index\n\n                local_index.append(temp_index)\n\n            return tuple(local_index)",
    "docstring": "class to handle bound errors in mpi wrapper"
}
{
    "class": "class BaseAsyncResult(object):\n\n    TimeoutError = TimeoutError\n\n    def __init__(self, task_id, backend):\n        self.task_id = task_id\n        self.backend = backend\n\n    @with_connection\n    def revoke(self, connection=None, connect_timeout=None):\n        from celery.task import control\n        control.revoke(self.task_id)\n\n    def get(self, timeout=None):\n        return self.wait(timeout=timeout)\n\n    def wait(self, timeout=None):\n        return self.backend.wait_for(self.task_id, timeout=timeout)\n\n    def ready(self):\n        status = self.backend.get_status(self.task_id)\n        return status not in self.backend.UNREADY_STATES\n\n    def successful(self):\n        return self.backend.is_successful(self.task_id)\n\n    def __str__(self):\n        return self.task_id\n\n    def __hash__(self):\n        return hash(self.task_id)\n\n    def __repr__(self):\n        return \"<AsyncResult: %s>\" % self.task_id\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.task_id == other.task_id\n        return other == self.task_id\n\n    def __copy__(self):\n        return self.__class__(self.task_id, backend=self.backend)\n\n    @property\n    def result(self):\n        return self.backend.get_result(self.task_id)\n\n    @property\n    def traceback(self):\n        return self.backend.get_traceback(self.task_id)\n\n    @property\n    def status(self):\n        return self.backend.get_status(self.task_id)",
    "docstring": "Base class for pending result, supports custom task result backend.\n\n    :param task_id: see :attr:`task_id`.\n    :param backend: see :attr:`backend`.\n\n    .. attribute:: task_id\n\n        The unique identifier for this task.\n\n    .. attribute:: backend\n\n        The task result backend used."
}
{
    "class": "class _MOM_Error_ (Exception, metaclass = TFL.Meta.Object.__class__) :\n\n    _real_name       = \"Error\"\n\n    arg_sep          = \", \"\n    is_required      = False\n\n    _json_attributes = \\\n        ( \"as_text\"\n        , \"is_required\"\n        )\n\n    _json_map        = dict \\\n        ( as_text    = \"description\"\n        )\n\n    _rank_d          = 1\n    _rank_s          = 1\n\n    @Once_Property\n    def as_json_cargo (self) :\n        json_map = self._json_map\n        result   = {}\n        for k in self._json_attributes :\n            v = getattr (self, k, None)\n            if v :\n                result [json_map.get (k, k)] = v\n        return result\n\n    @Once_Property\n    def as_text (self) :\n        result = self.arg_sep.join (self.str_arg (self.args))\n        if not result :\n            result = _T (self.__class__.__doc__)\n        return result\n\n    @Once_Property\n    def encoded (self) :\n        return pyk.encoded (self.as_text)\n\n    @Once_Property\n    def rank (self) :\n        return (self._rank_s, self._rank_d)\n\n    def str_arg (self, args) :\n        for a in args :\n            try :\n                s = pyk.decoded (a)\n            except Exception as exc :\n                s = \"%s --> %s\" % (portable_repr (a), exc)\n            yield s\n\n    def __eq__ (self, other) :\n        return self.encoded == pyk.encoded (other)\n\n    def __le__ (self, other) :\n        return self.encoded <= pyk.encoded (other)\n\n    def __lt__ (self, other) :\n        return self.encoded < pyk.encoded (other)\n\n    def __hash__ (self) :\n        return hash (self.encoded)\n\n    def __repr__ (self) :\n        return pyk.reprify (str (self))\n\n    def __str__ (self) :\n        return self.as_text",
    "docstring": "Root class of MOM exceptions"
}
{
    "class": "class Parameter(Vendor):\n\n    FLAG = 'isinstance(value, int) and 0 <= value <= 65535'\n    LINK = 'https://www.iana.org/assignments/hip-parameters/hip-parameters-4.csv'\n\n    def process(self, data):\n        reader = csv.reader(data)\n        next(reader)\n\n        enum = list()\n        miss = list()\n        for item in reader:\n            long = item[1]\n            plen = item[2]\n            rfcs = item[3]\n\n            match = re.match(r'(\\w*) *(\\(.*\\))*', long)\n            group = match.groups()\n\n            name = group[0]\n            cmmt = f' {group[1]}' if group[1] else ''\n            plen = f' {plen}' if re.match(r'\\d+', plen) else ''\n\n            temp = list()\n            for rfc in filter(None, re.split(r'\\[|\\]', rfcs)):\n                if 'RFC' in rfc and re.match(r'\\d+', rfc[3:]):\n                    temp.append(f'[:rfc:`{rfc[3:]}`]')\n                else:\n                    temp.append(f'[{rfc}]'.replace('_', ' '))\n            tmp1 = f\" {''.join(temp)}\" if rfcs else ''\n            desc = self.wrap_comment(f\"{name}{tmp1}{plen}{cmmt}\")\n\n            try:\n                code, _ = item[0], int(item[0])\n                renm = self.rename(name, code, original=long)\n\n                pres = f\"{renm} = {code}\"\n                sufs = f\"\n\n\n                enum.append(f'{sufs}\\n    {pres}')\n            except ValueError:\n                start, stop = item[0].split('-')\n\n                miss.append(f'if {start} <= value <= {stop}:')\n                miss.append(f\"\n                miss.append(f\"    extend_enum(cls, '{self.safe_name(name)}_%d' % value, value)\")\n                miss.append('    return cls(value)')\n        return enum, miss",
    "docstring": "HIP Parameter Types"
}
{
    "class": "class GPToolbox(object):\n\n    _imports_template = Template('''\nimport os\n\ntry:\n    from urllib.request import urlopen, pathname2url, urlretrieve\n    from urllib.parse import urlparse, urljoin\nexcept ImportError:\n    from urllib2 import urlopen\n    from urllib import pathname2url, urlretrieve\n    from urlparse import urlparse, urljoin\n\nimport time\nimport arcpy\nfrom gsf import Task\n\nclass Toolbox(object):\n    def __init__(self):\n        self.label = \"$label\"\n        self.alias = \"$alias\"\n\n        self.tools = $toolList\nclass $taskName(object):\n    def __init__(self):\n        self.label = \"$taskDisplayName\"\n        self.description = \"$taskDescription\"\n        self.canRunInBackground = $canRunInBackground\n        self.task = Task(\"$taskUri\")\n\n    def getParameterInfo(self):\n\n        $parameterInfo\n\n    def isLicensed(self):\n        return True\n\n    def updateParameters(self, parameters):\n        $updateParameter\n        return\n\n    def updateMessages(self, parameters):\n        return\n\n    def execute(self, parameters, messages):\n        $preExecute\n        \n        job = self.task.submit(input_params)\n        messages.AddMessage('Submitted Job to: ' + self.task.uri)\n        messages.AddMessage('Submit Job ID: ' + str(job.job_id))\n\n        job.wait_for_done()\n        \n        if job.status == 'Failed':\n            messages.addErrorMessage(\"Task failed to execute\")\n            messages.addErrorMessage(job.error_message)\n            raise arcpy.ExecuteError\n\n        messages.AddMessage('Job Finished')\n        task_results = job.results\n        $postExecute\n        return\n",
    "docstring": "GPToolbox is used to create GPTool wrappers to GSF tasks"
}
{
    "class": "class GivenNamesV30Rc1(object):\n    swagger_types = {\n        'value': 'str'\n    }\n\n    attribute_map = {\n        'value': 'value'\n    }\n\n    def __init__(self, value=None):\n        self._value = None\n        self.discriminator = None\n        if value is not None:\n            self.value = value\n\n    @property\n    def value(self):\n        return self._value\n\n    @value.setter\n    def value(self, value):\n\n        self._value = value\n\n    def to_dict(self):\n        result = {}\n\n        for attr, _ in six.iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n        if issubclass(GivenNamesV30Rc1, dict):\n            for key, value in self.items():\n                result[key] = value\n\n        return result\n\n    def to_str(self):\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n        return self.to_str()\n\n    def __eq__(self, other):\n        if not isinstance(other, GivenNamesV30Rc1):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        return not self == other",
    "docstring": "NOTE: This class is auto generated by the swagger code generator program.\n\n    Do not edit the class manually."
}
{
    "class": "class Record(object):\n    def __init__(self, chrom, pos, id, ref, alt, qual, filter, info, format=None, genotypes=None):\n        self.chrom = chrom\n        self.pos = pos\n        self.id = id\n        self.ref = ref\n        self.alt = alt\n        self.qual = qual\n        self.filter = filter\n        self.info = info\n        self.format = format\n        self.genotypes = genotypes \n\n\n    def __str__(self):\n        fields = [self.chrom, self.pos, self.id, self.ref, self.alt, self.qual]\n        if self.filter is not None:\n            fields.append(self.filter)\n        if self.genotypes is not None:\n            fields.extend(self.genotypes)\n        return '\\t'.join(fields)",
    "docstring": "info should be a dictionary\n    genotypes, if present, should be a list"
}
{
    "class": "class CupertinoSearchBar(BoxLayout):\n\n    background_color = ColorProperty([0.85, 0.85, 0.85, 0.7])\n\n    foreground_color = ColorProperty([0, 0, 0, 1])\n\n    symbol_color = ColorProperty([0.55, 0.55, 0.6, 1])",
    "docstring": "iOS style search bar\n\n    .. image:: ../_static/search_bar/demo.gif"
}
{
    "class": "class Hopkins2007QLF(DoublePowerLawLF):\n\n    def __init__(self):\n\n\n        log_phi_star = Parameter(-4.825, 'log_phi_star', one_sigma_unc=[0.06,\n                                                                        0.06])\n        log_lum_star = Parameter(13.036, 'log_lum_star', one_sigma_unc=[\n            0.043, 0.043])\n        gamma_one = Parameter(0.417, 'gamma_one', one_sigma_unc=[0.055, 0.055])\n        gamma_two = Parameter(2.174, 'gamma_two', one_sigma_unc=[0.055, 0.055])\n\n        kl1 = Parameter(0.632, 'kl1', one_sigma_unc=[0.077, 0.077])\n        kl2 = Parameter(-11.76, 'kl2', one_sigma_unc=[0.38, 0.38])\n        kl3 = Parameter(-14.25, 'kl2', one_sigma_unc=[0.8, 0.8])\n\n        kg1 = Parameter(-0.623, 'kg1', one_sigma_unc=[0.132, 0.132])\n        kg2_1 = Parameter(1.46, 'kg2_1', one_sigma_unc=[0.096, 0.096])\n        kg2_2 = Parameter(-0.793, 'kg2_2', one_sigma_unc=[0.057, 0.057])\n\n        z_ref = Parameter(2.0, 'z_ref', vary=False)\n\n        parameters = {'log_phi_star': log_phi_star,\n                      'log_lum_star': log_lum_star,\n                      'gamma_one': gamma_one,\n                      'gamma_two': gamma_two,\n                      'kl1': kl1,\n                      'kl2': kl2,\n                      'kl3': kl3,\n                      'kg1': kg1,\n                      'kg2_1': kg2_1,\n                      'kg2_2': kg2_2,\n                      'z_ref': z_ref}\n\n        param_functions = {'lum_star': self.lum_star,\n                           'phi_star': self.phi_star,\n                           'alpha': self.alpha,\n                           'beta': self.beta}\n\n        lum_type = 'bolometric'\n\n        super(Hopkins2007QLF, self).__init__(parameters, param_functions,\n                                             lum_type=lum_type)\n\n    @staticmethod\n    def lum_star(redsh, z_ref, log_lum_star, kl1, kl2, kl3):\n\n        xi = np.log10((1 + redsh) / (1 + z_ref))\n\n        log_lum_star = log_lum_star + kl1 * xi + kl2 * xi ** 2 + kl3 * xi ** 3\n\n        lum_star = pow(10, log_lum_star)\n\n        return lum_star\n\n    @staticmethod\n    def phi_star(log_phi_star):\n        return pow(10, log_phi_star)\n\n    @staticmethod\n    def alpha(redsh, z_ref, gamma_one, kg1):\n        xi = np.log10((1 + redsh) / (1 + z_ref))\n\n        alpha = gamma_one * pow(10, kg1 * xi)\n\n        return alpha\n\n    @staticmethod\n    def beta(redsh, z_ref, gamma_two, kg2_1, kg2_2):\n\n        xi = np.log10((1 + redsh) / (1 + z_ref))\n\n        beta = gamma_two * 2 /  (pow(10, kg2_1 * xi) +\n                                      ( pow(10,kg2_2 *  xi)))\n\n        if (beta < 1.3) & (redsh > 5.0):\n            beta = 1.3\n\n        return beta\n\n    def evaluate(self, lum, redsh, parameters=None):\n\n        if parameters is None:\n            parameters = self.parameters.copy()\n\n        main_parameter_values = self.evaluate_main_parameters(lum, redsh,\n                                                        parameters=parameters)\n\n        phi_star = main_parameter_values['phi_star']\n        lum_star = main_parameter_values['lum_star']\n        alpha = main_parameter_values['alpha']\n        beta = main_parameter_values['beta']\n\n        return lum_double_power_law(np.power(10, lum), phi_star, lum_star,\n                                              alpha, beta)",
    "docstring": "Implementation of the bolometric quasar luminosity function of\n    Hopkins+2007.\n\n    ADS reference: https://ui.adsabs.harvard.edu/abs/2007ApJ...654..731H/abstract\n\n    The luminosity function is described by Equations 6, 9, 10, 17, 19.\n    The values for the best fit model adopted here are found in Table 3 in\n    the row named \"Full\"."
}
{
    "class": "class TaxonomyItemVersion(MODEL, IdMixin, NameDescriptionMixin, CreateDeleteMixin):\n\n    __tablename__ = \"TaxonomyItemVersion\"\n\n    taxonomy_item_id: Column = DB.Column(\n        DB.Integer, ForeignKey(TaxonomyItem.id), nullable=False\n    )\n    version: Column = DB.Column(DB.Integer, nullable=False)\n    sort_key: Column = DB.Column(DB.Float, nullable=True)\n\n    @declared_attr\n    def __table_args__(cls):\n        return (\n            Index(\n                f\"ix_uq_version_{cls.__tablename__}\",\n                \"taxonomy_item_id\",\n                \"version\",\n                unique=True,\n            ),\n            Index(\n                f\"ix_search{cls.__tablename__}\",\n                \"name\",\n                \"description\",\n                **FULLTEXT_INDEX_PARAMS,\n            ),\n        )\n\n    taxonomy_item: TaxonomyItem = relationship(\n        TaxonomyItem,\n        innerjoin=True,\n        lazy=\"select\",\n        back_populates=\"versions\",\n        primaryjoin=TaxonomyItem.id == taxonomy_item_id,\n    )\n\n    def __init__(\n        self,\n        taxonomy_item: TaxonomyItem,\n        version: int,\n        name: str,\n        description: Optional[str] = None,\n        sort_key: Optional[float] = 10,\n        **kwargs,\n    ) -> None:\n        self.taxonomy_item = taxonomy_item\n        self.version = version\n        self.name = name\n        self.description = description\n        self.sort_key = sort_key if sort_key is not None else 10",
    "docstring": "Taxonomy Item version model."
}
{
    "class": "class SceneSpecificNetwork(nn.Module):\r\n    def __init__(self, action_space_size):\r\n        super(SceneSpecificNetwork, self).__init__()\r\n        self.fc1 = nn.Linear(512, 512)\r\n\r\n        self.fc2_policy = nn.Linear(512, action_space_size)\r\n\r\n        self.fc2_value = nn.Linear(512, 1)\r\n\r\n    def forward(self, x):\r\n        x = self.fc1(x)\r\n        x = F.relu(x)\r\n        x_policy = self.fc2_policy(x)\r\n\r\n        x_value = self.fc2_value(x)[0]\r\n        return (x_policy, x_value, )",
    "docstring": "Input for this network is 512 tensor"
}
{
    "class": "class Line(Projection):\n    def __init__(self, p=None, v=None, p0=None, p1=None):\n        Projection.__init__(self)\n        if p is not None and v is not None:\n            self.p = Vector(p).to_2d()\n            self.v = Vector(v).to_2d()\n        elif p0 is not None and p1 is not None:\n            self.p = Vector(p0).to_2d()\n            self.v = Vector(p1).to_2d() - self.p\n        else:\n            self.p = Vector((0, 0))\n            self.v = Vector((0, 0))\n        self.line = None\n\n    @property\n    def copy(self):\n        return Line(self.p.copy(), self.v.copy())\n\n    @property\n    def p0(self):\n        return self.p\n\n    @property\n    def p1(self):\n        return self.p + self.v\n\n    @p0.setter\n    def p0(self, p0):\n        p1 = self.p1\n        self.p = Vector(p0).to_2d()\n        self.v = p1 - p0\n\n    @p1.setter\n    def p1(self, p1):\n        self.v = Vector(p1).to_2d() - self.p\n\n    @property\n    def length(self):\n        return self.v.length\n\n    @property\n    def angle(self):\n        return atan2(self.v.y, self.v.x)\n\n    @property\n    def a0(self):\n        return self.angle\n\n    @property\n    def angle_normal(self):\n        return atan2(-self.v.x, self.v.y)\n\n    @property\n    def reversed(self):\n        return Line(self.p, -self.v)\n\n    @property\n    def oposite(self):\n        return Line(self.p + self.v, -self.v)\n\n    @property\n    def cross_z(self):\n        return Vector((self.v.y, -self.v.x))\n\n    @property\n    def cross(self):\n        return Vector((self.v.y, -self.v.x))\n\n    def signed_angle(self, u, v):\n        return atan2(u.x * v.y - u.y * v.x, u.x * v.x + u.y * v.y)\n\n    def delta_angle(self, last):\n        if last is None:\n            return self.angle\n        return self.signed_angle(last.straight(1, 1).v, self.straight(1, 0).v)\n\n    def normal(self, t=0):\n        return Line(self.lerp(t), self.cross_z)\n\n    def sized_normal(self, t, size):\n        return Line(self.lerp(t), size * self.cross_z.normalized())\n\n    def lerp(self, t):\n        return self.p + self.v * t\n\n    def intersect(self, line):\n        c = line.cross_z\n        d = self.v * c\n        if d == 0:\n            return False, 0, 0\n        t = (c * (line.p - self.p)) / d\n        return True, self.lerp(t), t\n\n    def intersect_ext(self, line):\n        c = line.cross_z\n        d = self.v * c\n        if d == 0:\n            return False, 0, 0, 0\n        dp = line.p - self.p\n        c2 = self.cross_z\n        u = (c * dp) / d\n        v = (c2 * dp) / d\n        return u > 0 and v > 0 and u < 1 and v < 1, self.lerp(u), u, v\n\n    def point_sur_segment(self, pt):\n        dp = pt - self.p\n        dl = self.length\n        if dl == 0:\n            return dp.length < 0.00001, 0, 0\n        d = (self.v.x * dp.y - self.v.y * dp.x) / dl\n        t = (self.v * dp) / (dl * dl)\n        return t > 0 and t < 1, d, t\n\n    def steps(self, len):\n        steps = max(1, round(self.length / len, 0))\n        return 1 / steps, int(steps)\n\n    def in_place_offset(self, offset):\n        self.p += offset * self.cross_z.normalized()\n\n    def offset(self, offset):\n        return Line(self.p + offset * self.cross_z.normalized(), self.v)\n\n    def tangeant(self, t, da, radius):\n        p = self.lerp(t)\n        if da < 0:\n            c = p + radius * self.cross_z.normalized()\n        else:\n            c = p - radius * self.cross_z.normalized()\n        return Arc(c, radius, self.angle_normal, da)\n\n    def straight(self, length, t=1):\n        return Line(self.lerp(t), self.v.normalized() * length)\n\n    def translate(self, dp):\n        self.p += dp\n\n    def rotate(self, a):\n        ca = cos(a)\n        sa = sin(a)\n        self.v = Matrix([\n            [ca, -sa],\n            [sa, ca]\n            ]) * self.v\n        return self\n\n    def scale(self, length):\n        self.v = length * self.v.normalized()\n        return self\n\n    def tangeant_unit_vector(self, t):\n        return self.v.normalized()\n\n    def as_curve(self, context):\n        curve = bpy.data.curves.new('LINE', type='CURVE')\n        curve.dimensions = '2D'\n        spline = curve.splines.new('POLY')\n        spline.use_endpoint_u = False\n        spline.use_cyclic_u = False\n        pts = self.pts\n        spline.points.add(len(pts) - 1)\n        for i, p in enumerate(pts):\n            x, y, z = p\n            spline.points[i].co = (x, y, 0, 1)\n        curve_obj = bpy.data.objects.new('LINE', curve)\n        context.scene.objects.link(curve_obj)\n        curve_obj.select = True\n\n    def make_offset(self, offset, last=None):\n        line = self.offset(offset)\n        if last is None:\n            return line\n\n        if hasattr(last, \"r\"):\n            res, d, t = line.point_sur_segment(last.c)\n            c = (last.r * last.r) - (d * d)\n            if c <= 0:\n                p0 = line.lerp(t)\n            else:\n                if t > 0:\n                    p0 = line.lerp(t) - line.v.normalized() * sqrt(c)\n                else:\n                    p0 = line.lerp(t) + line.v.normalized() * sqrt(c)\n            u = last.p0 - last.c\n            v = p0 - last.c\n            da = self.signed_angle(u, v)\n            if last.ccw:\n                if da < 0:\n                    da = 2 * pi + da\n            elif da > 0:\n                da = 2 * pi - da\n            last.da = da\n            line.p0 = p0\n        else:\n            c = line.cross_z\n            d = last.v * c\n            if d == 0:\n                return line\n            v = line.p - last.p\n            t = (c * v) / d\n            c2 = last.cross_z\n            u = (c2 * v) / d\n            if u > 1 or t < 0:\n                return line\n            p = last.lerp(t)\n            line.p0 = p\n            last.p1 = p\n\n        return line\n\n    @property\n    def pts(self):\n        return [self.p0.to_3d(), self.p1.to_3d()]",
    "docstring": "2d Line\n        Internally stored as p: origin and v:size and direction\n        moving p will move both ends of line\n        moving p0 or p1 move only one end of line\n            p1\n            ^\n            | v\n            p0 == p"
}
{
    "class": "class AsyncSOCKSProxy(AsyncConnectionPool):\n\n    def __init__(\n        self,\n        proxy_url: typing.Union[URL, bytes, str],\n        proxy_auth: typing.Optional[\n            typing.Tuple[typing.Union[bytes, str], typing.Union[bytes, str]]\n        ] = None,\n        ssl_context: typing.Optional[ssl.SSLContext] = None,\n        max_connections: typing.Optional[int] = 10,\n        max_keepalive_connections: typing.Optional[int] = None,\n        keepalive_expiry: typing.Optional[float] = None,\n        http1: bool = True,\n        http2: bool = False,\n        network_backend: typing.Optional[AsyncNetworkBackend] = None,\n    ) -> None:\n        super().__init__(\n            ssl_context=ssl_context,\n            max_connections=max_connections,\n            max_keepalive_connections=max_keepalive_connections,\n            keepalive_expiry=keepalive_expiry,\n            http1=http1,\n            http2=http2,\n            network_backend=network_backend,\n        )\n        self._ssl_context = ssl_context\n        self._proxy_url = enforce_url(proxy_url, name=\"proxy_url\")\n        if proxy_auth is not None:\n            username, password = proxy_auth\n            username_bytes = enforce_bytes(username, name=\"proxy_auth\")\n            password_bytes = enforce_bytes(password, name=\"proxy_auth\")\n            self._proxy_auth: typing.Optional[typing.Tuple[bytes, bytes]] = (\n                username_bytes,\n                password_bytes,\n            )\n        else:\n            self._proxy_auth = None\n\n    def create_connection(self, origin: Origin) -> AsyncConnectionInterface:\n        return AsyncSocks5Connection(\n            proxy_origin=self._proxy_url.origin,\n            remote_origin=origin,\n            proxy_auth=self._proxy_auth,\n            ssl_context=self._ssl_context,\n            keepalive_expiry=self._keepalive_expiry,\n            http1=self._http1,\n            http2=self._http2,\n            network_backend=self._network_backend,\n        )",
    "docstring": "A connection pool that sends requests via an HTTP proxy."
}
{
    "class": "class TypeAnnotation(FakeAnnotation):\n\n    supported_types = (\n        Union,\n        Any,\n        Dict,\n        List,\n        Set,\n        Optional,\n        Callable,\n        Generator,\n        Iterator,\n        IO,\n        overload,\n        Type,\n    )\n\n    type_name_map = (\n        (Union, \"Union\",),\n        (Any, \"Any\",),\n        (Dict, \"Dict\",),\n        (List, \"List\",),\n        (Set, \"Set\",),\n        (Optional, \"Optional\",),\n        (Callable, \"Callable\",),\n        (Generator, \"Generator\",),\n        (Iterator, \"Iterator\",),\n        (IO, \"IO\",),\n        (overload, \"overload\",),\n        (Type, \"Type\",),\n    )\n\n    def __init__(self, wrapped_type: Any) -> None:\n        if isinstance(wrapped_type, FakeAnnotation):\n            raise ValueError(f\"Cannot wrap FakeAnnotation: {wrapped_type}\")\n        if wrapped_type not in self.supported_types:\n            raise ValueError(f\"Cannot wrap {wrapped_type}\")\n\n        self.wrapped_type = wrapped_type\n\n    def render(self, parent_name: str = \"\") -> str:\n        return self.get_import_name()\n\n    def get_import_name(self) -> str:\n        for type_class, type_name in self.type_name_map:\n            if self.wrapped_type is type_class:\n                return type_name\n        raise ValueError(f\"Unknown type {self.wrapped_type}\")\n\n    def get_import_record(self) -> ImportRecord:\n        return ImportRecord(source=ImportString(\"typing\"), name=self.get_import_name())\n\n    def is_dict(self) -> bool:\n        return self.wrapped_type is Dict\n\n    def is_list(self) -> bool:\n        return self.wrapped_type is List\n\n    def copy(self) -> \"TypeAnnotation\":\n        return TypeAnnotation(self.wrapped_type)",
    "docstring": "Wrapper for simple type annotation like `str` or `Dict`.\n\n    Arguments:\n        wrapped_type -- Original type annotation."
}
{
    "class": "class ChatClient(object):\n\n    def __init__(\n        self, endpoint: str,\n        credential: CommunicationTokenCredential,\n        **kwargs: Any\n    ) -> None:\n\n        if not credential:\n            raise ValueError(\"credential can not be None\")\n\n        try:\n            if not endpoint.lower().startswith('http'):\n                endpoint = \"https://\" + endpoint\n        except AttributeError:\n            raise ValueError(\"Host URL must be a string\")\n\n        parsed_url = urlparse(endpoint.rstrip('/'))\n        if not parsed_url.netloc:\n            raise ValueError(\"Invalid URL: {}\".format(endpoint))\n\n        self._endpoint = endpoint\n        self._credential = credential\n\n        self._client = AzureCommunicationChatService(\n            self._endpoint,\n            authentication_policy=BearerTokenCredentialPolicy(self._credential),\n            sdk_moniker=SDK_MONIKER,\n            **kwargs)\n\n    @distributed_trace\n    def get_chat_thread_client(\n            self, thread_id: str,\n            **kwargs: Any\n    ) -> ChatThreadClient:\n\n        if not thread_id:\n            raise ValueError(\"thread_id cannot be None.\")\n\n        return ChatThreadClient(\n            endpoint=self._endpoint,\n            credential=self._credential,\n            thread_id=thread_id,\n            **kwargs\n        )\n\n    @distributed_trace_async\n    async def create_chat_thread(\n        self, topic: str,\n        **kwargs\n    ) -> CreateChatThreadResult:\n\n\n        if not topic:\n            raise ValueError(\"topic cannot be None.\")\n\n        idempotency_token = kwargs.pop('idempotency_token', None)\n        if idempotency_token is None:\n            idempotency_token = str(uuid4())\n\n        thread_participants = kwargs.pop('thread_participants', None)\n        participants = []\n        if thread_participants is not None:\n            participants = [m._to_generated() for m in thread_participants]\n\n        create_thread_request = \\\n            CreateChatThreadRequest(topic=topic, participants=participants)\n\n        create_chat_thread_result = await self._client.chat.create_chat_thread(\n            create_chat_thread_request=create_thread_request,\n            repeatability_request_id=idempotency_token,\n            **kwargs)\n\n        errors = None\n        if hasattr(create_chat_thread_result, 'errors') and \\\n                create_chat_thread_result.errors is not None:\n            errors = CommunicationErrorResponseConverter._convert(\n                participants=[thread_participants],\n                chat_errors=create_chat_thread_result.invalid_participants\n            )\n\n        chat_thread = ChatThreadProperties._from_generated(\n            create_chat_thread_result.chat_thread)\n\n        create_chat_thread_result = CreateChatThreadResult(\n            chat_thread=chat_thread,\n            errors=errors\n        )\n\n        return create_chat_thread_result\n\n\n    @distributed_trace\n    def list_chat_threads(\n        self,\n        **kwargs: Any\n    ):\n        results_per_page = kwargs.pop(\"results_per_page\", None)\n        start_time = kwargs.pop(\"start_time\", None)\n\n        return self._client.chat.list_chat_threads(\n            max_page_size=results_per_page,\n            start_time=start_time,\n            **kwargs)\n\n    @distributed_trace_async\n    async def delete_chat_thread(\n        self,\n        thread_id: str,\n        **kwargs\n    ) -> None:\n        if not thread_id:\n            raise ValueError(\"thread_id cannot be None.\")\n\n        return await self._client.chat.delete_chat_thread(thread_id, **kwargs)\n\n    async def close(self) -> None:\n        await self._client.close()\n\n    async def __aenter__(self) -> \"ChatClient\":\n        await self._client.__aenter__()\n        return self\n\n    async def __aexit__(self, *args) -> None:\n        await self._client.__aexit__(*args)",
    "docstring": "A client to interact with the AzureCommunicationService Chat gateway.\n\n    This client provides operations to create a chat thread, delete a chat thread,\n    get chat thread by id, list chat threads.\n\n    :param str endpoint:\n        The endpoint of the Azure Communication resource.\n    :param CommunicationTokenCredential credential:\n        The credentials with which to authenticate.\n\n    .. admonition:: Example:\n\n        .. literalinclude:: ../samples/chat_client_sample_async.py\n            :start-after: [START create_chat_client]\n            :end-before: [END create_chat_client]\n            :language: python\n            :dedent: 8\n            :caption: Creating the ChatClient from a URL and token."
}
{
    "class": "class InvalidRequestError(HEREError):\n",
    "docstring": "Invalid Request Error Type.\n\n    Indicates an invalid or missing parameter value in the request, for example value given for the product parameter does not exist."
}
{
    "class": "class TemplateParameter(_user_module.TemplateParameterMixin, Element):\n\n    default = EReference(ordered=False, unique=True, containment=False, derived=False)\n    ownedDefault = EReference(\n        ordered=False, unique=True, containment=True, derived=False\n    )\n    parameteredElement = EReference(\n        ordered=False, unique=True, containment=False, derived=False\n    )\n    signature = EReference(ordered=False, unique=True, containment=False, derived=False)\n    ownedParameteredElement = EReference(\n        ordered=False, unique=True, containment=True, derived=False\n    )\n\n    def __init__(\n        self,\n        default=None,\n        ownedDefault=None,\n        parameteredElement=None,\n        signature=None,\n        ownedParameteredElement=None,\n        **kwargs,\n    ):\n\n        super(TemplateParameter, self).__init__(**kwargs)\n\n        if default is not None:\n            self.default = default\n\n        if ownedDefault is not None:\n            self.ownedDefault = ownedDefault\n\n        if parameteredElement is not None:\n            self.parameteredElement = parameteredElement\n\n        if signature is not None:\n            self.signature = signature\n\n        if ownedParameteredElement is not None:\n            self.ownedParameteredElement = ownedParameteredElement",
    "docstring": "A TemplateParameter exposes a ParameterableElement as a formal parameter of a\n    template.\n    <p>From package UML::CommonStructure.</p>"
}
{
    "class": "class command_scp(HoneyPotCommand):\n    download_path = CowrieConfig().get('honeypot', 'download_path')\n    download_path_uniq = CowrieConfig().get('honeypot', 'download_path_uniq', fallback=download_path)\n\n    def help(self):\n        self.write(\n\n    def start(self):\n        try:\n            optlist, args = getopt.getopt(self.args, '12346BCpqrvfstdv:cFiloPS:')\n        except getopt.GetoptError:\n            self.help()\n            self.exit()\n            return\n\n        self.out_dir = ''\n\n        for opt in optlist:\n            if opt[0] == '-d':\n                self.out_dir = args[0]\n                break\n\n        if self.out_dir:\n            outdir = self.fs.resolve_path(self.out_dir, self.protocol.cwd)\n\n            if not self.fs.exists(outdir):\n                self.errorWrite('-scp: {}: No such file or directory\\n'.format(self.out_dir))\n                self.exit()\n\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n        self.write('\\x00')\n\n    def lineReceived(self, line):\n        log.msg(eventid='cowrie.session.file_download',\n                realm='scp',\n                input=line,\n                format='INPUT (%(realm)s): %(input)s')\n        self.protocol.terminal.write('\\x00')\n\n    def drop_tmp_file(self, data, name):\n        tmp_fname = '%s-%s-%s-scp_%s' % \\\n                    (time.strftime('%Y%m%d-%H%M%S'),\n                     self.protocol.getProtoTransport().transportId,\n                     self.protocol.terminal.transport.session.id,\n                     re.sub('[^A-Za-z0-9]', '_', name))\n\n        self.safeoutfile = os.path.join(self.download_path, tmp_fname)\n\n        with open(self.safeoutfile, 'wb+') as f:\n            f.write(data)\n\n    def save_file(self, data, fname):\n        self.drop_tmp_file(data, fname)\n\n        if os.path.exists(self.safeoutfile):\n            with open(self.safeoutfile, 'rb'):\n                shasum = hashlib.sha256(data).hexdigest()\n                hash_path = os.path.join(self.download_path_uniq, shasum)\n\n            if not os.path.exists(hash_path):\n                os.rename(self.safeoutfile, hash_path)\n                duplicate = False\n            else:\n                os.remove(self.safeoutfile)\n                duplicate = True\n\n            log.msg(format='SCP Uploaded file \\\"%(filename)s\\\" to %(outfile)s',\n                    eventid='cowrie.session.file_upload',\n                    filename=os.path.basename(fname),\n                    duplicate=duplicate,\n                    url=fname,\n                    outfile=shasum,\n                    shasum=shasum,\n                    destfile=fname)\n\n            self.safeoutfile = None\n\n            self.fs.update_realfile(self.fs.getfile(fname), hash_path)\n            self.fs.chown(fname, self.protocol.user.uid, self.protocol.user.gid)\n\n    def parse_scp_data(self, data):\n\n        pos = data.find('\\n')\n        if pos != -1:\n            header = data[:pos]\n\n            pos += 1\n\n            if re.match(r'^C0[\\d]{3} [\\d]+ [^\\s]+$', header):\n\n                r = re.search(r'C(0[\\d]{3}) ([\\d]+) ([^\\s]+)', header)\n\n                if r and r.group(1) and r.group(2) and r.group(3):\n\n                    dend = pos + int(r.group(2))\n\n                    if dend > len(data):\n                        dend = len(data)\n\n                    d = data[pos:dend]\n\n                    if self.out_dir:\n                        fname = os.path.join(self.out_dir, r.group(3))\n                    else:\n                        fname = r.group(3)\n\n                    outfile = self.fs.resolve_path(fname, self.protocol.cwd)\n\n                    try:\n                        self.fs.mkfile(outfile, 0, 0, r.group(2), r.group(1))\n                    except fs.FileNotFound:\n                        self.errorWrite('-scp: {}: No such file or directory\\n'.format(outfile))\n                        self.safeoutfile = None\n                        return ''\n\n                    self.save_file(d, outfile)\n\n                    data = data[dend + 1:]\n            else:\n                data = ''\n        else:\n            data = ''\n\n        return data\n\n    def handle_CTRL_D(self):\n        if self.protocol.terminal.stdinlogOpen and self.protocol.terminal.stdinlogFile and \\\n                os.path.exists(self.protocol.terminal.stdinlogFile):\n            with open(self.protocol.terminal.stdinlogFile, 'rb') as f:\n                data = f.read()\n                header = data[:data.find(b'\\n')]\n                if re.match(r'C0[\\d]{3} [\\d]+ [^\\s]+', header.decode()):\n                    data = data[data.find(b'\\n') + 1:]\n                else:\n                    data = ''\n\n            if data:\n                with open(self.protocol.terminal.stdinlogFile, 'wb') as f:\n                    f.write(data)\n\n        self.exit()",
    "docstring": "scp command"
}
{
    "class": "class Link(itypes.Object):\n    def __init__(self, url=None, action=None, encoding=None, transform=None, title=None, description=None, fields=None):\n        if (url is not None) and (not isinstance(url, string_types)):\n            raise TypeError(\"Argument 'url' must be a string.\")\n        if (action is not None) and (not isinstance(action, string_types)):\n            raise TypeError(\"Argument 'action' must be a string.\")\n        if (encoding is not None) and (not isinstance(encoding, string_types)):\n            raise TypeError(\"Argument 'encoding' must be a string.\")\n        if (transform is not None) and (not isinstance(transform, string_types)):\n            raise TypeError(\"Argument 'transform' must be a string.\")\n        if (title is not None) and (not isinstance(title, string_types)):\n            raise TypeError(\"Argument 'title' must be a string.\")\n        if (description is not None) and (not isinstance(description, string_types)):\n            raise TypeError(\"Argument 'description' must be a string.\")\n        if (fields is not None) and (not isinstance(fields, (list, tuple))):\n            raise TypeError(\"Argument 'fields' must be a list.\")\n        if (fields is not None) and any([\n            not (isinstance(item, string_types) or isinstance(item, Field))\n            for item in fields\n        ]):\n            raise TypeError(\"Argument 'fields' must be a list of strings or fields.\")\n\n        self._url = '' if (url is None) else url\n        self._action = '' if (action is None) else action\n        self._encoding = '' if (encoding is None) else encoding\n        self._transform = '' if (transform is None) else transform\n        self._title = '' if (title is None) else title\n        self._description = '' if (description is None) else description\n        self._fields = () if (fields is None) else tuple([\n            item if isinstance(item, Field) else Field(item, required=False, location='')\n            for item in fields\n        ])\n\n    @property\n    def url(self):\n        return self._url\n\n    @property\n    def action(self):\n        return self._action\n\n    @property\n    def encoding(self):\n        return self._encoding\n\n    @property\n    def transform(self):\n        return self._transform\n\n    @property\n    def title(self):\n        return self._title\n\n    @property\n    def description(self):\n        return self._description\n\n    @property\n    def fields(self):\n        return self._fields\n\n    def __eq__(self, other):\n        return (\n            isinstance(other, Link) and\n            self.url == other.url and\n            self.action == other.action and\n            self.encoding == other.encoding and\n            self.transform == other.transform and\n            self.description == other.description and\n            sorted(self.fields, key=lambda f: f.name) == sorted(other.fields, key=lambda f: f.name)\n        )\n\n    def __repr__(self):\n        return _repr(self)\n\n    def __str__(self):\n        return _str(self)",
    "docstring": "Links represent the actions that a client may perform."
}
{
    "class": "class ClientConnectorError(ClientOSError):\r\n    def __init__(self, connection_key: ConnectionKey,\r\n                 os_error: OSError) -> None:\r\n        self._conn_key = connection_key\r\n        self._os_error = os_error\r\n        super().__init__(os_error.errno, os_error.strerror)\r\n        self.args = (connection_key, os_error)\r\n\r\n    @property\r\n    def os_error(self) -> OSError:\r\n        return self._os_error\r\n\r\n    @property\r\n    def host(self) -> str:\r\n        return self._conn_key.host\r\n\r\n    @property\r\n    def port(self) -> Optional[int]:\r\n        return self._conn_key.port\r\n\r\n    @property\r\n    def ssl(self) -> Union[SSLContext, None, bool, 'Fingerprint']:\r\n        return self._conn_key.ssl\r\n\r\n    def __str__(self) -> str:\r\n        return ('Cannot connect to host {0.host}:{0.port} ssl:{1} [{2}]'\r\n                .format(self, self.ssl if self.ssl is not None else 'default',\r\n                        self.strerror))\r\n\r\n    __reduce__ = BaseException.__reduce__",
    "docstring": "Client connector error.\r\n\r\n    Raised in :class:`aiohttp.connector.TCPConnector` if\r\n        connection to proxy can not be established."
}
{
    "class": "class SetBondRotatableFlag(SetRotatableBonds):\n\n\n    def setupUndoBefore(self, atoms, rotatable):\n        self.addUndoCall( (atoms, not rotatable), {'redraw':1},\n                  self.name )\n\n\n    def __call__(self, atoms, rotatable, **kw):\n        atoms = self.vf.expandNodes(atoms)\n        if len(atoms)<2: return\n        assert isinstance( atoms[0], Atom )\n        self.doitWrapper(*(atoms, rotatable), **kw)\n\n\n    def doit(self, atoms, rotatable):\n        dict = self.vf.atorsDict\n\n        assert rotatable in [ 0, 1 ]\n        if len(atoms) < 2:\n            return 'ERROR'\n\n        bonds = atoms[:2].bonds\n        if len(bonds[0])==0:\n            print('ERROR: no bond between ...')\n            return 'ERROR'\n\n        bond = bonds[0][0]\n\n        mol = dict['molecule']\n        if not hasattr(mol, 'LPO'):\n            msg = 'ERROR: ', mol.name, ' not formatted! Choose as ligand first!'\n            self.vf.warningMsg(msg)\n            return 'ERROR'\n\n        if bond.possibleTors==0:\n            return 'ERROR'\n\n        ind1 = mol.allAtoms.index(bond.atom1)\n        ind2 = mol.allAtoms.index(bond.atom2)\n\n        if bond.activeTors!=rotatable:\n            mol.LPO.toggle_torsion(ind1, ind2)\n        else:\n            return 'ERROR'\n        if self.vf.hasGui:\n            self.buildCol()",
    "docstring": "set the flag that tells whether a bond is rotatable in aan AutoDock\n    ligand"
}
{
    "class": "class DedicatedIpList(BaseModel):\n\n    __root__: Annotated[\n        List[DedicatedIp],\n        Field(\n            description='A list of dedicated IP addresses that are associated with your AWS account.'\n        ),\n    ]",
    "docstring": "A list of dedicated IP addresses that are associated with your AWS account."
}
{
    "class": "class NumpyFunctionInterface(ParamGroupsManager):\n\n    def __init__(self, params, forward, *, \n            isfrozen=False, x_proj=None, grad_proj=None, always_refresh=True, \n            **kw):\n        defaults = dict(isfrozen=isfrozen, \n                x_proj=x_proj, grad_proj=grad_proj, **kw)\n        super(NumpyFunctionInterface, self).__init__(params, defaults)\n        self.dtype = next(self.params).data.cpu().numpy().dtype\n        self._forward = forward\n        self.options_refresh()\n        self.always_refresh = always_refresh\n\n    def options_refresh(self):\n        self._need_backward = True\n        self._grad_cache = None\n        self._x_cache = None\n        self._loss = None\n        self._numel = None\n\n    @staticmethod\n    def _proj_check(kw):\n        if not kw['isfrozen'] and None in set([kw['x_proj'],kw['grad_proj']]):\n            if not (kw['x_proj'] is None and kw['grad_proj'] is None):\n                print(kw)\n                warnings.warn(\"Exactly one of {x_proj,grad_proj} is not None, \"\n                        \"and the parameters are not set to be frozen, \"\n                        \"make sure what you are doing now.\")\n        return None\n    def set_options(self, idx, **kw):\n        self.param_groups[idx].update(**kw)\n        NumpyFunctionInterface._proj_check(self.param_groups[idx])\n        self.options_refresh()\n    def add_param_group(self, param_group):\n        super(NumpyFunctionInterface, self).add_param_group(param_group)\n        param_group_tmp = self.param_groups[-1]\n        NumpyFunctionInterface._proj_check(param_group_tmp)\n        for _,p in param_group_tmp['params'].items():\n            if not p.is_leaf:\n                raise ValueError(\"can't manage a non-leaf Tensor\")\n            if not p.requires_grad:\n                raise ValueError(\"managing a Tensor that does not \"\n                        \"require gradients\")\n        self.options_refresh()\n\n    @property\n    def forward(self):\n        self.options_refresh()\n        return self._forward\n    @forward.setter\n    def forward(self, v):\n        self.options_refresh()\n        self._forward = v\n\n    def numel(self):\n        if not self._numel is None:\n            return self._numel\n        return reduce(lambda a,p: a+p.numel(), self.params, 0)\n\n    def _all_x_proj(self):\n        for param_group in self.param_groups:\n            x_proj = param_group['x_proj']\n            if not x_proj is None:\n                x_proj(param_group['params'])\n    def _all_grad_proj(self):\n        for param_group in self.param_groups:\n            grad_proj = param_group['grad_proj']\n            if not grad_proj is None:\n                grad_proj(param_group['params'])\n\n    @property\n    def flat_param(self):\n        views = []\n        self._all_x_proj()\n        for p in self.params:\n            view = p.data.view(-1).cpu()\n            views.append(view)\n        return torch.cat(views,0).numpy()\n    @flat_param.setter\n    def flat_param(self, x):\n        assert isinstance(x, np.ndarray)\n        assert x.size == self.numel()\n        x = x.astype(dtype=self.dtype,copy=False)\n        offset = 0\n        for isfrozen,p in self.params_with_info('isfrozen'):\n            numel = p.numel()\n            if not isfrozen:\n                p_tmp = torch.from_numpy(x[offset:offset+numel])\\\n                        .view_as(p)\n                p.data.copy_(p_tmp)\n            offset += numel\n        self._all_x_proj()\n\n    def _flat_grad(self):\n        views = []\n        self._all_grad_proj()\n        for isfrozen, p in self.params_with_info('isfrozen'):\n            if isfrozen or p.grad is None:\n                view = torch.zeros(p.numel(), dtype=p.dtype)\n            else:\n                view = p.grad.data.view(-1).cpu()\n            views.append(view)\n        return torch.cat(views, 0).numpy()\n\n    def f(self, x, *args, **kw):\n        if self.always_refresh:\n            self.options_refresh()\n        self.flat_param = x\n        _x_cache = self.flat_param\n        if self._loss is None or not np.array_equal(_x_cache, self._x_cache):\n            self._x_cache = _x_cache\n            self._loss = self._forward()\n            self._need_backward = True\n        return self._loss.item()\n    def fprime(self, x, always_double=True, *args, **kw):\n        self.f(x)\n        if self._need_backward:\n            self.zero_grad()\n            self._loss.backward()\n            self._grad_cache = self._flat_grad()\n        self._need_backward = False\n        if always_double:\n            return self._grad_cache.astype(np.float64)\n        else:\n            return self._grad_cache",
    "docstring": "Interfaces class for representing torch forward & backward procedures \n           as Numpy functions. \n\n    .. warning::\n    If you are going to change options of one of self.param_groups with \n        always_refresh=False, please use self.set_options. This is because, for \n        example, any changes on 'grad_proj's will have impact on self.fprime(x), \n        even for the same input x; so do 'isfrozen's, 'x_proj's. So do changes \n        on 'x_proj's, 'isfrozen's.\n\n    .. warning::\n    Right now all parameters have to be dense Variable and their dtype \n        (float or double) have to be the same. This will be improved in the \n        future.\n\n    Arguments:\n        params (iterable): See ParamGroupsManager.__doc__\n        forward (callable): callable forward(**kw)\n            torch forward procedure, return a :class:`torch.Tensor`\n        isfrozen (bool): whether parameters should be frozen, if you set \n            isfrozen=True, as a result, grad of this param_group would be \n            set to be 0 after calling self.fprime(x).\n        x_proj (callable): callable x_proj(param_group['params']). \n            It is similar to nn.module.register_forward_pre_hook(x_proj) but \n            they are not have to be the same. Each time you call \n            self.set_options(idx,{'x_proj':x_proj}), self._x_cache will be \n            set to be None.\n            It can be used to make parameters to satisfied linear constraint. \n            Wether isfrozen or not, x_proj&grad_proj will go their own way.\n        grad_proj (callable): callable grad_proj(param_group['params']).\n            It is similar to nn.module.register_backward_hook(grad_proj).\n            grad_proj(param_group['params']) should project gradients of \n            param_group['params'] to the constrained linear space if needed.\n        always_refresh (bool): If always_refresh=True, then any changes on \n            forward & backward procedure is OK. We recommand you to set \n            always_refresh=True unless you are familiar with \n            :class:`NumpyFunctionInterface`.\n            When always_refresh=False, NumpyFunctionInterface will cache \n            parameters for fast forward & backward.\n        **kw (keyword args): other options for parameter groups"
}
{
    "class": "class DefaultServer(FingerPrint):\n\n    def __init__(self):\n        self.platform = 'railo'\n        self.version = None\n        self.title = RINTERFACES.DSR\n        self.uri = '/'\n        self.port = 8888\n        self.hash = None\n\n    def check(self, ip, port = None):\n\n        try:\n            rport = self.port if port is None else port\n            url = 'http://{0}:{1}{2}'.format(ip, rport, self.uri)\n\n            response = utility.requests_get(url)\n            if response.status_code is 200:\n                \n                data = findall(\"<title>Welcome to Railo (.*?)</title>\", response.content)\n                if len(data) > 0 and self.version in data[0]:\n                    return True\n\n        except exceptions.Timeout:\n            utility.Msg(\"{0} timeout to {1}:{2}\".format(self.platform, ip,\n                                                        rport), LOG.DEBUG)\n        except exceptions.ConnectionError:\n            utility.Msg(\"{0} connection error to {1}:{2}\".format(self.platform,\n                                                          ip, rport), LOG.DEBUG)\n\n        return False",
    "docstring": "This tests for the default welcome page at /"
}
{
    "class": "class CreateRelatedAgendaItemView(SingleObjectMixin, RedirectView):\n    required_permission = 'agenda.can_manage_agenda'\n    url_name = 'item_overview'\n    url_name_args = []\n\n    def get(self, request, *args, **kwargs):\n        self.object = self.get_object()\n        return super(CreateRelatedAgendaItemView, self).get(request, *args, **kwargs)\n\n    def pre_redirect(self, request, *args, **kwargs):\n        self.item = Item.objects.create(content_object=self.object)",
    "docstring": "View to create and agenda item for a related object.\n\n    This view is only for subclassing in views of related apps. You\n    have to define 'model = ....'"
}
{
    "class": "class ListUserPoolClientsRequest(BaseModel):\n\n    UserPoolId: UserPoolIdType\n    MaxResults: Optional[QueryLimit] = None\n    NextToken: Optional[PaginationKey] = None",
    "docstring": "Represents the request to list the user pool clients."
}